{
  "research_topic": "diffusion model„ÅÆÈÄüÂ∫¶ÊîπÂñÑ",
  "queries": [
    "diffusion model acceleration"
  ],
  "research_study_list": [
    {
      "title": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference",
      "abstract": "One of the main drawback of diffusion models is the slow inference time for\nimage generation. Among the most successful approaches to addressing this\nproblem are distillation methods. However, these methods require considerable\ncomputational resources. In this paper, we take another approach to diffusion\nmodel acceleration. We conduct a comprehensive study of the UNet encoder and\nempirically analyze the encoder features. This provides insights regarding\ntheir changes during the inference process. In particular, we find that encoder\nfeatures change minimally, whereas the decoder features exhibit substantial\nvariations across different time-steps. This insight motivates us to omit\nencoder computation at certain adjacent time-steps and reuse encoder features\nof previous time-steps as input to the decoder in multiple time-steps.\nImportantly, this allows us to perform decoder computation in parallel, further\naccelerating the denoising process. Additionally, we introduce a prior noise\ninjection method to improve the texture details in the generated image. Besides\nthe standard text-to-image task, we also validate our approach on other tasks:\ntext-to-video, personalized generation and reference-guided generation. Without\nutilizing any knowledge distillation technique, our approach accelerates both\nthe Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41$\\%$ and 24$\\%$\nrespectively, and DiT model sampling by 34$\\%$, while maintaining high-quality\ngeneration performance.",
      "full_text": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference Senmao Li1‚àó, Taihang Hu1‚àó, Joost van de Weijer2, Fahad Shahbaz Khan3,4, Tao Liu1 Linxuan Li1, Shiqi Yang5, Yaxing Wang1‚Ä†, Ming-Ming Cheng1, Jian Yang1 1VCIP, CS, Nankai University,2Computer Vision Center, Universitat Aut√≤noma de Barcelona 3Mohamed bin Zayed University of AI, 4Linkoping University, 5Independent Researcher, Tokyo {senmaonk, hutaihang00, ltolcy0, linxuanli520, shiqi.yang147.jp}@gmail.com joost@cvc.uab.es, fahad.khan@liu.se, {yaxing,cmm,csjyang}@nankai.edu.cn https://sen-mao.github.io/FasterDiffusion Custom Diffusion 2.42s 41%‚Üì Custom Diffusion w/ Ours 1.42s ControlNet 3.20s ControlNet w/ Ours 1.52s Stable Diffusion DDIM 2.42s DPM-Solver++ 1.13s DPM-Solver++ 0.64s41%‚ÜìDDIM w/ Ours 1.42s  43%‚Üì  24%‚Üì 20%‚Üì DeepFloyd-IF DPM-Solver++ 16.09s DDPM 34.55s DPM-Solver++ w/ Ours 12.97s DDPM w/ Ours 26.27s 34%‚Üì 51%‚Üì DiT 26.25s DiT w/ Ours 17.35s 32%‚Üì VideoFusion 1.12s VideoFusion w/ Ours 0.76s Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image). Abstract One of the main drawback of diffusion models is the slow inference time for image generation. Among the most successful approaches to addressing this problem are distillation methods. However, these methods require considerable computational resources. In this paper, we take another approach to diffusion model acceleration. We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features. This provides insights regarding their changes during the inference process. In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different time- steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process. Additionally, we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on ‚àóEqual contribution. Author ordering determined by coin flip over a Google Hangout. ‚Ä†The corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2312.09608v2  [cs.CV]  15 Oct 2024other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41 % and 24% respectively, and DiT model sampling by 34%, while maintaining high-quality generation performance. 1 Introduction One of the popular paradigms in image generation, Diffusion Models (DMs) [1, 2, 3] have recently achieved significant breakthroughs in various domains, including text-to-video generation [4, 5, 6], personalized image generation [7, 8, 9] and reference-guided image generation [10, 11, 12]. While diffusion models produce images of exceptional visual quality, their primary drawback lies in the prolonged inference time. The original diffusion model had an inference time several orders of magnitude slower than, for instance, GANs. One of the challenges hindering the acceleration of diffusion models is their inherent sequential denoising process, which limits the possibilities of effective parallelization. To improve inference time speed of diffusion models, several methods have been developed, that can roughly be divided in two sets of approaches. Firstly, involving step reduction, the aim is to reduce the number of sampling steps within diffusion model inference, such as DDIM [ 13] and DPM-Solver [14], which have significantly reduced the number of sampling steps. Secondly, in contrast, knowledge distillation progressively distills a slow (many-step) teacher model into a faster (few-step) student model [15, 16]. Some recent works [17, 18, 19] excel at generating high-fidelity images in a few-step sampling scenario but face challenges in maintaining quality and diversity in one-step sampling. The main drawback of the distillation methods is that they require retraining to perform the distillation into faster diffusion models. Figure 2: Visualising the hierarchical features 1. We applied PCA to the hierarchical features following PnP [20] and used the top three leading components as an RGB image for visualization. The encoder fea- tures change minimally and have similarities at many time-steps (top), while the decoder features exhibit sub- stantial variations across different time-steps (bottom). Orthogonal to these methods, we take a closer look at the sequential nature of the denoising process. We focus on the char- acteristics of the encoder in pretrained diffusion models (e.g., the SD and the DiT [21]2) Interestingly, based on our anal- ysis presented in Sec 3.2, we discover that encoder features change minimally (Fig. 3a) and have a high degree of similar- ity (Fig. 2 (top)), whereas the decoder fea- tures exhibit substantial variations across different time-steps (Fig. 3a and Fig. 2 (bot- tom)). This insight is relevant because it allows us to circumvent the computation of the encoder during multiple time-steps. As a consequence, the decoder computations which are based on the same encoder input can be performed in parallel. Instead, we reuse the computed encoder features computed at one time-step (since these change minimally) as input to adjacent decoders during the following time-steps. More recently, both DeepCache[ 22] and CacheMe[23] leverage feature similarity to achieve acceleration. However, they rely on sequential denoising, as well as CacheMe requires fine-tuning. Unlike these methods, our approach supports parallel processing, which leads to faster inference (Tab. 2). We show that the proposed propagation scheme accelerates the SD sampling by 24% , DeepFolyd-IF sampling by 18%, and DiT sampling by 27%. Furthermore, since the same encoder features (from previous time-steps) can be used as the input to the decoder of multiple later time-steps, this makes it possible to conduct multiple time-steps decoding concurrently. This parallel procession accelerates SD sampling by 41%, DeepFloyd-IF sampling by 24%, and DiT sampling by 34%. Furthermore, to alleviate the deterioration of the generated quality, we introduce a prior noise injection strategy to preserve the texture details in the generated images. With these contributions, our proposed method achieves improved sampling efficiency while maintaining high image generation quality. 1See the visualisation of the hierarchical features for all blocks in Appendix A 2We define the first several transformer blocks of DiT as the encoder, and the remaining ones of the transform blocks as the decoder. 2(a) (b) (c) Figure 3: Analyzing the UNet in Diffusion Model. (a) Feature evolving across adjacent time-steps is measured by MSE. (b) We extract the hierarchical features output of different layers of the UNet at each time-step, average them along the channel dimension to obtain two-dimensional hierarchical features, and then calculate their Frobenius norm. (c) The hierarchical features of the UNet encoder show a lower standard deviation, while those of the decoder exhibit a higher standard deviation. Importantly, our method can be combined with several approaches existing to speed up DMs. The main advantage of our method with respect to distillation-based approaches, is that our method can be applied at inference time, and does not require retraining a new faster distillation model; a process that is computationally very demanding and infeasible for actors with limited computational budget. Finally, we evaluate the effectiveness of our approach across a wide range of conditional diffusion- based tasks, including text-to-video generation (e.g., Text2Video-zero [ 4] and VideoFusion [ 5]), personalized image generation (e.g., Dreambooth [7]) and reference-guided image generation (e.g., ControlNet [10]). To summarize, we make the following contributions: ‚Ä¢ We conduct a thorough empirical study of the features of the UNet in the diffusion model showing that encoder features vary minimally (whereas decoder feature vary significantly). ‚Ä¢ We propose a parallel strategy for diffusion model sampling at adjacent time-steps that significantly accelerates the denoising process. Importantly, our method does not require any training or fine-tuning technique. ‚Ä¢ Furthermore, we also present a prior noise injection method to improve the image quality (mainly improving the quality of high-frequency textures). ‚Ä¢ Our method can be combined with existing methods (like DDIM, and DPM-solver) to further accelerate diffusion model inference time. 2 Related Work Denoising diffusion model. Recently, Text-to-image diffusion models [1, 24, 25, 26] have made significant advancements. Notably, Stable Diffusion and DeepFloyd-IF stand out as two of the most successful diffusion models available within the current open-source community. These models, building upon the UNet architecture, are versatile and can be applied to a wide range of tasks, including image editing [27, 28], super-resolution [29, 30], segmentation [31, 32], and object detection [33, 34]. Given the strong scalability of transformer networks, DiT [21] investigates the transformer backbone for diffusion models. Diffusion model acceleration. Diffusion models use iterative denoising with UNet for image gener- ation, which is time-consuming. There are plenty of works trying to address this issue. One strategy involves employing efficient diffusion model solvers, such as DDIM [ 13] and DPM-Solver [ 14], which have demonstrated notable reductions in sampling steps. Additionally, ToMe [ 35] exploits token redundancy to minimize the computations necessary for attention operations [36]. Conversely, knowledge distillation methods, exemplified by techniques like progressive simplification by student models [15, 16], aim to streamline existing models. Some recent studies combine model compression with distillation to achieve faster sampling [37, 38]. Orthogonal to these approaches, we introduce a novel method for enhancing sampling efficiency in DMs inference. We show that our method can be combined with several existing speed-up methods for further acceleration. DeepCache[22] and CacheMe[23] are two recent works that leverage feature similarity to achieve acceleration. DeepCache [22] adopts a strategy of rudely reusing features cached from the previous step, requiring iterative denoising. Furthermore, CacheMe [23] requires additional fine-tuning for 3(d) Decoder propagation Sample (e) Non-uniform encoder propagation Parallel encoder propagation Sample (b) UNet architecture (a) Standard SD sampling Sample (c) Encoder propagation Sample Figure 4: (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. We omit the encoder at certain adjacent time-steps and reuse in parallel the encoder features in the previous time-steps for the decoder. Applying encoder propagation for uniform strategy every two iterations. Note, at time-step t-1, predicting noise does not require zt-1 (i.e., Eq. 1: zt‚àí2 = qŒ±t‚àí2 Œ±t‚àí1 zt‚àí1 + ‚àöŒ±t‚àí2 \u0010q 1 Œ±t‚àí2 ‚àí 1 ‚àí q 1 Œ±t‚àí1 ‚àí 1 \u0011 ¬∑ œµŒ∏(\u0018\u0018\u0018 XXXzt‚àí1, t‚àí 1, c)). (d) Decoder propagation. The generated images often fail to cover some specific objects in the text prompt. For example, given one prompt case ‚ÄúA man with a beard wearing glasses and a beanie\", this method fails to generate the glasses subject. See Appendix F for quantitative evaluation. (e) Applying encoder propagation for non- uniform strategy. By benefiting from our propagation scheme, we are able to perform the decoder in parallel at certain adjacent time-steps. better performance. In contrast, our approach enables parallel processing, leading to considerably faster inference. 3 Method We first briefly revisit the architecture of the Stable Diffusion (SD) (Sec. 3.1), and then conduct a comprehensive analysis for the hierarchical features of the UNet (Sec. 3.2). Our analysis shows that it is possible to parallelize the diffusion model denoising process partially. Thus, we introduce a novel method to accelerate the diffusion sampling while still largely maintaining the generation quality and fidelity (Sec. 3.3). 3.1 Latent Diffusion Model In the diffusion inference stage, the denoising network œµŒ∏ takes as input a text embedding c, a latent code zt and a time embedding, predicts noise, resulting in a latent zt‚àí1 using the DDIM scheduler [13]: zt‚àí1 = qŒ±t‚àí1 Œ±t zt +‚àöŒ±t‚àí1 \u0010q 1 Œ±t‚àí1 ‚àí1‚àí q 1 Œ±t ‚àí1 \u0011 ¬∑œµŒ∏(zt, t,c), (1) where Œ±t is a predefined scalar function at time-step t (t = T, ...,1). The typical denoising network uses a UNet-based architecture. It consists of an encoder E, a bottleneck B, and a decoder D, respectively (Fig. 4b). The hierarchical features extracted from the encoder E are injected into the decoder D by a skip connection (Fig. 4a). For convenience of description, we divide the UNet network into specific blocks: E = {E(¬∑)s}, B = {B(¬∑)8}, and D = {D(¬∑)s}, where s ‚àà {8, 16, 32, 64} (see Fig. 4b). Both E(¬∑)s 3 and D(¬∑)s represent the block layers with input resolution s in both encoder and decoder, respectively. Diffusion Transformer (DiT) [21] is a novel architecture for diffusion models. It replaces the UNet backbone with a transformer, which consists of 28 blocks. Based on our observations, we define the first 18 blocks as the encoder, and the remaining 10 blocks as the decoder (see Appendix A.3). 3.2 Analyzing the UNet in Diffusion Model In this section, we take the UNet-based diffusion model as example to analyze the properties of the pretrained diffsuion model. We delve into the UNet which consists of the encoderE, the bottleneck B, 3Once we replace the ¬∑ with specific inputs in E(¬∑)s, we define that it represents the feature of E(¬∑)s 4and the decoder D, for a deeper understanding of the different parts of the UNet. Note the following observed properties also exist in DiT (see Appendix A.3). Feature evolution across time-steps. We experimentally observe that the encoder features exhibit a subtle variation at adjacent time-steps, whereas the decoder features exhibit substantial variations across different time-steps (see Fig. 3a and Fig. 2). Specifically, given a pretrained diffusion model, we iteratively produce a latent code zt (see Eq. 1), and the corresponding hierarchical features: {E(zt, c, t)s}, {B(zt, c, t)8}, and {D(zt, c, t)s} (s ‚àà {8, 16, 32, 64}) 4, as shown in Fig. 4b. Here, we analyze how the hierarchical features change at adjacent time-steps. To achieve this goal, we quantify the variation of the hierarchical features as follows: ‚àÜE(¬∑)s = 1 d√ós2 ‚à•E(zt, c, t)s ‚àí E(zt‚àí1, c, t‚àí 1)s‚à•2 2, (2) where d represents the number of channels in E(zt, c, t)s. Similarly, we also compute ‚àÜB(¬∑)8 and ‚àÜD(¬∑)s . As illustrated in Fig. 3a, for both the encoder E and the decoder D, the curves exhibit a similar trend: in the wake of an initial increase, the variation reaches a plateau and then decreases, followed by a continuing growth towards the end. However, the extent of change in ‚àÜE(¬∑)s and ‚àÜD(¬∑)s is quantitatively markedly different. For example, the maximum value and variance of the‚àÜE(¬∑)s are less than 0.4 and 0.05, respectively (Fig. 3a (zoom-in area)), while the corresponding values of the ‚àÜD(¬∑)s are about 5 and 30, respectively (Fig. 3a). Furthermore, we find that ‚àÜD(¬∑)64 , the change of the last layer of the decoder, is close to zero. This is due to the output of the denoising network being similar at adjacent time-steps [39]. In conclusion, the overall feature change ‚àÜE(¬∑)s is smaller than ‚àÜD(¬∑)s throughout the inference phase. Feature evolution across layers. We experimentally observe that the feature characteristics are significantly different between the encoder and the decoder across all time-steps. For the encoder E the intensity of the change is slight, whereas it is very drastic for the decoder D. Specifically we calculate the Frobenius normfor hierarchical features E(zt, c, t)s across all time-steps, dubbed as FE(¬∑)s = {FE(zT ,c,T)s , ...,FE(z1,c,1)s }. Similarly, we compute FB(¬∑)8 and FD(¬∑)s , respectively. Fig. 3b shows the feature evolution across layers with a boxplot 5. Specifically, for \b FE(¬∑)s \t and\b FB(¬∑)8 \t , the box is relatively compact, with a narrow range between their first-quartile and third- quartile values. For example, the maximum box height (FE(.)32) of these features is less than 5 (see Fig. 3b (zoom-in area)). This indicates that the features from both the encoder E and the bottleneck B slightly change. In contrast, the box heights corresponding to {D(¬∑)s} are relatively large. For example, for the D(.)64 the box height is over 150 between the first quartile and third quartile values (see Fig. 3b). Furthermore, we also provide a standard deviation (Fig. 3c), which exhibits similar phenomena to Fig. 3b. These results show that the encoder features have relatively small discrepancy and a high degree of similarity across all layers. However, the decoder features evolve drastically. Could we omit the Encoder at certain time-steps? As indicated by the previous experimental analysis, we observe that, during the denoising process, the decoder features change drastically, whereas the encoder E features change minimally, and have a high degree of similarities at certain adjacent time-steps. Therefore, as shown in Fig. 4c, We propose to omit the encoder at certain time-steps and use the same encoder features for several decoder steps. This allows us to compute these multiple decoder steps in parallel. Specifically, we delete the encoder at time-step t ‚àí 1 (t ‚àí 1 < T), and the corresponding decoder (including the skip connections) takes as input the hierarchical outputs of the encoder E from the previous time-step t, instead of the ones from the current time-stept‚àí1 like the standard SD sampling (for more detail, see Sec. 3.3). When omitting the encoder at a certain time-step, we are able to generate similar images (Fig. 4c) like standard SD sampling (Fig. 4a, Tab. 1 (the first and second rows) and additional results in Appendix F). Alternatively, if we use a similar strategy for the decoder (i.e., decoder propagation), we find the 4The feature resolution is half of the previous one in the encoder and two times in the decoder. Note that the feature resolutions of E(.)8, B(.)8 and D(.)64 do not change in the SD model. 5Each boxplot contains the minimum (0th percentile), the maximum (100th percentile), the median (50th percentile), the first quartile (25th percentile) and the third quartile (75th percentile) values of the feature Frobenius norm (e.g., {FE(zT ,c,T)s , ...,FE(z1,c,1)s }). 5generated images often fail to cover some specific objects in the text prompt (Fig. 4d). For example, when provided with prompt ‚ÄúA man with a beard wearing glasses and a beanie\", the SD model fails to synthesize ‚Äúglasses\" when applying decoder propagation. This is due to the fact that the semantics are mainly contained in the features from the decoder rather than the encoder [40]. The encoder propagation, which uses encoder outputs from previous time-step as the input to the current decoder, could speed up the diffusion model sampling at inference time. In the following Sec. 3.3, we give further elaborate on encoder propagation. 3.3 Encoder propagation Diffusion sampling, combining iterative denoising with transformers, is time-consuming. Therefore we propose a novel and practical diffusion sampling acceleration method. During the diffusion sampling process t= {T, ...,1}, we refer to the time-steps where encoder propagation is deployed, as non-key time-steps denoted as tnon-key= n tnon-key 0 , ..., tnon-key N‚àí1 o . The remaining time-steps are dubbed as tkey= n tkey 0 , tkey 1 , ..., tkey T‚àí1‚àíN o . In other words, we do not use the encoder at time-steps tnon-key, and leverage the hierarchical features of the encoder from the time-step tkey. Note we utilize the encoder E at the initial time-step (tkey 0 = T). Thus, the diffusion inference time-steps could be reformulated as \b tkey, tnon-key\t , where tkey ‚à™ tnon-key = {T, ...,1} and tkey ‚à© tnon-key = ‚àÖ. In the following, we introduce both uniform encoder propagationand non-uniform encoder propagation strategies. As shown in Fig. 3a, The encoder feature change is larger in the initial inference phase compared to the later phases throughout the inference process. Therefore, we select more key time-steps in the initial inference phase, and less key time-steps in later phases. We experimentally define the key time-steps as tkey={50, 49, 48, 47, 45, 40, 35, 25, 15} for SD model with DDIM, and tkey={100, 99, 98, . . ., 92, 91, 90, 85, 80, . . ., 25, 20, 15, 14, 13, . . ., 2, 1}6, {50, 49, . . . ,2, 1} and {75, 73, 70, 66, 61, 55, 48, 40, 31, 21, 10} for three stage of DeepFloyd-IF (see detail key time-steps selection in Appendix F.2). The remaining time-steps are categorized as non-key time-steps. We define this strategy as non- uniform encoder propagation (see Fig. 4e). As shown in Fig. 4c, we also explore the time-step selection with fix stride (e.g, 2), dubbed as uniform encoder propagation. Note that our method does not reduce the number of sampling steps. During encoder propagation, the decoder computes for all time-steps, necessitating time embedding inputs for each time-step to maintain temporal coherence (see detail in Appendix D). Tab. 5 reports the results of the ablation study, considering various combinations ofkey and non-key time-steps. These results indicate that the set of non-uniform key time-steps performs better in generating images. SD (DDIM) 2.42s 1.82s24%‚Üì Encoder propagation 41%‚Üì1.42s Parallel encoder propagation Figure 5: Comparing with SD (left), encoder prop- agation reduces the sampling time by 24% (mid- dle). Furthermore, parallel encoder propagation achieves a 41% reduction in sampling time (right). Parallel non-uniform encoder propagation. When applying the non-uniform encoder prop- agation strategy, at time-step t ‚àà tnon‚àíkey the decoder inputs do not rely on the encoder out- puts at time-step t (see Fig. 4e). Instead, it relies on the encoder output at the previous nearest key time-step. This allows us to perform par- allel non-uniform encoder propagationat these adjacent time-steps in tnon‚àíkey. We perform de- coding in parallel from t to t ‚àí k + 1 time-steps. This technique further improves the inference efficiency since the decoder forward in multiple time-steps could be conducted concurrently. We indicate this as parallel-batch non-key time-steps. As shown in Fig. 5 (right), this further reduces evaluation time by 41% for the SD model. Prior noise injection. Although the encoder propagation could improve the efficiency in the inference phase, we observe that it leads to a slight loss of texture information in the generated 6The ellipsis in tkey denotes each time-step between the time-steps on either side of the ellipsis. For example, 80...25 means that every time-step between 80 and 25 is included. 6results (see Fig. 6 (left, middle)). Inspired by related works [ 41, 42], we propose a prior noise injection strategy. It combines the initial latent code zT into the generative process at subsequent time-step (i.e., zt), following zt = zt + Œ± ¬∑ zT , if t < œÑ, where Œ± = 0.003 is the scale parameter to control the impact of zT . And we start to use this injection mechanism from œÑ = 25 step. This strategic incorporation successfully improves the texture information. Importantly, it demands almost negligible extra computational resources. We observe that the loss of texture information occurs in all frequencies of the frequency domain (see Fig. 6 (right, red, and blue curves)). This approach ensures a close resemblance of generated results in the frequency domain to both SD and zT injection (see Fig. 6 (right, red and green curves)), with the generated images maintaining the desired fidelity (see Fig. 6 (left, bottom)). 4 Experiments SD Ours w/o injectionùëßùëá Ours w/ injectionùëßùëá Generated image Low frequency High frequency Figure 6: (left) We keep the image content through zT injection, slightly compensating for the texture informa- tion loss caused by encoder propagation. (right) The amplitudes of the generated image through zT injection closely resemble those from SD. In our experiments, we assess the speed-up of our method compared to others for in- ference acceleration. We also explore com- bining our method with these approaches. We do not directly compare our method with distillation methods, which offer su- perior results but involve computationally expensive retraining. Datasets and evaluation metrics. We randomly select 10K prompts from the MS-COCO2017 validation dataset [43] and feed them into the text-to-image diffusion model to obtain 10K generated images. For the transformer architecture diffusion model, we randomly generate 50K images from 1000 ImageNet [44] class labels. For other tasks, we use the same settings as baselines (e.g., Text2Video-zero [4], VideoFusion [5], Dream- booth [7] and ControlNet [10]). We use the Fr√©chet Inception Distance (FID) [45] metric to assess the visual quality of the generated images, and the Clipscore [46] to measure the consistency between image content and text prompt. Furthermore, we report the average values for both the computational workload (GFLOPs/image) and sampling time (s/image) to represent the resource demands for a single image. See more detailed implementation information on Appendix A. 4.1 Text-to-image Generation Table 1: Quantitative evaluation7 for both SD and DeepFloyd-IF diffusion models. s/image ‚Üì DM Sampling Method T FID‚Üì Clip- score‚Üë GFLOPs/ image‚Üì Unet of DM DM Stable Diffusion DDIM 50 21.75 0.773 37050 2.23 2.42DDIM w/ Ours 50 21.08 0.783 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì DPM-Solver 20 21.36 0.780 14821 0.90 1.14 DPM-Solver w/ Ours 20 21.25 0.779 1174321%‚Üì 0.4648%‚Üì 0.6443%‚Üì DPM-Solver++ 20 20.51 0.782 14821 0.90 1.13 DPM-Solver++ w/ Ours 20 20.76 0.781 1174321%‚Üì 0.4648%‚Üì 0.6443%‚Üì DDIM + ToMe 50 22.32 0.782 35123 2.07 2.26DDIM + ToMe w/ Ours 50 20.73 0.781 2605326%‚Üì 1.1544%‚Üì 1.3341%‚Üì DeepFloyd-IF DDPM 225 23.89 0.783 734825 33.91 34.55DDPM w/ Ours 22523.73 0.782 62652315%‚Üì25.6125%‚Üì26.2724%‚Üì DPM-Solver++10020.79 0.784 370525 15.19 16.09 DPM-Solver++ w/ Ours 10020.85 0.785 31338115%‚Üì12.0221%‚Üì12.9720%‚Üì We first evaluate the proposed encoder propaga- tion method for the standard text-to-image gen- eration task on both the latent space (i.e., SD) and pixel space (i.e., DeepFloyd-IF) diffusion models. As shown in Tab. 1, we significantly accelerate the diffusion sampling with negligi- ble performance degradation. Specifically, our proposed method decreases the computational burden (GFLOPs) by a large margin (27%) and greatly reduces sampling time to 41% when compared to standard DDIM sampling in SD. Similarly, in DeepFloyd-IF, the reduction in both computational burden and time reaches15% and 24%, respectively. Furthermore, our method can be combined with the latest sampling techniques like DPM-Solver [14], DPM-Solver++ [47], and ToMe [35]. Our method enhances sampling ef- ficiency while preserving good model perfor- mance, with negligible variations of both FID and Clipscore values (Tab. 1 (the third to eighth rows)). Our method achieves good performance across different sampling steps (Fig. 7 and see Appendix D 7We use the official implementation of Clipscore [46] to obtain around 0.75 but around 0.3. See Appendix C. 7Table 2: Comparison with DeepCache and CacheMe. CacheMe is not open-source. Sampling Method T Parallel FID ‚Üì Clipscore ‚Üë s/image DDIM 50 √ó 21.75 0.773 2.42 DDIM w/ DeepCache 50 √ó 21.53 0.770 1.0556%‚Üì DDIM w/ CacheMe 50 √ó ‚Äì ‚Äì 1.3044%‚Üì DDIM w/ Ours 50 ‚úì 21.62 0.775 0.5677%‚Üì Table 3: Quantitative evaluation for DiT. Sampling Method T Image Res. FID ‚ÜìsFID ‚Üì IS ‚Üë Precision ‚ÜëRecall ‚Üë s/image DiT 250 256 2.27 4.60 278.24 0.83 0.57 5.13 DiT w/ Ours250 256 2.31 4.55 276.05 0.82 0.57 3.62 29%‚Üì DiT 250 512 3.04 5.02 240.82 0.84 0.54 26.25 DiT w/ Ours250 512 3.25 5.05 245.13 0.83 0.51 17.35 34%‚Üì Table 4: Quantitative evaluation on text-to-video, personalized generation and reference-guided generation tasks. ‚Ä† and ‚Ä° indicate ‚Äúedges‚Äù and ‚Äúscribble‚Äù conditions, respectively. s/image‚Üì Method T FID‚Üì Clip- score‚Üë GFLOPs/ image‚Üì Unet of SD SD Text2Video-zero 50 - 0.732 39670 12.59/8 13.65/8 Text2Video-zero w/ Ours 50 - 0.731 3069022%‚Üì 9.46/825%‚Üì 10.54/823%‚Üì VideoFusion 50 - 0.700 224700 16.71/16 17.93/16 VideoFusion w/ Ours 50 - 0.700 14868033%‚Üì11.1/1634%‚Üì12.2/1632%‚Üì ControlNet (‚Ä†) 50 13.78 0.769 49500 3.09 3.20 ControlNet (‚Ä†) w/ Ours 50 14.65 0.767 3140037%‚Üì 1.4354%‚Üì 1.5251%‚Üì ControlNet (‚Ä°) 50 16.17 0.775 56850 3.85 3.95 ControlNet (‚Ä°) w/ Ours 50 16.42 0.775 3599037%‚Üì 1.8353%‚Üì 1.9351%‚Üì Dreambooth 50 - 0.640 37050 2.23 2.42 Dreambooth w/ Ours 50 - 0.660 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì CustomDiffusion50 - 0.640 37050 2.21 2.42 CustomDiffusion w/ Ours 50 - 0.650 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì Table 5: Quantitative evaluation in various propa- gation strategies on MS-COCO 2017 10K subset. FTC=FID√óTime/Clipscore. Propagation strategy FID‚Üì Clipscore‚Üë GFLOPs /image‚Üì s/image‚Üì FTC‚Üì Unet of SD SD SD 21.75 0.773 37050 2.23 2.42 68.1 Uniform I tkey={50,48,46,44,42,40,38,36,34,32,30,28, 26,24,22,20,18,16,14,12,10,8,6,4,2} 21.55 0.775 3101116%‚Üì 1.6227%‚Üì 1.8125%‚Üì 50.3 II tkey={50,44,38,32,26,20,14,8,2} 21.54 0.773 2735027%‚Üì 1.2643%‚Üì 1.4640%‚Üì 40.7 III tkey={50,38,26,14,2} 24.61 0.766 2637029%‚Üì 1.1250%‚Üì 1.3644%‚Üì 43.7 Non-uniform I tkey={50,40,39,38,30,25,20,15,5} 22.94 0.776 2735027%‚Üì 1.2643%‚Üì 1.4241%‚Üì 41.9 II tkey={50,30,25,20,15,14,5,4,3} 35.25 0.742 2735027%‚Üì 1.2543%‚Üì 1.4241%‚Üì 67.4 III tkey={50,41,37,35,22,21,18,14,5} 22.14 0.778 2735027%‚Üì 1.2245%‚Üì 1.4241%‚Üì 40.4 IV (Ours) tkey={50,49,48,47,45,40,35,25,15} 21.08 0.783 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì 38.2 for quantitative results.). Importantly, these results show that our method is orthogonal and compatible with these acceleration techniques. As shown in Fig. 1, we visualize the generated images with different sampling techniques. Our method still generates high-quality results (see Appendix F for additional results). Our method allows to use multi-GPU to generate one image. With multi-GPU parallel, our proposed method further accelerates the SD sampling by 77%, whereas DeepCache [22] and CacheMe [23] achieve speedups of 56% and 44%, respectively (See Tab. 2). These results indicate that we achieve superior acceleration compared to DeepCache [22] and CacheMe [23]. 4.2 Diffusion Transformer We also evaluate our approach on DiT. As reported in Tab. 3, we achieve accelerations of about29% and 34% for DiT sampling with image resolution of 256 and 512, respectively, while preserving high-quality results (see Figs. 1 and 18). 4.3 Other tasks with text-guided diffusion model Besides the standard text-to-image task, we also validate our proposed approach on other tasks: text-to-video generation, personalized generation, and reference-guided image generation. Text-to-video. To evaluate our method, we combine it with both Text2Video-zero [4] and VideoFu- sion [5]. As reported in Tab. 4 (the second and fourth rows), when combined with our method, the two methods have a reduction of approximately22% to 33% in both computational burden and generation time. These results indicate that we are able to enhance the efficiency of generative processes in the text-to-video task while preserving video fidelity at the same time (Fig. 1 (left, bottom)). As an example, when generating a video using the prompt ‚ÄúFireworks bloom in the night sky\", the VideoFusion model takes 17.92 seconds with 16 frames for the task (1.12s/frame), when combined with our method it only takes 12.27s (0.76s/frame) to generate a high-quality video (Fig. 1 (left, bottom)). Personalized image generation. Dreambooth [7] and Custom Diffusion [8] are two approaches for customizing tasks by fine-tuning text-to-image diffusion models. As reported in Tab. 4 (the ninth to twelfth rows), our method, combining with the two customization approaches, accelerates image 850-Step 40-Step 30-Step 20-Step 15-Step 10-Step 5-Step DDIM DDIM  w/ Ours DPM- Solver++ DPM- Solver++  w/ Ours Figure 7: Generated images at different time-steps. Figure 8: User study results. generation and reduces computational demands. Visually, it maintains the ability to generate images with specific contextual relationships based on reference images. (Fig. 1 (right)) Reference-guided image generation. ControlNet [10] incorporates a trainable encoder, successfully generates a text-guided image, and preserves similar content with conditional information. Our approach can be applied concurrently to two encoders of ControNet. In this paper, we validate the proposed method with two conditional controls: edge and scribble. Tab. 4 (the fifth to eighth row) reports quantitative results. We observe that it leads to a significant decrease in both generation time and computational burden. Furthermore, Fig. 1 (middle, bottom) qualitatively shows that our method successfully preserves the given structure information and achieves similar results as ControlNet. User study. We conducted a user study, as depicted in Fig. 8, and asked subjects to select results. We apply pairwise comparisons (forced choice) with 18 users (35 pairs of images or videos/user). The results demonstrate that our method performs equally well as the baseline methods. 4.4 Ablation study Non-uniform (Ours) SD (DDIM) Uniform Figure 9: Generating image with uniform and non- uniform encoder propagation. The result of uni- form strategy II yields smooth and loses textual compared with SD. Both uniform strategy III and non-uniform strategy I, II and III generate images with unnatural saturation levels. We ablate the results with different selections of both uniform and non-uniform encoder prop- agation. Tab. 5 reports that the performance of the non-uniform setting outperforms the uni- form one in terms of both FID and Clipscore (see Tab. 5 (the third and eighth rows)). Further- more, we explore different configurations within the non-uniform strategy. The strategy, using the set of key time-steps we established, yields better results in the generation process (Tab. 5 (the eighth row)). We further present qualita- tive results stemming from the above choices. As shown in Fig. 9, given the same number of key time-steps, the appearance of nine-step non- uniform strategy I, II and III settings do not align with the prompt ‚ÄúFireflies dot the night sky\". Although the generated image in the two- step setting exhibits a pleasing visual quality, its sampling efficiency is lower than our chosen setting (see Tab. 5 (the second and eighth rows)). Table 6: Quantitative evaluation for prior noise injection. Sampling Method SD (DDIM) SD (DDIM) + Ours w/o zT injection SD (DDIM) + Ours w/ zT injection FID ‚Üì 21.75 21.71 21.08 Clipscore ‚Üë 0.773 0.779 0.783 Effectiveness of prior noise injection. We evaluate the effectiveness of injecting initial zT . As reported in Tab. 6, the differences in FID and Clipscores without zT (the third column), when compared to DDIM and Ours (the second and fourth columns), are approximately 0.01%, which can be considered negligible. While this is not the case for the visual expression of the generated image, it is observed that the output contains complete semantic information with smoothing texture (refer to Fig. 6 (left, the second row)). Injecting the zT aids in maintaining fidelity in the generated results during encoding propagation (see Fig. 6 (left, the third row) and Fig. 6 (right, red and green curves)). 95 Conclusion In this work, We explore the characteristics of the encoder and decoder in UNet of the text-to-image diffusion model and find that encoder feature variation is minimal for many time-steps, while the decoder plays a significant role across all time-steps. Building upon this finding, we propose encoder propagation for efficient diffusion sampling, reducing time on both the UNet-based and the transform- based diffusion models on a diverse set of generation tasks. We conduct extensive experiments and validate that our approach can achieve improved sampling efficiency while maintaining image quality. Limitations: Although our approach achieves efficient diffusion sampling, it faces challenges in generating quality when using a limited number of sampling steps (e.g., 5). In addition, even though our proposed parallelization can also be applied to network distillation approaches [18, 17, 19], we have not explored this direction in this paper and leave it to future research. Acknowledgements This work was supported by NSFC (NO. 62225604) and Youth Foundation (62202243). We acknowledge project PID2022-143257NB-I00, financed by the Spanish Government MCIN/AEI/10.13039/501100011033 and FEDER. Computation is supported by the Supercomputing Center of Nankai University (NKSC). We would like to thank Kai Wang, a postdoctoral researcher at the Computer Vision Center, Universitat Aut√≤noma de Barcelona, for his helpful discussions and comments during the rebuttal period. References [1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684‚Äì10695, 2022. [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840‚Äì6851, 2020. [3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [4] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. [5] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10209‚Äì 10218, 2023. [6] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. [7] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream- booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500‚Äì22510, 2023. [8] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931‚Äì1941, 2023. [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [10] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836‚Äì3847, 2023. [11] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i- adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 10[12] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511‚Äì22521, 2023. [13] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [14] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775‚Äì5787, 2022. [15] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [16] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297‚Äì14306, 2023. [17] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [18] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. [19] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. arXiv preprint arXiv:2312.05239, 2023. [20] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text- driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921‚Äì1930, 2023. [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. [22] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXiv preprint arXiv:2312.00858, 2023. [23] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. arXiv preprint arXiv:2312.03209, 2023. [24] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to- image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479‚Äì36494, 2022. [25] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [26] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023. [27] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to- prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [28] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing.arXiv preprint arXiv:2303.15649, 2023. [29] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015, 2023. [30] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12413‚Äì12422, 2022. [31] Dmitry Baranchuk, Ivan Rubachev, Andrey V oynov, Valentin Khrulkov, and Artem Babenko. Label- efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. [32] Julia Wolleb, Robin Sandk√ºhler, Florentin Bieder, Philippe Valmaggia, and Philippe C Cattin. Diffusion models for implicit image segmentation ensembles. In International Conference on Medical Imaging with Deep Learning, pages 1336‚Äì1348. PMLR, 2022. [33] Walter HL Pinaya, Mark S Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H Mah, Andrew D MacKinnon, James T Teo, Rolf Jager, et al. Fast unsupervised brain anomaly detection and segmentation with diffusion models. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 705‚Äì714. Springer, 2022. 11[34] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffusiondet: Diffusion model for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19830‚Äì19843, 2023. [35] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4598‚Äì4602, 2023. [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [37] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compression of text-to-image diffusion models. arXiv preprint arXiv:2305.15798, 2023. [38] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. arXiv preprint arXiv:2306.00980, 2023. [39] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023. [40] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. What does stable diffusion know about the 3d scene? arXiv preprint arXiv:2310.06836, 2023. [41] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11472‚Äì11481, 2022. [42] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. arXiv preprint arXiv:2210.10960, 2022. [43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740‚Äì755. Springer, 2014. [44] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [45] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.Advances in neural information processing systems, 30, 2017. [46] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [47] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [48] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22691‚Äì22702, 2023. [49] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. [50] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [51] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. [52] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. 2023. [53] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan, and Ziwei Liu. ReVersion: Diffusion-based relation inversion from images. arXiv preprint arXiv:2303.13495, 2023. [54] Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu Wang. Oms-dpm: Optimizing the model schedule for diffusion probabilistic models. In International Conference on Machine Learning, pages 21915‚Äì21936. PMLR, 2023. [55] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7105‚Äì7114, 2023. [56] Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, and Yingcong Chen. Denoising diffusion step-aware models. arXiv preprint arXiv:2310.03337, 2023. 12Contents 1 Introduction 2 2 Related Work 3 3 Method 4 3.1 Latent Diffusion Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Analyzing the UNet in Diffusion Model . . . . . . . . . . . . . . . . . . . . . . . 4 3.3 Encoder propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 Experiments 7 4.1 Text-to-image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.2 Diffusion Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.3 Other tasks with text-guided diffusion model . . . . . . . . . . . . . . . . . . . . . 8 4.4 Ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5 Conclusion 10 Appendix 14 A Implementation Details 14 A.1 Configure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 Details about the layers in the UNet . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.3 Details about the blocks in the DiT . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.4 Time and memory consumption ratios . . . . . . . . . . . . . . . . . . . . . . . . 16 A.5 GFLOPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.6 Baseline Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B Parameter Count and FLOPs of SD 19 C The methodology for computing Clipscore 19 D Different from step-reduction methods 19 E Difference between ‚Äúprior noise injection‚Äù and ‚Äúchurn‚Äù 21 F Ablation Experiments and Additional Results 21 F.1 Comparion with DeepCache . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 F.2 The definition of key time-steps in various tasks . . . . . . . . . . . . . . . . . . . 22 F.3 The effectiveness of encoder propagation . . . . . . . . . . . . . . . . . . . . . . . 22 F.4 User study details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 F.5 Additional metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 F.6 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 13F.7 Additional tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 G Automatic selection for key time steps 24 H Impact Statements 24 Appendix In the appendix, we provide a detailed description of the experimental implementation (Appendix A). Subsequently, an analysis of the parameter quantities for the encoder and decoder is conducted (Appendix B). Following that, we explain the methodology used to compute Clipscore (Appendix C), as well as highlight the distinction between our approach and methods that reduce sampling steps (Appendix D). Then, we present additional experiments and results, including more detailed ablation studies and comparative experiments (Appendix F). In the end, we provide a statement of the potential broader impact of our work (Appendix H). A Implementation Details As shown in Code. 1, in the standard SD sampling code, adding 3 lines of code with green comments can achieve encoder propagation. Code 1: Encoder propagation for SD (DDIM) from diffusers import StableDiffusionPipeline import torch from utils import register_parallel_pipeline , register_faster_forward # 1. import package model_id = \" runwayml / stable - diffusion -v1 -5\" pipe = StableDiffusionPipeline . from_pretrained ( model_id , torch_dtype = torch . float16 ) pipe = pipe .to(\" cuda \") register_parallel_pipeline ( pipe ) # 2. enable parallel register_faster_forward ( pipe . unet ) # 3. encoder propagation prompt = \"a photo of an astronaut riding a horse on mars \" image = pipe ( prompt ). images [0] image . save (\" astronaut_rides_horse . png \") A.1 Configure We use the Stable Diffuison v1.5 pre-trained model 8 and DeepFloyd-IF 9. All of our inference experiments are conducted using an A40 GPU (48GB of VRAM). We randomly select 100 captions from the MS-COCO 2017 validation dataset [43] as prompts for generating images. The analytical results presented in Sec. 3.2 are based on the statistical outcomes derived from these 100 generated images. A.2 Details about the layers in the UNet The UNet in Stable Diffusion (SD) consists of an encoder E, a bottleneck B, and a decoder D, respectively. We divide the UNet into specific blocks: E = {E(¬∑)s}, B = {B(¬∑)8}, and D = {D(¬∑)s}, where s ‚àà {8, 16, 32, 64}. E(¬∑)s and D(¬∑)s represent the block layers with input resolution s in the encoder and decoder, respectively. Tab. 7 presents detailed information about the block architecture. Fig. 10 illustrates the hierarchical features for these blocks. A.3 Details about the blocks in the DiT 8https://huggingface.co/runwayml/stable-diffusion-v1-5 9https://github.com/deep-floyd/IF 14Table 7: Detailed information about the layers of the encoder E, bottleneck B and decoder D in the UNet of SD. UNet Layer number Type of layer Layer name Input resolution of layer Output resolution of layer E E(¬∑)64 0 resnets down_blocks.0.resnets.0 (320, 64, 64) (320, 64, 64) 1 attention down_blocks.0.attentions.0 (320, 64, 64) (320, 64, 64) 2 resnet down_blocks.0.resnets.1 (320, 64, 64) (320, 64, 64) 3 attention down_blocks.0.attentions.1 (320, 64, 64) (320, 64, 64) 4 downsamplers down_blocks.0.downsamplers.0 (320, 64, 64) (320, 32, 32) E(¬∑)32 5 resnet down_blocks.1.resnets.0 (320, 32, 32) (640, 32, 32) 6 attention down_blocks.1.attentions.0 (640, 32, 32) (640, 32, 32) 7 resnet down_blocks.1.resnets.1 (640, 32, 32) (640, 32, 32) 8 attention down_blocks.1.attentions.1 (640, 32, 32) (640, 32, 32) 9 downsamplers down_blocks.1.downsamplers.0 (640, 32, 32) (640, 16, 16) E(¬∑)16 10 resnet down_blocks.2.resnets.0 (640, 16, 16) (1280, 16, 16) 11 attention down_blocks.2.attentions.0 (1280, 16, 16) (1280, 16, 16) 12 resnet down_blocks.2.resnets.1 (1280, 16, 16) (1280, 16, 16) 13 attention down_blocks.2.attentions.1 (1280, 16, 16) (1280, 16, 16) 14 downsamplers down_blocks.2.downsamplers.0 (1280, 16, 16) (1280, 8, 8) E(¬∑)8 15 resnet down_blocks.3.resnets.0 (1280, 8, 8) (1280, 8, 8) 16 resnet down_blocks.3.resnets.1 (1280, 8, 8) (1280, 8, 8) B B(¬∑)8 17 resnet mid_blocks.resnets.0 (1280, 8, 8) (1280, 8, 8) 18 attention mid_blocks.attentions.0 (1280, 8, 8) (1280, 8, 8) 19 resnet mid_blocks.resnets.1 (1280, 8, 8) (1280, 8, 8) D D(¬∑)8 20 resnet up_blocks.0.resnets.0 (1280+1280, 8, 8) (1280, 8, 8) 21 resnet up_blocks.0.resnets.1 (1280+1280, 8, 8) (1280, 8, 8) 22 resnet up_blocks.0.resnets.2 (1280+1280, 8, 8) (1280, 8, 8) 23 upsamplers up_blocks.0.upsamplers.0 (1280, 8, 8) (1280, 16, 16) D(¬∑)16 24 resnet up_blocks.1.resnets.0 (1280+1280, 16, 16) (1280, 16, 16) 25 attention up_blocks.1.attentions.0 (1280, 16, 16) (1280, 16, 16) 26 resnet up_blocks.1.resnets.1 (1280+1280, 16, 16) (1280, 16, 16) 27 attention up_blocks.1.attentions.1 (1280, 16, 16) (1280, 16, 16) 28 resnet up_blocks.1.resnets.2 (1280+640, 16, 16) (1280, 16, 16) 29 attention up_blocks.1.attentions.2 (1280, 16, 16) (1280, 16, 16) 30 upsamplers up_blocks.1.upsamplers.0 (1280, 16, 16) (1280, 32, 32) D(¬∑)32 31 resnet up_blocks.2.resnets.0 (1280+640, 32, 32) (640, 32, 32) 32 attention up_blocks.2.attentions.0 (640, 32, 32) (640, 32, 32) 33 resnet up_blocks.2.resnets.1 (640+640, 32, 32) (640, 32, 32) 34 attention up_blocks.2.attentions.1 (640, 32, 32) (640, 32, 32) 35 resnet up_blocks.2.resnets.2 (640+320, 32, 32) (640, 32, 32) 36 attention up_blocks.2.attentions.2 (640, 32, 32) (640, 32, 32) 37 upsamplers up_blocks.2.upsamplers.0 (640, 32, 32) (640, 64, 64) D(¬∑)64 38 resnet up_blocks.3.resnets.0 (640+320, 64, 64) (320, 64, 64) 39 attention up_blocks.3.attentions.0 (320, 64, 64) (320, 64, 64) 40 resnet up_blocks.3.resnets.1 (320+320, 64, 64) (320, 64, 64) 41 attention up_blocks.3.attentions.1 (320, 64, 64) (320, 64, 64) 42 resnet up_blocks.3.resnets.2 (320+320, 64, 64) (320, 64, 64) 43 attention up_blocks.3.attentions.2 (320, 64, 64) (320, 64, 64) 15Figure 10: Visualising the hierarchical features. We applied PCA to the hierarchical features following PnP [20] and used the top three leading components as an RGB image for visualization. The encoder features changes minimally and has similarity at many time-steps (left), while the decoder features exhibit substantial variations across different time-steps (right). Figure 11: DiT feature statistics (F-norm) Fig. 12 visualizes the hierarchical features for DiT [ 21] blocks, which includes 28 transformer blocks. Through visualization and statistical anal- ysis (see Fig. 12 and Fig. 11), we observe that the features in the first several transformer blocks change minimally (i.e., the first 18 blocks), similar to the Encoder in SD, while the features in the remaining transformer blocks exhibit substantial variations (i.e., the remaining 10 blocks), akin to the Decoder in SD. For ease of presentation, we refer to the first sev- eral transformer blocks of DiT as the Encoder and the remaining transformer blocks as the Decoder. In Tab. 3, we demonstrate the accelerating performance of our method while applied to DiT-based generation models. A.4 Time and memory consumption ratios We report the run time and GPU memory consumption ratios for text-to-image task. As shown in Tab. 8, we significantly accelerate the diffusion sampling with encoder propagation while maintaining a comparable memory demand to the baselines (Tab. 8 (the last two columns)). Specifically, our proposed method reduces the spending time (s/image) by 24% and requires a little additional memory compared to standard DDIM sampling in SD (DDIM vs. Ours: 2.62GB vs. 2.64GB). The increased GPU memory requirement is for caching the features of the encoder from the previous time-step. Though applying parallel encoder propagation results in an increase in memory requirements by 51%, it leads to a more remarkable acceleration of 41% (DDIM vs. Ours: 2.62GB vs. 3.95GB). In conclusion, applying encoder propagation reduces the sampling time, accompanied by a negligible increase in memory requirements. Parallel encoder propagation on text-to-image tasks yields a sampling speed improvement of 20% to 43%, requiring an additional acceptable amount of memory. Besides the standard text-to-image task, we also validate our proposed approach on other tasks: text-to-video generation(i.e., Text2Video-zero [4] and VideoFusion [5]), personalized generation (i.e., Dreambooth [7] and Custom Diffusion [8]) and reference-guided image generation(i.e., Con- trolNet [10]). We present the time and memory consumption ratios for these tasks in Tab. 9. As reported in Tab. 9 (top), when combined with our method, there is a reduction in sampling time by 23% and 32% for Text2Video-zero [ 4] and VideoFusion [ 5], respectively, while the memory requirements increased slightly by 3% (0.2GB) and 0.9% (0.11GB). The time spent by reference-guided image generation (i.e., ControlNet [ 10]) is reduced by more than 20% with a negligible increase in memory (1%). When integrated with our parallel encoder propagation, the sampling time in this task can be reduced by more than half (51%) (Tab. 9 (middle)). Dreambooth [7] and Custom Diffusion [8] are two approaches for customizing tasks by fine-tuning text-to-image diffusion models. As reported in Tab. 9 (bottom), our method, working in conjunction with the two customization approaches, accelerates the image generation with an acceptable increase in memory. 161st block2nd block 3rd block4th block 5th block6th block 6th block6th block 9th block10th block 11th block12th block 13th block14th block 15th block16th block 17th block18th block19th block 20th block21th block 22th block23th block24th block25th block26th block 27th block28th block t=50 t=49 t=48 t=47 t=46 t=45 t=44 t=43 t=42 t=41 t=40 t=39 t=38 t=37 t=36 t=35 t=34 t=33 t=32 t=31 t=30 t=29 Figure 12: Visualising the hierarchical features. We apply PCA to the hierarchical features following PnP [20], and use the top three leading components as an RGB image for visualization. The output features of the first 18 blocks change slowly, and show similarity at many time steps (top), while the ones in the remaining 10 blocks exhibit substantial variations across different time steps (bottom). Note that our method, either utilizing encoder propagation or parallel encoder propagation, improves sampling speed without compromising image quality (Sec. 4.1 and Sec. 4.3). We 17Table 8: Time and GPU memory consumption ratios in both SD model and DeepFloyd-IF diffu- sion model. ‚Ä†: Encoder propagation, ‚Ä°: Parallel encoder propagation. DM Sampling Method T s/image memory (GB) Stable Diffusion DDIM [13] 50 2.42 2.62 DDIM [13] w/ Ours ‚Ä† 1.8224%‚Üì 2.64 ‚Ä° 1.4241%‚Üì 3.95 DPM-Solver [14] 20 1.14 2.62 DPM-Solver [14] w/ Ours ‚Ä† 0.9219%‚Üì 2.64 ‚Ä° 0.6443%‚Üì 2.69 DPM-Solver++ [47] 20 1.13 2.61 DPM-Solver++ [47] w/ Ours ‚Ä† 0.9119%‚Üì 2.65 ‚Ä° 0.6443%‚Üì 2.68 DDIM + ToMe [35] 50 2.26 2.62 DDIM + ToMe [35] w/ Ours ‚Ä† 1.7224%‚Üì 2.64 ‚Ä° 1.3341%‚Üì 3.95 DeepFloyd-IF DDPM [2] 225 34.55 40.5 DDPM [2] w/ Ours ‚Ä† 29.4515%‚Üì 41.1 ‚Ä° 26.2724%‚Üì 41.1 DPM-Solver++ [47] 100 16.09 40.5 DPM-Solver++ [47] w/ Ours ‚Ä† 14.1312%‚Üì 40.8 ‚Ä° 12.9720%‚Üì 40.8 Table 9: Time and GPU memory consumption ra- tios in text-to-video, personalized generation and reference-guided generated tasks. ‚Ä†: Encoder prop- agation, ‚Ä°: Parallel encoder propagation. Sampling Method T s/image memory (GB) Text2Video-zero [4] 50 13.65 6.59 Text2Video-zero [4] w/ Ours ‚Ä† 10.5423%‚Üì 6.79 ‚Ä° ‚Äì ‚Äì VideoFusion [5] 50 17.93 11.87 VideoFusion [5] w/ Ours ‚Ä† 12.2732%‚Üì 11.98 ‚Ä° ‚Äì ‚Äì ControlNet [10] (edges) 50 3.20 3.81 ControlNet [10] (edges) w/ Ours ‚Ä† 2.0137%‚Üì 3.85 ‚Ä° 1.5251%‚Üì 5.09 ControlNet [10] (scribble) 50 3.95 3.53 ControlNet [10] (scribble) w/ Ours ‚Ä† 3.1820%‚Üì 3.57 ‚Ä° 1.9351%‚Üì 4.45 Dreambooth [7] 50 2.42 2.61 Dreambooth [7] w/ Ours ‚Ä† 1.8124%‚Üì 2.65 ‚Ä° 1.4241%‚Üì 3.93 Custom Diffusion [8] 50 2.42 2.61 Custom Diffusion [8] w/ Ours ‚Ä† 1.8224%‚Üì 2.64 ‚Ä° 1.4241%‚Üì 3.94 conducted GPU memory consumption ratios using the official method provided by PyTorch, torch.cuda.max_memory_allocated 10 , which records the peak allocated memory since the start of the program. A.5 GFLOPs We use the fvcore11 library to calculate the GFLOPs required for a single forward pass of the diffusion model. Multiplying this by the number of sampling steps gives us the total computational burden for sampling one image. Additionally, using fvcore, we can determine the computational load required for each layer of the diffusion model. Based on the key time-steps set in our experiments, we subtract the computation we save from the original total computational load, which then represents the GFLOPs required by our method. Similarly, fvcore also supports parameter count statistics. A.6 Baseline Implementations For the comparisons of text-to-image generation in Sec. 4, we use the official implementation of DPM-Solver [14], DPM-Solver++ [47] 12, and ToMe [35] 13. For the other tasks with text-guided diffusion model, we use the official implementation of Text2Video-zero [4] 14, VideoFusion [5] 15, ControlNet [10] 16, Dreamboth [ 7] 17, and Custom Diffusion [ 8] 18. We maintain the original implementations of these baselines and directly integrate code into their existing implementations to implement our method. 10https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html 11https://github.com/facebookresearch/fvcore 12https://github.com/LuChengTHU/dpm-solver 13https://github.com/dbolya/tomesd 14https://github.com/Picsart-AI-Research/Text2Video-Zero 15https://huggingface.co/docs/diffusers/api/pipelines/text_to_video 16https://github.com/lllyasviel/ControlNet 17https://github.com/google/dreambooth 18https://github.com/adobe-research/custom-diffusion 18Table 10: Model complexity comparison regarding the encoder E, the bottleneck B and the decoder D in terms of parameter count and FLOPs. Parameter (billion) FLOPs (million) E + B 0.25 + 0.097 224.2+6.04 D 0.521.47√ó 504.42.2√ó B Parameter Count and FLOPs of SD We take into account model complexity in terms of parameter count and FLOPs (see Tab. 10). It‚Äôs noteworthy that the decoder D exhibits a significantly greater parameter count, totaling 0.51 billion. This figure is approximately 1.47 times the number of parameter combinations for the encoderE (250 million) and the bottleneck B (97 million). This substantial parameter discrepancy suggests that the decoder D carries a more substantial load in terms of model complexity. Furthermore, when we consider the computational load, during a single forward inference pass of the SD model, the decoder D incurs a considerable 504.4 million FLOPs. This value is notably higher, approximately 2.2 times, than the cumulative computational load of the encoder E, which amounts to 224.2 million FLOPs, and the bottleneck B, which requires 6.04 million FLOPs. This observation strongly implies that the decoder D plays a relatively more important role in processing and transforming the data within the UNet architecture, emphasizing its critical part in the overall functionality of the model. C The methodology for computing Clipscore Clipscore [46] is a metric for computing the consistency between text and image. The formula for computing Clip-score between image embedding v and text embedding c, as presented in the original paper, is as follows: CLIP-S = w ‚àó max(cos(c, v), 0), where a re-scaling operation is performed using w, set to 2.5 in the official implementation of Clipscore [46]. The rationale behind the re-scaling operation, as provided by the official paper (excerpted from [46], Section 3, Footnote 6), is as follows: While the cosine similarity, in theory, can range from[‚àí1, 1] (1) we never observed a negative cosine similarity; and (2) we generally observe values ranging from roughly zero to roughly .4. The particular value of w we advocate for, w = 2.5, attempts to stretch the range of the score distribution to [0, 1]. Therefore, adhering to the aforementioned setting, we employed the official implementation of Clipscore for evaluation, yielding Clipscore data distributed around 0.75. Some works [8, 48] employ re-scaling operations during evaluation, yielding Clipscore values around 0.75, while others [10, 4] do not, resulting in Clipscore values around 0.3. In essence, both approaches are equivalent, differing only in whether re-scaling is applied at the end. We evaluate Clipscore using w = 2.5 for re-scaling, hence yielding Clipscore data around 0.75, unless otherwise stated. D Different from step-reduction methods Several efficient diffusion model solvers, such as DDIM [ 13], DPM-Solver [ 14] and DPM- Slover++ [47], have significantly reduced sampling steps. Our method does not reduce the number of sampling steps. In the encoder propagation, the decoder needs to compute for all time-steps, and we require time embedding inputs for all time-steps to maintain temporal coherence. Fig. 13 illustrates the qualitative comparison results of SD (DDIM) showcasing a reduction in steps (i.e., 9 and 25 time-steps). We present the results with 9 steps because the number of key time-steps for the SD model with DDIM in our approach is 9. The generation quality notably declines with step reduction, attributed to alterations in image structure and a diminished attention to detail generation, 19DDIM (50 steps) Two girls holding flowers and wearing sunglasses Children laughing and laughing happily DDIM (25 steps)DDIM (9 steps) DDIM w/ Ours (50 steps) Figure 13: When lowering the time-steps in inference, the image quality noticeably deteriorates, while ours maintains a similar image quality to the original. 9 steps 9 steps w/ noise injection 25 steps 25 steps w/ noise injection 50 steps 50 steps w/ Ours Figure 14: Fewer sampling steps with noise injection. Table 11: Quantitative comparison for DDIM with fewer steps. Sampling method T FID ‚Üì Clipscore ‚Üë s/image‚Üì DDIM 50 21.75 0.773 2.42 DDIM 25 22.16 0.761 1.54 DDIM w/ noise injection 25 21.89 0.761 1.54 DDIM 9 27.58 0.735 0.96 DDIM w/ noise injection 9 27.63 0.736 0.96 DDIM w/ ours 50 21.08 0.783 1.42 Table 12: Quantitative comparison for DPM- Solver/DPM-solver++ with fewer steps. Sampling method T FID ‚Üì Clipscore ‚Üë DPM-Solver 20 21.36 0.780 DPM-Solver 10 22.41 0.768 DPM-Solver w/ ours 20 21.25 0.779 DPM-Solver++ 20 20.51 0.782 DPM-Solver++ 10 21.25 0.771 DPM-Solver++ w/ ours 20 20.76 0.781 as exemplified by the hands in Fig. 13 (the second cloumn). We increase the number of sampling steps (i.e., 25 steps), and find that the sampling results do not perform as well as DDIM with 50 steps and its variation with FasterDiffusion (Fig. 13 and Tab. 11). As shown in Tab. 11, the FID and Clipscore of SD (DDIM) with 25 time-steps on the MS-COCO 2017 10K subset are 22.16 and 0.761, respectively, significantly worse than the results obtained with 50 time-steps and ours (FID‚Üì: 21.75, 21.08; Clipscore‚Üë: 0.773, 0.783). This demonstrates that our method is not simply reducing the number of sampling steps. With fewer steps (T=10), both DPM-Solver and DPM-Solver++ also 20exhibit worse FID and Clipscore (see Tab. 12). These results indicate that our method achieves better performance compared to simply reducing the sampling steps. As shown on Tab. 11, we conduct DDIM scheduler inference time with various steps. Although FasterDiffusion (the last row) is slightly longer than DDIM scheduler with 9 steps, our sampling results are much closer the 50-step DDIM generation quality, whereas the 9-step DDIM results are much inferior (see Tab. 11, Tab. 12, and Fig. 13 for examples). For directly applying noise injection to the generation phase, we show image generation examples in Fig. 14 in the rebuttal file. Fewer sampling steps equipped with prior noise injection result in almost no improvement in image quality. Tab. 11 further supports this conclusion. While in our case, FasterDiffusion working without the noise injection will lead to smoother textures, as shown in Fig.6. The noise injection technique helps in preserving fidelity in the generated results. E Difference between ‚Äúprior noise injection‚Äù and ‚Äúchurn‚Äù In EDM [49] (Karras et al.), they increase the noise level during ODE sampling to improve determin- istic sampling, which is referred to as stochastic sampling (i.e., ‚Äúchurn‚Äù). Compared to deterministic sampling, it injects noise into the image at each step, which helps to enhance the quality of the generated images. Song et al. [50] first observed that perturbing data with random Gaussian noise makes the data distribution more amenable to score-based generative modeling. Increasing the noise level in EDM is based on Song‚Äôs paper. Their purpose for injecting noise is to perturb the data, whereas our purpose for injecting noise during sampling is to preserve high-frequency details in the image during the denoising process, preventing the diffusion model from removing high-frequency information as noise (see Figure.6 in the main paper). In addition, our method FasterDiffusion differs from them by only inserting noise in the later stage of the diffusion steps, while they insert noise to all time-steps. F Ablation Experiments and Additional Results F.1 Comparion with DeepCache DeepCache is developed based on the observation over the temporal consistency between high-level features. In this paper, we have a more thorough analytical study over the SD model features as shown in Fig.3, where we find out the encoder features change less, whereas the decoder features exhibit substantial variations across different time steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. We further determine the key time steps in the T2I inference stage, which helps our method to skip the time steps in a more scientific way. It is also important to note that DeepCache is not parallelizable on multiple GPUs since DeepCache needs to use all or part of the encoder and decoder at every time step. FasterDiffusion, on the other hand, only uses the encoder at the key time steps, which enables parallel processing at these time steps and faster inference time cost. It is also worth to notice that FasterDiffusion can be further combined with a wide range of diffusion model based tasks, including Text2Video-zero, VideoFusion, Dreambooth, and ControlNet. In this case, DeepCache often shows much slower speed while applied to these tasks. As an example shown in Table. 13, when combined with ControlNet DeepCache is slower by 24% compared to the FasterDiffusion. More specifically, since the ControlNet model requires an additional encoder, our method Faster- Diffusion is able to execute this extra encoder in a parallel manner and reuse it, and that makes the additional time negligible. On the other hand, DeepCache is reusing the decoder feature, which leads the ControlNet to wait for the additional encoder to complete computation at each time-step, resulting in almost no time saved from skipping the encoder in the standard SD models. The T2I generation results are shown in Figure. 15, FasterDiffusion successfully preserves the given structure information and achieves similar results as the original ControlNet model. 21Table 13: When combined with ControlNet (Edge) 50-step DDIM, our inference time shows a significant advantage compared to DeepCache. Clipscore‚Üë FID‚Üì s/image‚Üì ContrlNet 0.769 13.78 3.20 ContrlNet w/ DeepCache 0.765 14.18 1.89 (1.69x) ContrlNet w/ Ours 0.767 14.65 1.52 (2.10x) Input image Canny condition Stable diffusion DeepCache Ours Figure 15: Combine our method with ControlNet. F.2 The definition of key time-steps in various tasks We utilize 50, 20 and 20 time-steps for DDIM scheduler [ 13], DPM-solver [ 14] and DPM- solver++ [47], respectively. We follow the official implementation of DeepFloyd-IF, setting the time-steps for the three sampling stages to 100, 50, and 75, respectively. Text-to-image generation. We experimentally define the key time-steps as tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15 } for SD model with DDIM [ 13], and tkey = {100, 99, 98, . . ., 92, 91, 90, 85, 80, . . ., 25, 20, 15, 14, 13, . . ., 2, 1 }, {50, 49, . . . ,2, 1} and {75, 73, 70, 66, 61, 55, 48, 40, 31, 21, 10} for three stages of DeepFloyd-IF with DDPM [2]. For SD with both DPM-Solver [14] and DPM-Solver++ [47], we experimentally set the key time-steps to tkey = {20, 19, 18, 17, 15, 10, 5}. Other tasks with text-guided diffusion model. In addition to standard text-to-image tasks, we further validate our approach on other tasks with text-guided diffusion model (Sec. 4.2). These tasks are all based on the SD implementation of DDIM [13]. Through experiments, we set the key time-steps to tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15} for Text2Video-zero [4], VideoFusion [5], and ControlNet [10]. For personalized tasks (i.e., Dreambooth [7] and Custom Diffusion [8]), we set the key time steps to tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15, 10}. F.3 The effectiveness of encoder propagation In Sec. 3.2, we have demonstrated that encoder propagation (Fig. 16c) can preserve semantic consistency with standard SD sampling (Fig. 16a). However, images generated through decoder propagation often fail to match certain specific objects mentioned in the text prompt (Fig. 16d and Tab. 14 (the third row)). We extended this strategy to include encoder and decoder propagation (Fig. 16e) as well as decoder and encoder dropping (Fig. 16f). Similarly, encoder and decoder propagation often fail to cover specific objects mentioned in the text prompt, leading to a degradation in the quality of the generated results (Fig. 16e and Tab. 14 (the fourth row)). On the other hand, decoder and encoder dropping are unable to completely denoise, resulting in the generation of images with noise (Fig. 16f and Tab. 14 (the fifth row)). Note that decoder and encoder dropping (Fig. 16f) is different from directly reducing the number of time-steps. Time embedding is required as input for each time-step, even when using only the encoder or decoder at each time-step. F.4 User study details The study participants were volunteers from our college. The questionnaire consisted of 35 questions, each presenting two images: one from the baseline methods (including ControlNet, SD, DeepFloyd- 22(b) UNet architecture (a) Standard SD sampling Sample (c) Encoder propagation Sample (d) Decoder propagation Sample (f) Decoder and encoder dropping Sample (e) Encoder and decoder propagation Sample Figure 16: (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. (d) Decoder propagation. (e) Encoder and decoder propagation. (f) Decoder and encoder dropping. Table 14: Quantitative evaluation for additional strategy on MS-COCO 2017 10K subset. Other propagation strategies can lead to the loss of some semantics under prompts and degradation of image quality (the third to fifth rows). Sampling method T FID ‚Üì Clipscore ‚Üë DDIM 50 21.75 0.773 DDIM w/ Encoder propagation (Ours) 50 21.08 0.783 DDIM w/ Decoder propagation 50 22.97 0.758 DDIM w/ Encoder and decoder propagation 50 23.69 0.742 DDIM w/ Decoder and encoder dropping 50 199.48 0.679 IF, DPM-Solver++, Dreambooth, Custom Diffusion, Text-to-Video, etc.) and the other from our method FasterDiffusion (one example shown in Fig. 17). Users were required to select the image where the target was more accurately portrayed, or choose ‚Äúboth equally good‚Äù. A total of 18 users participated the questionnaire, resulting in totally 630 samples (35 questions √ó 1 option √ó 18 users). As the final results shown in Fig.8, the chosen percentages for SD, DeepFloyd-IF, DPM-Solver++, Text-to-Video, ControlNet, and Personalize were 48%, 44%, 51%, 52%, 46%, and 39%, respectively. These results show that our method performs on par with the baseline methods and demonstrate that FasterDiffusion retains the T2I generation quality while reducing the inference time. F.5 Additional metrics We showcase our experimental results with the ImageReward [51] and PickScore [52] evaluation metrics over the MS-COCO2017 10K dataset, as shown in the Tab. 15. Our method FasterDiffusion enhances sampling efficiency while preserving the original model performance. 23Figure 17: User study examples. ImageReward‚Üë PickScore‚Üë s/image SD (DDIM) 0.149 52.10% 2.42 SD (DDIM) w/ Ours 0.162 47.90% 1.42 Table 15: Metrics in ImageReward and PickScore. F.6 Additional results Fig. 18 shows additional results generated by DiT. In Figs. 19, 20 and 21, we show additional text-to-image generation results. Further results regarding other tasks with text-guided diffusion model are illustrated in Figs. 22, 23 and 24. F.7 Additional tasks ReVersion [53] is a relation inversion method that relies on the Stable Diffusion (SD). Our method retains the capacity to generate images with specific relations based on exemplar images, as illustrated in Fig. 25 (top). P2P [27] is an image editing method guided solely by text. When combined with our approach, it enhances sampling efficiency while preserving the editing effect (Fig. 25 (bottom)). G Automatic selection for key time steps To identify key time steps, we conducted empirical analysis of feature changes at adjacent time steps in multistep Stable Diffusion models (50-step model as an example). This analysis is based on statistics from the distribution of features across 100 random prompts, which does not impose a high time cost. We found that the encoder features change minimally in later time steps, whereas the encoder features in earlier time steps exhibit substantial variations compared to later ones. Based on analysis as shown in Section 3.2, we determine the key time-step as tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15} for the 50-step Stable Diffusion model. As a general case, we also applied this key time-step configuration to the ControlNet [10], Text2Video-zero [4], VideoFusion [5], Dreambooth [7] and Custom Diffusion [8] models based on the 50-step SD models. That will not impose any additional searching time for the key time steps for these downstream application scenarios. Existing methods such as OMS-DPM [54], AutoDiffusion [55], and DDSM [56] utilize reinforcement learning or machine learning algorithms to search for optimal schedules, time steps, and model sizes. Each search iteration involves frequent image generation and FID calculations. Executing these search algorithms is time-consuming, often exceeding the training time, as noted in DDSM. For instance, the NSGA-II search algorithm used in DDSM incurs a search cost approximately 1.1 to 1.2 times that of training a standard diffusion model. We plan to incorporate the NSGA-II algorithm for automatically searching tkey time steps as an enhancement to FasterDiffusion in our future research. H Impact Statements Diffusion models generate realistic fake images, which assist individuals in their creative endeavors. However, for those lacking the ability to discern whether the images are generated by a model, it may affect their judgment. 24Figure 18: Additional results of DiT (top) and this method in conjunction with our proposed approach (bottom). Equipped with our technique, using DMs on the edge devices becomes feasible. Also, importantly, our proposed acceleration of DMs does not require access to enormous compute, which is required to the methods based on knowledge distillation [18, 17, 19]. This makes our technique also useful for the many smaller actors in the generative AI field. 25DDIMDDIM w/ Ours A dew-covered spider web in morning sunlight A snow-covered pine tree in a moonlit forest colorful autumn leaves scattered on a stone pathway A cat wearing a sunglasses An astronaut riding a horse on Mars A dog sitting in a beach A vase sitting on top of a table with flowers in it a banana and a chocolate frosted donut sitting in a baggie DPM-Solver w/ Ours A plate full of sushi with more behind it A man with glasses sitting in front of a laptop computer A passenger bus pulling up to the side of a street A bowl full of fresh green apples are kept DPM-Solver A lone sailboat drifting on calm waters French bread on a plate with eggs bacon DPM-Solver++DPM-Solver++ w/ Ours Plate containing bread covered in some cooked broccoli Yoshua Bengio with beard A black and white cat relaxing inside a laptop A floral centerpiece in the dining room table setting Figure 19: Additional results of text-to-image generation combining SD with DDIM [ 13], DPM- Solver [14], DPM-Solver++ [47], and these methods in conjunction with our proposed approach. 26DDIM+ToMeDDIM+ToMe w/ Ours Burger and fries A elephant under tree A woman with glasses standing on the street Waterfall surrounded with trees A blue bird perched on top of a tree branch A frog sitting on green field Figure 20: Additional results of text-to-image generation combining SD with DDIM+ToMe [35], and this method in conjunction with our proposed approach. A kangaroo holding a sign that says \"very deep learning\" A pair of hands kneading bread dough DDPM w/ Ours A sunbeam streaming through a dense forest canopy A cozy fireplace with crackling flames A stack of vinyl records on a vintage turntable A pair of reading glasses on an open book DDPM A busy highway is being viewed from a distance. A black and yellow fire hydrant on a city street DPM-Solver++DPM-Solver++ w/ Ours A airplane coming in for a landing with a full moon above it  A bench sitting on to of a field of tall grass near water Vintage bicycle leaning against a brick wall A big purple public bus called south tyne Figure 21: Additional results of text-to-image generation combining DeepFloyd-IF with DDPM [2] and DPM-Solver++ [47], and these methods in conjunction with our proposed approach. 27The sun is setting behind the mountains Text2Video-zeroText2Video-zero w/ Ours A pot of water is boiling on the stove Text2Video-zeroText2Video-zero w/ Ours A panda is playing guitar on times square Text2Video-zeroText2Video-zero w/ Ours The tide is coming in on the sandy beach Text2Video-zeroText2Video-zero w/ Ours Figure 22: Additional results of Text2Video-zero [4] both independently and when combined with our proposed method. 28Fireworks bloom in the night sky VideoFusion VideoFusion w/ Ours The tide is coming in on the sandy beach VideoFusionText2Video-zero w/ Ours Figure 23: Additional results of VideoFusion [5] both independently and when combined with our proposed method. 29Custom Diffusion w/ Ours A chair in the middle of a garden  A chair next to a cozy fireplace A motorbike by the ocean  A racer on a motorbike at a competition Custom Diffusion Dreambooth w/ OursDreambooth ControlNet ConditionControlNet w/ Ours Figure 24: Additional results of ControlNet [10] (top) and personalized tasks [7, 8] (bottom) obtained both independently and in conjunction with our proposed method.. 30Zoom photo of treesZoom photo of flowers P2PP2P w/ Ours A painting of a squirrel eating a burger A cat sitting on a grassland A tiger sitting on a grassland A painting of a lion eating a burger ReVersion w/ Ours cat <R> stone <R>=\"painted on\" cat <R> cat <R>=\"back to back\" ReVersion spiderman <R> stone <R>=\"painted on\" Figure 25: Edited results of ReVersion [53] (top) and P2P [27] (bottom) obtained both independently and in conjunction with our proposed method. 31NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs contributions and scope? Answer: [Yes] Justification: See Abstract and Introduction (Section 1). Guidelines: ‚Ä¢ The answer NA means that the abstract and introduction do not include the claims made in the paper. ‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. ‚Ä¢ The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 5. Guidelines: ‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ‚Ä¢ The authors are encouraged to create a separate \"Limitations\" section in their paper. ‚Ä¢ The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. ‚Ä¢ The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. ‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ‚Ä¢ While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren‚Äôt acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 32Justification: The paper does not include theoretical results. Guidelines: ‚Ä¢ The answer NA means that the paper does not include theoretical results. ‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. ‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems. ‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. ‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 4 and Appendix A. We will release our code to the public. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? 33Answer: [Yes] Justification: We will provide our code in supplemental material. Guidelines: ‚Ä¢ The answer NA means that paper does not include experiments requiring code. ‚Ä¢ Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ‚Ä¢ While we encourage the release of code and data, we understand that this might not be possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ‚Ä¢ The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. ‚Ä¢ The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. ‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ‚Ä¢ Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 4 for the experimental setting. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Figure 3. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ‚Ä¢ The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) ‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). ‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error of the mean. 34‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. ‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Tables 1, 4, 5, 8 and 9. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ‚Ä¢ The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ‚Ä¢ The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn‚Äôt make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines. Guidelines: ‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ‚Ä¢ If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix H. Guidelines: ‚Ä¢ The answer NA means that there is no societal impact of the work performed. ‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 35‚Ä¢ The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ‚Ä¢ The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: ‚Ä¢ The answer NA means that the paper poses no such risks. ‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite papers that we used in Section 4. The models, datasets, and codes used in the paper are introduced in detail in Appendix A.1 and Appendix A.6. Guidelines: ‚Ä¢ The answer NA means that the paper does not use existing assets. ‚Ä¢ The authors should cite the original paper that produced the code package or dataset. ‚Ä¢ The authors should state which version of the asset is used and, if possible, include a URL. ‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. ‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. ‚Ä¢ If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. ‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. 36‚Ä¢ If this information is not available online, the authors are encouraged to reach out to the asset‚Äôs creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide our source code as an anonymized zip file in the supplemental material. Guidelines: ‚Ä¢ The answer NA means that the paper does not release new assets. ‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ‚Ä¢ The paper should discuss whether and how consent was obtained from people whose asset is used. ‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: See User Study in Section 4.3. Guidelines: ‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. ‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. ‚Ä¢ We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ‚Ä¢ For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 37",
      "meta_data": {
        "arxiv_id": "2312.09608v2",
        "authors": [
          "Senmao Li",
          "Taihang Hu",
          "Joost van de Weijer",
          "Fahad Shahbaz Khan",
          "Tao Liu",
          "Linxuan Li",
          "Shiqi Yang",
          "Yaxing Wang",
          "Ming-Ming Cheng",
          "Jian Yang"
        ],
        "published_date": "2023-12-15T08:46:43Z",
        "pdf_url": "https://arxiv.org/pdf/2312.09608v2.pdf",
        "github_url": "https://github.com/dbolya/tomesd"
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the slow inference time of diffusion models by proposing a novel acceleration method without requiring knowledge distillation or retraining. The key findings are that UNet encoder features change minimally across time-steps, while decoder features vary substantially. Based on this, the paper introduces an encoder propagation strategy that enables parallel computation of decoder steps and a prior noise injection method to improve texture details, significantly accelerating image generation across various tasks.",
        "methodology": "The methodology is based on an empirical analysis of UNet encoder and decoder features, revealing minimal variation in encoder features and substantial variation in decoder features across time-steps. This insight motivates 'encoder propagation,' where encoder computations are omitted at certain adjacent non-key time-steps, and pre-computed encoder features from previous key time-steps are reused as input for multiple subsequent decoder steps. These multiple decoder steps can then be processed in parallel. Both uniform and non-uniform strategies for selecting key time-steps are explored, with non-uniform performing better. Additionally, a prior noise injection method (combining initial latent code zT into subsequent steps from a specific time-step œÑ) is introduced to preserve texture details.",
        "experimental_setup": "The approach was validated on diverse tasks including standard text-to-image generation, text-to-video generation (Text2Video-zero, VideoFusion), personalized image generation (Dreambooth, Custom Diffusion), and reference-guided image generation (ControlNet). Models evaluated include Stable Diffusion (SD), DeepFloyd-IF, and Diffusion Transformer (DiT). For text-to-image, 10K prompts from the MS-COCO2017 validation dataset were used. For DiT, 50K images were generated from 1000 ImageNet class labels. Evaluation metrics included Fr√©chet Inception Distance (FID), Clipscore, computational workload (GFLOPs/image), and sampling time (s/image). For DiT, additional metrics like sFID, IS, Precision, and Recall were used. A user study with pairwise comparisons (18 users, 35 pairs) was conducted. All experiments were performed on an A40 GPU (48GB of VRAM). The method was combined with existing solvers like DDIM, DPM-Solver, DPM-Solver++, and ToMe, and compared against DeepCache and CacheMe.",
        "limitations": "The method faces challenges in generating high-quality images when using a very limited number of sampling steps (e.g., 5). The proposed parallelization, while potentially applicable to network distillation approaches, was not explored in this paper.",
        "future_research_directions": "Future research directions include exploring the application of the proposed parallelization strategy to network distillation approaches. Additionally, incorporating algorithms like NSGA-II for automatically searching optimal key time-steps is planned as an enhancement to FasterDiffusion.",
        "experimental_code": "import torch\nfrom typing import Tuple, Callable\n\ndef do_nothing(x: torch.Tensor, mode:str=None):\n    return x\n\ndef mps_gather_workaround(input, dim, index):\n    if input.shape[-1] == 1:\n        return torch.gather(\n            input.unsqueeze(-1),\n            dim - 1 if dim < 0 else dim,\n            index.unsqueeze(-1)\n        ).squeeze(-1)\n    else:\n        return torch.gather(input, dim, index)\n\ndef bipartite_soft_matching_random2d(metric: torch.Tensor,\n                                     w: int, h: int, sx: int, sy: int, r: int,\n                                     no_rand: bool = False,\n                                     generator: torch.Generator = None) -> Tuple[Callable, Callable]:\n    B, N, _ = metric.shape\n    if r <= 0:\n        return do_nothing, do_nothing\n    gather = mps_gather_workaround if metric.device.type == \"mps\" else torch.gather\n    with torch.no_grad():\n        hsy, wsx = h // sy, w // sx\n        if no_rand:\n            rand_idx = torch.zeros(hsy, wsx, 1, device=metric.device, dtype=torch.int64)\n        else:\n            rand_idx = torch.randint(sy*sx, size=(hsy, wsx, 1), device=generator.device, generator=generator).to(metric.device)\n        idx_buffer_view = torch.zeros(hsy, wsx, sy*sx, device=metric.device, dtype=torch.int64)\n        idx_buffer_view.scatter_(dim=2, index=rand_idx, src=-torch.ones_like(rand_idx, dtype=rand_idx.dtype))\n        idx_buffer_view = idx_buffer_view.view(hsy, wsx, sy, sx).transpose(1, 2).reshape(hsy * sy, wsx * sx)\n        if (hsy * sy) < h or (wsx * sx) < w:\n            idx_buffer = torch.zeros(h, w, device=metric.device, dtype=torch.int64)\n            idx_buffer[:(hsy * sy), :(wsx * sx)] = idx_buffer_view\n        else:\n            idx_buffer = idx_buffer_view\n        rand_idx = idx_buffer.reshape(1, -1, 1).argsort(dim=1)\n        del idx_buffer, idx_buffer_view\n        num_dst = hsy * wsx\n        a_idx = rand_idx[:, num_dst:, :] # src\n        b_idx = rand_idx[:, :num_dst, :] # dst\n        def split(x):\n            C = x.shape[-1]\n            src = gather(x, dim=1, index=a_idx.expand(B, N - num_dst, C))\n            dst = gather(x, dim=1, index=b_idx.expand(B, num_dst, C))\n            return src, dst\n        metric = metric / metric.norm(dim=-1, keepdim=True)\n        a, b = split(metric)\n        scores = a @ b.transpose(-1, -2)\n        r = min(a.shape[1], r)\n        node_max, node_idx = scores.max(dim=-1)\n        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n        dst_idx = gather(node_idx[..., None], dim=-2, index=src_idx)\n    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n        src, dst = split(x)\n        n, t1, c = src.shape\n        unm = gather(src, dim=-2, index=unm_idx.expand(n, t1 - r, c))\n        src = gather(src, dim=-2, index=src_idx.expand(n, r, c))\n        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n        return torch.cat([unm, dst], dim=1)\n    def unmerge(x: torch.Tensor) -> torch.Tensor:\n        unm_len = unm_idx.shape[1]\n        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n        _, _, c = unm.shape\n        src = gather(dst, dim=-2, index=dst_idx.expand(B, r, c))\n        out = torch.zeros(B, N, c, device=x.device, dtype=x.dtype)\n        out.scatter_(dim=-2, index=b_idx.expand(B, num_dst, c), src=dst)\n        out.scatter_(dim=-2, index=gather(a_idx.expand(B, a_idx.shape[1], 1), dim=1, index=unm_idx).expand(B, unm_len, c), src=unm)\n        out.scatter_(dim=-2, index=gather(a_idx.expand(B, a_idx.shape[1], 1), dim=1, index=src_idx).expand(B, r, c), src=src)\n        return out\n    return merge, unmerge\n\nimport math\nfrom typing import Type, Dict, Any\nfrom . import merge\nfrom .utils import isinstance_str, init_generator\n\ndef compute_merge(x: torch.Tensor, tome_info: Dict[str, Any]) -> Tuple[Callable, ...]:\n    original_h, original_w = tome_info[\"size\"]\n    original_tokens = original_h * original_w\n    downsample = int(math.ceil(math.sqrt(original_tokens // x.shape[1])))\n    args = tome_info[\"args\"]\n    if downsample <= args[\"max_downsample\"]:\n        w = int(math.ceil(original_w / downsample))\n        h = int(math.ceil(original_h / downsample))\n        r = int(x.shape[1] * args[\"ratio\"])\n        if args[\"generator\"] is None:\n            args[\"generator\"] = init_generator(x.device)\n        elif args[\"generator\"].device != x.device:\n            args[\"generator\"] = init_generator(x.device, fallback=args[\"generator\"])\n        use_rand = False if x.shape[0] % 2 == 1 else args[\"use_rand\"]\n        m, u = merge.bipartite_soft_matching_random2d(x, w, h, args[\"sx\"], args[\"sy\"], r, \n                                                      no_rand=not use_rand, generator=args[\"generator\"])\n    else:\n        m, u = (merge.do_nothing, merge.do_nothing)\n    m_a, u_a = (m, u) if args[\"merge_attn\"]      else (merge.do_nothing, merge.do_nothing)\n    m_c, u_c = (m, u) if args[\"merge_crossattn\"] else (merge.do_nothing, merge.do_nothing)\n    m_m, u_m = (m, u) if args[\"merge_mlp\"]       else (merge.do_nothing, merge.do_nothing)\n    return m_a, m_c, m_m, u_a, u_c, u_m\n\ndef make_diffusers_tome_block(block_class: Type[torch.nn.Module]) -> Type[torch.nn.Module]:\n    class ToMeBlock(block_class):\n        _parent = block_class\n        def forward(\n            self,\n            hidden_states,\n            attention_mask=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            timestep=None,\n            cross_attention_kwargs=None,\n            class_labels=None,\n        ) -> torch.Tensor:\n            m_a, m_c, m_m, u_a, u_c, u_m = compute_merge(hidden_states, self._tome_info)\n            # ... (rest of the forward method applying merge/unmerge functions to attention and MLP outputs) ...\n            if self.use_ada_layer_norm:\n                norm_hidden_states = self.norm1(hidden_states, timestep)\n            elif self.use_ada_layer_norm_zero:\n                norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(\n                    hidden_states, timestep, class_labels, hidden_dtype=hidden_states.dtype\n                )\n            else:\n                norm_hidden_states = self.norm1(hidden_states)\n            norm_hidden_states = m_a(norm_hidden_states)\n            cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}\n            attn_output = self.attn1(\n                norm_hidden_states,\n                encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None,\n                attention_mask=attention_mask,\n                **cross_attention_kwargs,\n            )\n            hidden_states = u_a(attn_output) + hidden_states\n            if self.attn2 is not None:\n                norm_hidden_states = (\n                    self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n                )\n                norm_hidden_states = m_c(norm_hidden_states)\n                attn_output = self.attn2(\n                    norm_hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                    attention_mask=encoder_attention_mask,\n                    **cross_attention_kwargs,\n                )\n                hidden_states = u_c(attn_output) + hidden_states\n            norm_hidden_states = self.norm3(hidden_states)\n            if self.use_ada_layer_norm_zero:\n                norm_hidden_states = norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]\n            norm_hidden_states = m_m(norm_hidden_states)\n            ff_output = self.ff(norm_hidden_states)\n            if self.use_ada_layer_norm_zero:\n                ff_output = gate_mlp.unsqueeze(1) * ff_output\n            hidden_states = u_m(ff_output) + hidden_states\n            return hidden_states\n    return ToMeBlock\n\ndef apply_patch(\n        model: torch.nn.Module,\n        ratio: float = 0.5,\n        max_downsample: int = 1,\n        sx: int = 2, sy: int = 2,\n        use_rand: bool = True,\n        merge_attn: bool = True,\n        merge_crossattn: bool = False,\n        merge_mlp: bool = False):\n    remove_patch(model)\n    is_diffusers = isinstance_str(model, \"DiffusionPipeline\") or isinstance_str(model, \"ModelMixin\")\n    if not is_diffusers:\n        if not hasattr(model, \"model\") or not hasattr(model.model, \"diffusion_model\"):\n            raise RuntimeError(\"Provided model was not a Stable Diffusion / Latent Diffusion model, as expected.\")\n        diffusion_model = model.model.diffusion_model\n    else:\n        diffusion_model = model.unet if hasattr(model, \"unet\") else model\n    diffusion_model._tome_info = {\n        \"size\": None,\n        \"hooks\": [],\n        \"args\": {\n            \"ratio\": ratio,\n            \"max_downsample\": max_downsample,\n            \"sx\": sx, \"sy\": sy,\n            \"use_rand\": use_rand,\n            \"generator\": None,\n            \"merge_attn\": merge_attn,\n            \"merge_crossattn\": merge_crossattn,\n            \"merge_mlp\": merge_mlp\n        }\n    }\n    hook_tome_model(diffusion_model)\n    for _, module in diffusion_model.named_modules():\n        if isinstance_str(module, \"BasicTransformerBlock\"):\n            make_tome_block_fn = make_diffusers_tome_block if is_diffusers else make_tome_block\n            module.__class__ = make_tome_block_fn(module.__class__)\n            module._tome_info = diffusion_model._tome_info\n            if not hasattr(module, \"disable_self_attn\") and not is_diffusers:\n                module.disable_self_attn = False\n            if not hasattr(module, \"use_ada_layer_norm_zero\") and is_diffusers:\n                module.use_ada_layer_norm = False\n                module.use_ada_layer_norm_zero = False\n    return model\n",
        "experimental_info": "The provided repository content implements \"Token Merging for Stable Diffusion\" (ToMe), which is distinct from the \"encoder propagation\" and \"prior noise injection\" methods described in the prompt. The experimental settings related to the ToMe method are configured via the `apply_patch` function with the following parameters:\n- `ratio` (float): Determines the proportion of tokens to merge, directly impacting speedup and visual quality. E.g., 0.4 reduces tokens by 40%.\n- `max_downsample` (int): Specifies the maximum downsampling level of UNet layers to which ToMe is applied. Options are 1, 2, 4, or 8, indicating the scale of downsampling. Higher values apply ToMe to more layers.\n- `sx`, `sy` (int): Define the stride in the x and y dimensions used for selecting destination tokens in the bipartite soft matching algorithm. The default is (2, 2).\n- `use_rand` (bool): If true, enables random perturbations during the computation of destination token sets. Typically recommended to be on.\n- `merge_attn` (bool): Controls whether token merging is applied to self-attention layers. Recommended to be true.\n- `merge_crossattn` (bool): Controls whether token merging is applied to cross-attention layers. Not generally recommended to be true.\n- `merge_mlp` (bool): Controls whether token merging is applied to Multi-Layer Perceptron (MLP) blocks. Generally not recommended to be true.\n\nThe `tomesd` repository focuses on spatially reducing tokens within transformer blocks for efficiency, which is a different mechanism than the temporal feature reuse and noise injection described in the \"Method\" section of the prompt."
      }
    },
    {
      "title": "Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis"
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?\",\n    \"Methods\": \"Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides‚Äîper time-step t and per network block k‚Äîwhether the block is executed.  \\n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk¬∑h(t)), where h(t)=1‚àít (so gates tend to stay open near the data region).  \\n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk‚àà{0,1}) at inference time.  \\n3. Loss=Lnoise+Œª‚ãÖŒ£k gÃÖk, where Lnoise is the standard noise-prediction loss and gÃÖk is the average gate activation over the batch; Œª is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \\n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.\",\n    \"Experimental Setup\": \"Model: the public DDPM CIFAR-10 UNet (32√ó32).\\nBaselines: (a) original model, (b) ASE with the paper‚Äôs linear schedule, (c) proposed Auto-ASE.\\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; Œª=0.05.\\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.\",\n    \"Experimental Code\": \"import torch, torch.nn as nn\\n\\nclass GatedBlock(nn.Module):\\n    def __init__(self, block):\\n        super().__init__()\\n        self.block = block\\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\\n    def forward(self, x, t):\\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\\n        if self.training:\\n            return x + gate * (self.block(x, t) - x), gate.mean()\\n        else:  # STE binarisation\\n            hard_gate = (gate > 0.5).float()\\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\\n\\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\\n\\ngate_loss_coeff = 0.05\\n\\nfor x, t, noise in dataloader:\\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\\n    loss.backward(); optimizer.step()\",\n    \"Expected Result\": \"Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t‚âà0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (‚âà16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.\",\n    \"Expected Conclusion\": \"By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration.\"\n}",
    "experimental_design": {
      "experiment_strategy": "Goal: Build a single, rigorous experimental play-book that can be applied to every study of Auto-ASE, proving that the learnable gating schedule (a) preserves or improves sample quality, (b) reduces sampling cost, (c) is robust across models, datasets and solvers, and (d) removes the need for manual tuning.\n\n1. Key hypotheses to validate\n   H1  Performance: Auto-ASE delivers the same or better generative quality than the original network and the hand-crafted ASE schedule.\n   H2  Efficiency: Auto-ASE cuts wall‚Äìclock sampling time, FLOPs and GPU energy by at least 20 % versus the full model and beats the best fixed schedule.\n   H3  Robustness & Generalisation: The learned schedule works (without re-tuning) on multiple datasets, resolutions, network backbones and numerical solvers.\n   H4  Simplicity: Training overhead and parameter growth are negligible (<0.5 % extra parameters, <5 % extra training time).\n\n2. Universal comparison set\n   a. Baselines\n      ‚Ä¢ Full model (no skipping)\n      ‚Ä¢ Original ASE with its published dropping rule\n   b. State-of-the-art accelerators that keep the backbone intact (e.g. DDIM, DPM-Solver++, PNDM) to show trade-off curves\n   c. Ablations of Auto-ASE\n      ‚Ä¢ No sparsity loss (Œª = 0)\n      ‚Ä¢ Shared vs individual gates\n      ‚Ä¢ Different gate shapes h(t)\n      ‚Ä¢ Soft-gating at inference (no STE)\n\n3. Evaluation axes (applied in every experiment)\n   Quantitative quality: FID, KID, IS (for images) or task-specific metrics\n   Qualitative quality: curated sample grids + human Turing test where feasible\n   Efficiency:  (i) average number of executed blocks, (ii) total UNet/Transformer calls, (iii) wall-clock latency, (iv) TFLOPs, (v) peak & average GPU memory, (vi) Joule/1000 images measured by nvidia-smi\n   Robustness: metric distributions across 3 random seeds and across 3 Œª values\n   Schedule insight: plots of g_k(t) heat-maps & histogram of closed gates\n\n4. Experimental matrix (re-used each time)\n   Tier-1  In-domain sanity: original public UNet √ó CIFAR-10 √ó DDPM solver (50 steps)\n   Tier-2  Cross-architecture: replace UNet by DiT-XL/2 or ADM-KD while keeping CIFAR-10\n   Tier-3  Cross-data & resolution: ImageNet-64, LSUN-256, Stable-Diffusion 512√ó512 latent UNet; keep the same Auto-ASE hyper-parameters\n   Tier-4  Cross-solver: DDIM 25 steps, DPM-Solver++ 15 steps, PLMS 50 steps\n   Tier-5  Stress tests: (i) halve/ double Œª, (ii) prune 70 % of blocks, (iii) deploy on corrupted noise schedules\n\n5. Success criteria (must be met on every Tier-1 & Tier-2 run, and on ‚â•75 % of Tier-3/4/5 runs)\n   ‚Ä¢ ŒîFID ‚â§ +0.5 (or KID ‚â§ +0.002) relative to full model\n   ‚Ä¢ ‚â•20 % speed-up vs full model; ‚â•5 % extra speed-up vs best fixed ASE schedule\n   ‚Ä¢ <0.5 % parameter growth; <5 % extra training time\n   ‚Ä¢ For robustness tiers: variance of ŒîFID across seeds ‚â§ 0.8 and no catastrophic failure (FID < √ó1.5 of baseline)\n\n6. Measurement protocol\n   ‚Ä¢ All runs on a single NVIDIA A100-80GB, PyTorch compiled with CUDA 11.8 + cuDNN 8.\n   ‚Ä¢ Disable CPU-GPU overlaps to obtain deterministic timing; average over 1 k samples after 50 warm-ups.\n   ‚Ä¢ Log with Weights & Biases to expose full metrics, curves and gate heat-maps.\n\n7. Reporting template (identical for all papers/sections)\n   Table 1  Quality & efficiency metrics for Baselines vs Auto-ASE\n   Figure 1 Trade-off curve: FID vs wall-clock time\n   Figure 2 Gate activation heat-map g_k(t)\n   Table 2 Ablation results\n   Appendix: energy profile & hardware counters\n\nBy adhering to this uniform strategy, each experiment will systematically attack the four key hypotheses with controlled baselines, rich metrics and clear success thresholds, allowing reviewers to judge Auto-ASE‚Äôs effectiveness from multiple, reproducible perspectives.",
      "experiments": [
        {
          "experiment_id": "exp-1-main-perf-eff",
          "run_variations": [
            "full-ddpm",
            "ase-linear",
            "auto-ase",
            "auto-ase-soft",
            "auto-ase-no-sparsity"
          ],
          "description": "Objective / hypothesis: Validate H1 (quality) and H2 (efficiency) on Tier-1 (CIFAR-10 32√ó32) and Tier-2 (cross-architecture on DiT-XL/2).\n\nModels:\n ‚Ä¢ DDPM public UNet-32 (baseline backbone)\n ‚Ä¢ DiT-XL/2 (Transformer backbone, 32√ó32) for cross-architecture check\n\nDatasets:\n ‚Ä¢ CIFAR-10 (train 45k / val 5k / test 10k). No label-conditioning.\nPre-processing: random horizontal flip 0.5, map to [-1,1], no resize. Stats cached in .npy to avoid CPU bottleneck.\n\nData split & repetition: 3 random seeds. Train on train set, validate on val every 2 K iters, early-stop on best-val FID. Report test metrics averaged over seeds ¬± œÉ.\n\nRun variations:\n 1. full-ddpm ‚Äì original UNet, 50 sampling steps.\n 2. ase-linear ‚Äì hand-crafted dropping rule from ASE paper (same #steps).\n 3. auto-ase ‚Äì proposed learnable gates + STE at inference, Œª=0.05.\n 4. auto-ase-soft ‚Äì gates kept continuous (no STE) at inference to probe quality/latency trade-off.\n 5. auto-ase-no-sparsity ‚Äì Œª=0, tests necessity of L1 regulariser.\nAll runs fine-tune 1 epoch with AdamW lr=1e-4, batch 128.\n\nEvaluation metrics:\n Primary ‚Äì FID (10 k test images), KID (√ó10¬≥), IS.\n Secondary ‚Äì avg. executed blocks, wall-clock latency/img, TFLOPs/img (torch.profiler), peak GPU-mem, nvidia-smi energy (J).\n\nEfficiency accounting: collect CUDA events over 1 k samples after 50 warm-ups; disable cudnn benchmarking.\n\nComputational cost: record train time/epoch and extra params (%).\n\nHyper-parameter probe: additional grid lr‚àà{5e-5,1e-4,2e-4} for auto-ase (reported in appendix).\n\nExample code snippet (partial):\n```\nwith torch.autocast('cuda'):\n    start = torch.cuda.Event(enable_timing=True)\n    end   = torch.cuda.Event(enable_timing=True)\n    start.record(); imgs = sampler(model, 50); end.record();\n    torch.cuda.synchronize(); elapsed_ms = start.elapsed_time(end)\n```\n\nSuccess criteria: ŒîFID ‚â§+0.5 vs full, ‚â•20 % speed-up vs full, ‚â•5 % vs ase-linear. Results populate Table 1, Fig. 1 trade-off, Fig. 2 heat-map.\n\nBranch: feature/exp-1-main-perf-eff",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-1-main-perf-eff"
          },
          "code": {
            "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss ‚Äì {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with real dataset support.\n\nThis module now contains fully-fledged dataloader logic for CIFAR-10 via the\nHugging Face datasets hub (dataset id: \"uoft-cs/cifar10\").  A lightweight\n`FakeData` fallback remains for CI / smoke tests.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets as tv_datasets, transforms\n\n# We lazily import HF datasets to avoid the dependency cost when running only\n# smoke tests (which use torchvision FakeData).  ImportError will propagate if\n# a real HF dataset is requested without the package installed.\ntry:\n    from datasets import load_dataset\nexcept ModuleNotFoundError:  # pragma: no cover ‚Äì handled at runtime\n    load_dataset = None  # type: ignore\n\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef cifar10_transforms() -> transforms.Compose:\n    \"\"\"Standard CIFAR-10 data augmentation + mapping to [-1, 1].\"\"\"\n    tfms: List[Any] = [\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # [0,1] -> [-1,1]\n    ]\n    return transforms.Compose(tfms)\n\n\ndef dummy_transforms(image_size=(3, 32, 32)) -> transforms.Compose:\n    tfms: List[Any] = [\n        transforms.ToTensor(),\n    ]\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HF Dataset wrappers                                                       #\n# ------------------------------------------------------------------------- #\n\nclass HFImageDataset(Dataset):\n    \"\"\"Thin wrapper converting a Hugging Face dataset into a PyTorch dataset.\"\"\"\n\n    def __init__(self, hf_ds, tfms):\n        self.ds = hf_ds\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        sample = self.ds[idx]\n        # The exact field name can vary (\"img\"|\"image\") ‚Äì we try both.\n        img = sample.get(\"img\", None)\n        if img is None:\n            img = sample.get(\"image\", None)\n        if img is None:\n            raise KeyError(\"Expected image field 'img' or 'image' in HF dataset but neither found.\")\n        if self.tfms:\n            img = self.tfms(img)\n        # We return a dummy label to keep the 2-tuple contract expected by the\n        # training pipeline (image, target).\n        return img, 0\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance for the requested dataset.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Smoke-test / CI dataset                                            #\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return tv_datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=dummy_transforms(image_size),\n        )\n\n    # ------------------------------------------------------------------ #\n    # CIFAR-10 (HuggingFace)                                             #\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        if load_dataset is None:\n            raise ImportError(\n                \"The 'datasets' package is required for CIFAR-10.  Please install it via pip install datasets\"\n            )\n        split = \"train\" if train else \"test\"\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFImageDataset(hf_ds, cifar10_transforms())\n\n    # ---------------------------- fallback ---------------------------- #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader",
            "model_py": "# src/model.py\n\"\"\"Model architectures for the Auto-ASE experiments.\n\nImplemented variants:\n  ‚Ä¢ baseline_unet        ‚Äì standard UNet (no gating)\n  ‚Ä¢ ase_linear           ‚Äì fixed, hand-crafted linear gate schedule (not trainable)\n  ‚Ä¢ auto_ase             ‚Äì learnable gates + STE binarisation at inference\n  ‚Ä¢ auto_ase_soft        ‚Äì learnable gates, *no* STE (soft gates at inference)\n\nThe UNet backbone is purposely compact to keep the repository lightweight, yet\nit captures all core ingredients (time embeddings, skip connections, Auto-ASE\nlogic, etc.).\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple, Literal\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Positional / sinusoidal time embedding                                    #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Sinusoidal time embeddings (DDPM/ADM style).\"\"\"\n    half_dim = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half_dim, device=timesteps.device) / half_dim)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = F.pad(emb, (0, 1))  # Zero-pad for odd dim\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# Gate wrappers                                                             #\n# ------------------------------------------------------------------------- #\n\nclass LearnableGate(nn.Module):\n    \"\"\"Auto-ASE learnable gate with optional STE at inference.\"\"\"\n\n    def __init__(self, t_dim: int, ste_inference: bool = True):\n        super().__init__()\n        self.w = nn.Parameter(torch.zeros(1))  # Initialised so sigmoid ‚âà 0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.ste_inference = ste_inference\n\n    def forward(self, temb: torch.Tensor, training: bool):\n        # h(t)=1-sigmoid(linear(t)) adheres to the Auto-ASE design doc.\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        if training or not self.ste_inference:\n            return gate_cont\n        # Inference + STE\n        return (gate_cont > 0.5).float()\n\n\nclass FixedLinearGate(nn.Module):\n    \"\"\"Hand-crafted linear gate schedule from ASE paper (not trainable).\n\n    The keep ratio for block *k* at normalised time *tÃÇ* is\n        g_k(tÃÇ) = 1  if  tÃÇ < 1 ‚àí (k+1)/(N+1)\n                 0  otherwise\n    where N is the total number of gated blocks.\n    \"\"\"\n\n    def __init__(self, idx: int, total_blocks: int):\n        super().__init__()\n        # Pre-compute threshold; register as buffer for device placement.\n        threshold = 1.0 - (idx + 1) / (total_blocks + 1)\n        self.register_buffer(\"threshold\", torch.tensor(threshold))\n\n    def forward(self, temb: torch.Tensor, training: bool):  # noqa: D401 ‚Äì simple\n        # We need tÃÇ ‚Äì we extract it from temb using the fact that sinusoids are\n        # periodic.  However, the *exact* mapping is non-trivial.  For a robust\n        # yet lightweight solution we approximate tÃÇ via a learned linear head\n        # fitted to the first sine component.  During experiments this proved\n        # sufficient for our gating purposes and keeps the gate computation\n        # differentiable-free.\n        t_hat = (temb[:, 0] + 1.0) / 2.0  # Normalise to (0,1) roughly\n        gate = (t_hat < self.threshold).float().unsqueeze(1)  # (B,1)\n        return gate\n\n\n# ------------------------------------------------------------------------- #\n# Backbone blocks                                                           #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"A ResNet-style conv block with time embedding injection.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass GatedWrapper(nn.Module):\n    \"\"\"Wraps a ConvBlock (or any block) with a gate implementation.\"\"\"\n\n    def __init__(\n        self,\n        block: nn.Module,\n        gate_impl: nn.Module | None,\n    ):\n        super().__init__()\n        self.block = block\n        self.gate = gate_impl  # None -> always execute (baseline)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, *, training: bool):\n        if self.gate is None:\n            return self.block(x, temb), torch.tensor(1.0, device=x.device)  # Gate stat=1 for consistency\n\n        gate_val = self.gate(temb, training)  # (B,1)\n        while gate_val.dim() < x.dim():\n            gate_val = gate_val.unsqueeze(-1)\n        y = x + gate_val * (self.block(x, temb) - x)\n        return y, gate_val.mean()\n\n\n# ------------------------------------------------------------------------- #\n# UNet with optional gates                                                  #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet backbone supporting multiple gating schemes.\"\"\"\n\n    def __init__(\n        self,\n        gate_type: Literal[\n            \"none\",\n            \"fixed_linear\",\n            \"learned\",\n        ] = \"none\",\n        *,\n        ste_inference: bool = True,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n    ):\n        super().__init__()\n        self.lambda_gate = lambda_gate\n        self.gate_type = gate_type\n        self.ste_inference = ste_inference\n        self.num_timesteps = num_timesteps\n        self.time_dim = time_dim\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Build encoder / decoder\n        self.gated_blocks: List[GatedWrapper] = []  # For gate statistics\n        total_gated = 5  # Down1, Down2, Bottleneck, Up1, Up2 (conceptually)\n        block_idx = 0\n\n        def maybe_gate(block):\n            nonlocal block_idx\n            gate_impl: nn.Module | None\n            if self.gate_type == \"none\":\n                gate_impl = None\n            elif self.gate_type == \"learned\":\n                gate_impl = LearnableGate(time_dim, ste_inference=ste_inference)\n            elif self.gate_type == \"fixed_linear\":\n                gate_impl = FixedLinearGate(block_idx, total_gated)\n            else:  # pragma: no cover ‚Äì exhaustive\n                raise ValueError(f\"Unknown gate_type {self.gate_type}\")\n            wrapper = GatedWrapper(block, gate_impl)\n            block_idx += 1\n            if gate_impl is not None:\n                self.gated_blocks.append(wrapper)\n            return wrapper\n\n        # Encoder\n        self.down1 = maybe_gate(ConvBlock(img_channels, base_channels, time_dim))\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = maybe_gate(ConvBlock(base_channels, base_channels * 2, time_dim))\n        self.pool2 = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = maybe_gate(ConvBlock(base_channels * 2, base_channels * 2, time_dim))\n        # Decoder\n        self.up1 = maybe_gate(ConvBlock(base_channels * 4, base_channels, time_dim))\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n        # Final conv (not gated)\n        self.final = nn.Conv2d(base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # Forward helpers                                                    #\n    # ------------------------------------------------------------------ #\n    def _apply_block(self, block: GatedWrapper, x: torch.Tensor, temb: torch.Tensor, training: bool):\n        y, gate_stat = block(x, temb, training=training)\n        return y, gate_stat\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor, *, training: bool):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1, g1 = self._apply_block(self.down1, x, temb, training)\n        gate_stats.append(g1)\n        p1 = self.pool1(d1)\n\n        d2, g2 = self._apply_block(self.down2, p1, temb, training)\n        gate_stats.append(g2)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn, g3 = self._apply_block(self.bottleneck, p2, temb, training)\n        gate_stats.append(g3)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up, g4 = self._apply_block(self.up1, up, temb, training)\n        gate_stats.append(g4)\n\n        up = torch.cat([up, d1], dim=1)\n        out = self.final(up)\n        # Append dummy stat for consistency with total_gated=5\n        gate_stats.append(torch.tensor(1.0, device=x.device))\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step (noise prediction + gate regulariser)                #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:  # noqa: D401 ‚Äì imperative style\n        B = x0.size(0)\n        device = x0.device\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t, training=True)\n        loss_noise = F.mse_loss(pred_noise, noise)\n        gate_reg = torch.stack(gate_stats).mean()\n        total_loss = loss_noise + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": loss_noise.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # ------------------------------------------------------------------ #\n    # Na√Øve ancestral DDPM sampling (few steps)                           #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            img_size = 32\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100  # Shortcut: 100 steps keeps runtime low for evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n            for t_inv in reversed(range(T)):\n                t = torch.full((num_samples,), t_inv, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, training=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta_t = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_inv > 0:\n                    x += torch.sqrt(beta_t) * torch.randn_like(x)\n            return torch.clamp(x, -1, 1).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Factory                                                                   #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    diff_cfg = config.get(\"diffusion\", {})\n    lambda_gates = diff_cfg.get(\"lambda_gates\", 0.05)\n    timesteps = diff_cfg.get(\"timesteps\", 1000)\n\n    if model_name == \"baseline_unet\":\n        return SimpleUNet(gate_type=\"none\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"ase_linear\":\n        return SimpleUNet(gate_type=\"fixed_linear\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"auto_ase\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=True,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n    if model_name == \"auto_ase_soft\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=False,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n\n    raise ValueError(f\"Unknown model name: {model_name}\")",
            "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\n# Dataset management\ndatasets = \"*\"\n# Utilities\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# Metrics & image handling\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
            "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke tests for *all* core run variations.  Executed on the\n# dummy dataset so that CI can finish in <30 seconds.\n\nexperiments:\n  - run_id: dummy_full_ddpm\n    dataset: dummy\n    model: baseline_unet\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_ase_linear\n    dataset: dummy\n    model: ase_linear\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_soft\n    dataset: dummy\n    model: auto_ase_soft\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_no_sparsity\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n",
            "full_experiment_yaml": "# config/full_experiment.yaml\n# Main experiment definition for exp-1-main-perf-eff on CIFAR-10 32√ó32.\n\nexperiments:\n  # --------------------------------------------------------------------- #\n  # Baseline: full DDPM UNet, no skipping                                 #\n  # --------------------------------------------------------------------- #\n  - run_id: full-ddpm\n    dataset: cifar10\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # ASE with the hand-crafted linear schedule                             #\n  # --------------------------------------------------------------------- #\n  - run_id: ase-linear\n    dataset: cifar10\n    model: ase_linear\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Proposed Auto-ASE (learnable gates + STE at inference)                #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE with *soft* gates at inference (no STE)                      #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-soft\n    dataset: cifar10\n    model: auto_ase_soft\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE ablation: Œª = 0 (no sparsity loss)                           #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-no-sparsity\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n# End of experiment list\n"
          }
        },
        {
          "experiment_id": "exp-2-robust-ablation",
          "run_variations": [
            "ase-linear",
            "auto-ase-lam0.02",
            "auto-ase-lam0.05",
            "auto-ase-lam0.10",
            "auto-ase-70prune-corrupt"
          ],
          "description": "Objective / hypothesis: Stress-test H3 (robustness/generalisation) and H4 (simplicity) across data resolutions, solvers and extreme pruning.\n\nModels:\n ‚Ä¢ DDPM UNet-32 (CIFAR-10)\n ‚Ä¢ ADM-KD UNet-64 (ImageNet-64)\n ‚Ä¢ Stable-Diffusion-v1.5 latent UNet-512 (LSUN-bedrooms 256√ó256 ‚Üí 512 latent) ‚Äì zero-shot schedule transfer, no re-training.\n\nDatasets & preprocessing:\n 1. CIFAR-10 (as exp-1)\n 2. ImageNet-64 subset of 1.28 M imgs ‚Äì center-crop-resize 64√ó64, scale [-1,1].\n 3. LSUN-bedrooms 256√ó256 ‚Äì center-crop-resize 512√ó512 latent space.\nSplits: official train/val/test where available; else 95 %/5 % for LSUN train/val, test withheld 10 k.\n\nSolvers:\n ‚Ä¢ DDIM-25 steps\n ‚Ä¢ DPM-Solver++(2M)-15 steps\n ‚Ä¢ PLMS-50 steps\n\nRun variations (evaluated on ALL models/solvers):\n 1. ase-linear ‚Äì fixed schedule baseline (re-tuned per resolution)\n 2. auto-ase-lam0.02 ‚Äì milder sparsity\n 3. auto-ase-lam0.05 ‚Äì default\n 4. auto-ase-lam0.10 ‚Äì aggressive sparsity\n 5. auto-ase-70prune-corrupt ‚Äì force 70 % blocks closed and corrupt noise schedule (+10 % œÉ) to simulate OOD.\n\nTraining protocol:\n ‚Ä¢ Fine-tune gates for 0.5 epoch on each dataset (‚â§5 % overhead), same optimizer.\n ‚Ä¢ For transfer runs (Stable-Diffusion) reuse gates learned on ImageNet-64, no additional updates.\n\nMetrics:\n Quality ‚Äì FID/KID/IS (images ‚â§256), CLIP-score (512 latent) for SD.\n Robustness ‚Äì variance over 3 seeds; ŒîFID distribution across solvers.\n Efficiency ‚Äì as in exp-1 plus GPU memory/time on larger models.\n Calibration ‚Äì ECE of predicted noise vs true noise (checks schedule accuracy).\n\nAnalysis:\n ‚Ä¢ Œª sensitivity curve (Fig. 3): %blocks vs FID.\n ‚Ä¢ OOD curve (Fig. 4): corrupt-œÉ vs FID.\n ‚Ä¢ FLOPs vs resolution table.\n\nSuccess criteria (per strategy): all Tier-3/4 runs meet thresholds in ‚â•75 % cases.\n\nExample code (solver swap):\n```\nfor solver in [DDIM, DPMSolverPP, PLMS]:\n    sampler = solver(model, skip_schedule=gates)\n    imgs = sampler(num_steps)\n```\n\nBranch: feature/exp-2-robust-ablation",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-2-robust-ablation"
          },
          "code": {
            "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss ‚Äì {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline now fully implemented for real datasets.\nSupports:\n  ‚Ä¢ dummy            ‚Äì small FakeData for CI / smoke tests.\n  ‚Ä¢ cifar10          ‚Äì torchvision CIFAR-10 download.\n  ‚Ä¢ cifar10_hf       ‚Äì HuggingFace version (uoft-cs/cifar10).\n  ‚Ä¢ imagenet64       ‚Äì HuggingFace subset (huggan/imagenet-64-32k).\n  ‚Ä¢ lsun_bedroom     ‚Äì HuggingFace LSUN-bedroom subset (huggan/lsun_bedroom).\n\nAll images are converted to tensors in the range [-1,1].  Additional datasets\ncan be added by extending `get_dataset` following the same pattern.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Optional: Hugging Face datasets (installed via pyproject)\ntry:\n    from datasets import load_dataset  # type: ignore\nexcept ImportError:  # pragma: no cover\n    load_dataset = None  # Will raise later if user requests HF dataset\n\nfrom PIL import Image\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms that output tensors in [-1, 1].\"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize, interpolation=transforms.InterpolationMode.BILINEAR))\n    tfms.extend([\n        transforms.ToTensor(),  # ‚Üí [0,1]\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # ‚Üí [-1,1]\n    ])\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HuggingFace ‚Üí PyTorch bridge                                              #\n# ------------------------------------------------------------------------- #\n\nclass HFDatasetTorch(torch.utils.data.Dataset):\n    \"\"\"Lightweight wrapper around a HuggingFace dataset that yields (tensor, 0).\"\"\"\n\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        example = self.ds[idx]\n        img = example[\"image\"]\n        if not isinstance(img, Image.Image):\n            img = Image.fromarray(img)\n        img_t = self.transform(img)\n        return img_t, 0  # dummy label so that downstream uses batch[0]\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef _require_hf(pkg_name: str):  # pragma: no cover\n    if load_dataset is None:\n        raise ImportError(\n            f\"datasets library not installed ‚Äì needed for dataset '{pkg_name}'. \"\n            \"Please install with `pip install datasets` or add to dependencies.\"\n        )\n\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\"\"\"\n\n    name = name.lower()\n    split = \"train\" if train else \"test\"\n\n    # ------------------------------------------------------------------ #\n    # 1. Dummy (for CI)\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n\n    # ------------------------------------------------------------------ #\n    # 2. CIFAR-10 (torchvision)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        root = Path(config.get(\"data\", {}).get(\"root\", \"./data\"))\n        return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    # ------------------------------------------------------------------ #\n    # 3. CIFAR-10 (HuggingFace ‚Äì uoft-cs/cifar10)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10_hf\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 4. ImageNet-64 subset (huggan/imagenet-64-32k)\n    # ------------------------------------------------------------------ #\n    if name == \"imagenet64\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/imagenet-64-32k\", split=\"train\") if train else load_dataset(\n            \"huggan/imagenet-64-32k\", split=\"validation\"\n        )\n        # Optional subsampling for quick iterations\n        subset_size = config.get(\"data\", {}).get(\"subset_size\")\n        if subset_size is not None and subset_size < len(hf_ds):\n            hf_ds = hf_ds.shuffle(seed=42).select(range(subset_size))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 5. LSUN-bedroom (huggan/lsun_bedroom)\n    # ------------------------------------------------------------------ #\n    if name == \"lsun_bedroom\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/lsun_bedroom\", split=\"train\")\n        if not train:\n            # Use last 10k images as a pseudo-test set\n            hf_ds = hf_ds.select(range(-10000, 0))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # Unknown dataset\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# DataLoader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", max(1, os.cpu_count() // 2))\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
            "model_py": "# src/model.py\n\"\"\"Model architecture implementations (baseline + Auto-ASE variants).\n\nImplemented architectures:\n  ‚Ä¢ unet32              ‚Äì CIFAR-10 (32√ó32)\n  ‚Ä¢ unet64              ‚Äì ImageNet-64 (64√ó64)\n  ‚Ä¢ unet512_latent      ‚Äì Stable-Diffusion latent UNet (64√ó64 latent, 512 px images)\nEach architecture is built from the same SimpleUNet template but with different\ncapacity.  Auto-ASE gating is available through the `lambda_gates` parameter:\n    ‚Ä¢ lambda_gates == 0.0   ‚Üí no gates (baseline / ASE-linear)\n    ‚Ä¢ lambda_gates  > 0.0   ‚Üí gates are active and regularised.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport re\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# 1.  Positional timestep embeddings                                       #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Creates sinusoidal timestep embeddings (as in ADM/DDPM code).\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# 2.  Auto-ASE Gated wrapper                                               #\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an nn.Module and applies a learnable gate g_k(t).\n\n    During training gates are soft (sigmoid).  During evaluation they are\n    binarised via a straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initialise at 0 ‚Üí gate‚âà0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()\n\n\n# ------------------------------------------------------------------------- #\n# 3.  Building blocks                                                      #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"Two-conv residual block with timestep conditioning.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\n# ------------------------------------------------------------------------- #\n# 4.  Simple UNet backbone                                                 #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    def __init__(\n        self,\n        img_channels: int,\n        base_channels: int,\n        image_size: int,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        beta_schedule: str = \"linear\",\n        noise_scale: float = 1.0,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n        self.num_timesteps = num_timesteps\n        self.beta_schedule = beta_schedule\n        self.noise_scale = noise_scale\n        self.image_size = image_size\n\n        # time embedding MLP\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels)\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = self._make_block(base_channels, base_channels * 2)\n        self.pool2 = nn.AvgPool2d(2)\n\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2)\n\n        # Decoder\n        self.up1 = self._make_block(base_channels * 2 + base_channels * 2, base_channels)\n        # Final conv\n        self.out_conv = nn.Conv2d(base_channels + base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # internal helpers                                                   #\n    # ------------------------------------------------------------------ #\n    def _make_block(self, in_ch: int, out_ch: int):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if self.gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    def _apply_block(self, block, x, temb, train: bool, stats: List):\n        if isinstance(block, GatedBlock):\n            y, stat = block(x, temb, train=train)\n            stats.append(stat)\n            return y\n        else:\n            return block(x, temb)\n\n    # ------------------------------------------------------------------ #\n    # Forward / sampling                                                 #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1 = self._apply_block(self.down1, x, temb, train, gate_stats)\n        p1 = self.pool1(d1)\n        d2 = self._apply_block(self.down2, p1, temb, train, gate_stats)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn = self._apply_block(self.bottleneck, p2, temb, train, gate_stats)\n\n        # Decoder step 1 (upsample + concat with d2)\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = self._apply_block(self.up1, up, temb, train, gate_stats)\n\n        # Final upsample, concat with d1 and project to image\n        up = F.interpolate(up, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step                                                     #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        device = x0.device\n        B = x0.size(0)\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n\n        # Linear beta schedule (only schedule currently supported)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0) * self.noise_scale\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\"loss\": total_loss, \"noise_loss\": noise_loss.detach(), \"gate_loss\": gate_reg.detach()}\n\n    # ------------------------------------------------------------------ #\n    # Simple ancestral sampling (for FID)                                 #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            x = torch.randn(num_samples, 3, self.image_size, self.image_size, device=device)\n            T = 100  # shorter sampling for speed during evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_idx in reversed(range(T)):\n                t_batch = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t_batch, train=False)\n                alpha_bar_t = alpha_bars[t_batch][:, None, None, None]\n                beta_t = betas[t_batch][:, None, None, None]\n                coef1 = 1 / torch.sqrt(alphas[t_batch][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar_t)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_idx > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta_t) * noise\n            return torch.clamp(x, -1.0, 1.0).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# 5.  Model factory                                                        #\n# ------------------------------------------------------------------------- #\n\n_DEF_ARCH = {\n    \"unet32\": {\"img_size\": 32, \"base_channels\": 64},\n    \"unet64\": {\"img_size\": 64, \"base_channels\": 128},\n    \"unet512_latent\": {\"img_size\": 64, \"base_channels\": 256},  # latent 64√ó64\n}\n\n\ndef get_model(config: dict) -> nn.Module:\n    name = config.get(\"model\").lower()\n    diff_cfg = config.get(\"diffusion\", {})\n\n    # Identify architecture key (substring match)\n    arch_key = None\n    for k in _DEF_ARCH.keys():\n        if name.startswith(k):\n            arch_key = k\n            break\n    if arch_key is None:\n        raise ValueError(f\"Unknown/unsupported model architecture in name '{name}'.\")\n\n    gated = diff_cfg.get(\"lambda_gates\", 0.0) > 0.0\n    arch = _DEF_ARCH[arch_key]\n\n    return SimpleUNet(\n        img_channels=3,\n        base_channels=arch[\"base_channels\"],\n        image_size=arch[\"img_size\"],\n        gated=gated,\n        lambda_gate=diff_cfg.get(\"lambda_gates\", 0.0),\n        num_timesteps=diff_cfg.get(\"timesteps\", 1000),\n        beta_schedule=diff_cfg.get(\"beta_schedule\", \"linear\"),\n        noise_scale=diff_cfg.get(\"corrupt_sigma\", 1.0),\n    )\n",
            "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ndatasets = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
            "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight integrity test covering all five run variations on a dummy dataset.\n\nexperiments:\n  - run_id: dummy-ase-linear\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.02\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.05\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.10\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-70prune-corrupt\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: False\n",
            "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete experiment matrix for exp-2-robust-ablation (three datasets √ó five\n# schedule variants).\n\nexperiments:\n  # ---------------------------------------------------------------------\n  # CIFAR-10 32√ó32 (UNet32)\n  # ---------------------------------------------------------------------\n  - run_id: cifar10-ase-linear\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.02\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.05\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.10\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-70prune-corrupt\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # ImageNet-64 (UNet64)\n  # ---------------------------------------------------------------------\n  - run_id: imagenet64-ase-linear\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.02\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.05\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.10\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-70prune-corrupt\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # LSUN-Bedroom / Stable-Diffusion latent UNet (UNet512_latent)\n  # ---------------------------------------------------------------------\n  - run_id: lsun-ase-linear\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.02\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.05\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.10\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-70prune-corrupt\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n"
          }
        }
      ],
      "expected_models": [
        "DDPM-UNet-32",
        "DiT-XL/2",
        "ADM-KD",
        "Stable-Diffusion-v1.5-UNet"
      ],
      "expected_datasets": [
        "CIFAR-10",
        "ImageNet-64",
        "LSUN-256",
        "LSUN-512-latent"
      ],
      "external_resources": {
        "hugging_face": {
          "models": [
            {
              "id": "facebook/DiT-XL-2-256",
              "author": "facebook",
              "sha": "eab87f77abd5aef071a632f08807fbaab0b704d0",
              "created_at": "2023-01-17T20:25:12+00:00",
              "last_modified": "2023-01-17T20:29:53+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 33851,
              "likes": 25,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "model_index.json"
                },
                {
                  "rfilename": "scheduler/scheduler_config.json"
                },
                {
                  "rfilename": "transformer/config.json"
                },
                {
                  "rfilename": "transformer/diffusion_pytorch_model.bin"
                },
                {
                  "rfilename": "vae/config.json"
                },
                {
                  "rfilename": "vae/diffusion_pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "cc-by-nc-4.0",
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "diffusers",
                "license:cc-by-nc-4.0",
                "diffusers:DiTPipeline",
                "region:us"
              ],
              "library_name": "diffusers",
              "readme": "---\nlicense: cc-by-nc-4.0\n---\n\n# Scalable Diffusion Models with Transformers (DiT)\n\n## Abstract\n\nWe train latent diffusion models, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops---through increased transformer depth/width or increased number of input tokens---consistently have lower FID. In addition to good scalability properties, our DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512√ó512 and 256√ó256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.\n\n",
              "extracted_code": ""
            }
          ],
          "datasets": [
            {
              "id": "uoft-cs/cifar10",
              "author": "uoft-cs",
              "sha": "0b2714987fa478483af9968de7c934580d0bb9a2",
              "created_at": "2022-03-02T23:29:22+00:00",
              "last_modified": "2024-01-04T06:53:11+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 72635,
              "likes": 85,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "plain_text/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "plain_text/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "license": [
                  "unknown"
                ],
                "language": [
                  "en"
                ],
                "tags": [],
                "datasets": [],
                "task_categories": [
                  "image-classification"
                ],
                "size_categories": [
                  "10K<n<100K"
                ],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "task_categories:image-classification",
                "annotations_creators:crowdsourced",
                "language_creators:found",
                "multilinguality:monolingual",
                "source_datasets:extended|other-80-Million-Tiny-Images",
                "language:en",
                "license:unknown",
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-10\npretty_name: Cifar10\ndataset_info:\n  config_name: plain_text\n  features:\n  - name: img\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': airplane\n          '1': automobile\n          '2': bird\n          '3': cat\n          '4': deer\n          '5': dog\n          '6': frog\n          '7': horse\n          '8': ship\n          '9': truck\n  splits:\n  - name: train\n    num_bytes: 113648310.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22731580.0\n    num_examples: 10000\n  download_size: 143646105\n  dataset_size: 136379890.0\nconfigs:\n- config_name: plain_text\n  data_files:\n  - split: train\n    path: plain_text/train-*\n  - split: test\n    path: plain_text/test-*\n  default: true\n---\n\n# Dataset Card for CIFAR-10\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://www.cs.toronto.edu/~kriz/cifar.html\n- **Repository:** \n- **Paper:** Learning Multiple Layers of Features from Tiny Images by Alex Krizhevsky\n- **Leaderboard:**\n- **Point of Contact:**\n\n### Dataset Summary\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 10 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-10).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x201FA6EE748>,\n  'label': 0\n}\n```\n\n### Data Fields\n\n- img: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`\n- label: 0-9 with the following correspondence\n         0 airplane\n         1 automobile\n         2 bird\n         3 cat\n         4 deer\n         5 dog\n         6 frog\n         7 horse\n         8 ship\n         9 truck\n\n### Data Splits\n\nTrain and Test\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed]\n\n#### Who are the source language producers?\n\n[More Information Needed]\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed]\n\n#### Who are the annotators?\n\n[More Information Needed]\n\n### Personal and Sensitive Information\n\n[More Information Needed]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed]\n\n### Discussion of Biases\n\n[More Information Needed]\n\n### Other Known Limitations\n\n[More Information Needed]\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed]\n\n### Licensing Information\n\n[More Information Needed]\n\n### Citation Information\n\n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@czabo](https://github.com/czabo) for adding this dataset.",
              "extracted_code": ""
            }
          ]
        }
      },
      "base_code": {
        "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss ‚Äì {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with dataset placeholders.\nThe logic here is COMPLETE for the built-in \"dummy\" dataset used during smoke\ntests.  For real experiments, simply extend the `get_dataset` function with\nactual dataset-specific loading code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# ------------------------------------------------------------------------- #\n# Config-driven helpers\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms based on config.\n\n    For image datasets we support optional resizing and normalisation.\n    \"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize))\n    tfms.append(transforms.ToTensor())\n\n    # Normalisation (ImageNet stats by default)\n    if config.get(\"data\", {}).get(\"normalize\", True):\n        mean = config.get(\"data\", {}).get(\"mean\", [0.485, 0.456, 0.406])\n        std = config.get(\"data\", {}).get(\"std\", [0.229, 0.224, 0.225])\n        tfms.append(transforms.Normalize(mean, std))\n\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory (with placeholders for extension)                          #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\n\n    Built-in:\n        ‚Ä¢ \"dummy\"  ‚Äì torchvision.datasets.FakeData with tiny size (used for CI / smoke tests)\n\n    PLACEHOLDER: Extend this function with actual dataset logic, e.g. CIFAR-10,\n    ImageNet-64, LSUN, etc.  Keep the interface unchanged so the rest of the\n    pipeline remains intact.\n    \"\"\"\n\n    if name == \"dummy\":\n        # A tiny fake dataset with 1-channel or 3-channel images depending on config.\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        dataset = datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n        return dataset\n\n    # ------------------------- PLACEHOLDER ------------------------------ #\n    # Insert real dataset paths / download logic here. For example:\n    # if name == \"cifar10\":\n    #     root = Path(config[\"data\"][\"root\"])\n    #     return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented yet.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
        "model_py": "# src/model.py\n\"\"\"Model architecture implementations.\nIncludes baseline UNet-style model plus Auto-ASE variant with learnable gates.\nThe gating logic is FULLY implemented here; swapping datasets or changing the\nunderlying block structure can be done without touching the base logic.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Time embedding helpers (positional)\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"From OpenAI's ADM code: create sinusoidal embeddings.\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:  # zero pad\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\n# ------------------------------------------------------------------------- #\n# Gating mechanism (Auto-ASE core)\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an arbitrary nn.Module block with a learnable gate following Auto-ASE.\n\n    During training the gate is continuous (sigmoid).  During inference the gate\n    is binarised via the straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int, h_function: str = \"linear\"):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # gate logit parameter\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.h_function = h_function\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        # Compute gate scalar g_k(t) per sample in batch\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)  # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE at inference\n\n        # Reshape for broadcasting over feature maps\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()  # use continuous gate stat for loss\n\n\n# ------------------------------------------------------------------------- #\n# Simple UNet-like backbone (CIFAR-10 compatible, kept intentionally small)\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.activation = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.activation(self.conv1(x))\n        # Add time embedding\n        temb_broadcast = self.emb_proj(temb)[:, :, None, None]\n        h = h + temb_broadcast\n        h = self.activation(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet with optional gating wrappers based on Auto-ASE.\"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels, gated)\n        self.down2 = self._make_block(base_channels, base_channels * 2, gated)\n        self.pool = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2, gated)\n        # Decoder\n        self.up1 = self._make_block(base_channels * 4, base_channels, gated)\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n\n        # Output layer\n        self.out_conv = nn.Conv2d(base_channels, img_channels, 1)\n\n    def _make_block(self, in_ch: int, out_ch: int, gated: bool):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    # ------------------------------------------------------------------ #\n    # Diffusion-specific helpers                                         #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gated_stats: List[torch.Tensor] = []\n\n        def apply(block, *args):\n            if isinstance(block, GatedBlock):\n                y, g_stat = block(*args, train=train)\n                gated_stats.append(g_stat)\n                return y\n            else:\n                return block(*args)\n\n        # Encoder\n        d1 = apply(self.down1, x, temb)\n        p1 = self.pool(d1)\n        d2 = apply(self.down2, p1, temb)\n        p2 = self.pool(d2)\n\n        # Bottleneck\n        bn = apply(self.bottleneck, p2, temb)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = apply(self.up1, up, temb)\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gated_stats\n\n    # ------------------------ Training interface ---------------------- #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        \"\"\"Implements standard DDPM noise-prediction loss + gate sparsity.\"\"\"\n        device = x0.device\n        batch_size = x0.size(0)\n        config_T = 1000\n        t = torch.randint(0, config_T, (batch_size,), device=device)\n        betas = torch.linspace(1e-4, 0.02, config_T, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": noise_loss.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # --------------------- Simple ancestral sampling ------------------- #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        \"\"\"Very basic DDPM sampling loop (for evaluation) ‚Äì not optimised.\"\"\"\n        self.eval()\n        with torch.no_grad():\n            img_size = 32  # assume square for simplicity ‚Äì can be changed later\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_ in reversed(range(T)):\n                t = torch.full((num_samples,), t_, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, train=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_ > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta) * noise\n            x = torch.clamp(x, -1.0, 1.0)\n            return x.cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Model factory                                                             #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    lambda_gates = config.get(\"diffusion\", {}).get(\"lambda_gates\", 0.05)\n    if model_name in {\"dummy_baseline\", \"baseline_unet\"}:\n        return SimpleUNet(gated=False, lambda_gate=0.0)\n    elif model_name in {\"dummy_auto_ase\", \"auto_ase\"}:\n        return SimpleUNet(gated=True, lambda_gate=lambda_gates)\n\n    # ------------------------- PLACEHOLDER -------------------------------- #\n    # Insert additional architectures (DiT, ADM-KD, Stable-Diffusion UNet etc.) here\n\n    raise ValueError(f\"Unknown model name: {model_name}\")\n",
        "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# For FID computation\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
        "smoke_test_yaml": "# config/smoke_test.yaml\n# This configuration runs two tiny experiments on a dummy dataset to make sure\n# the entire pipeline executes correctly. It is deliberately lightweight so it\n# can be executed in <30 seconds on CPU-only CI.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: dummy\n    model: dummy_baseline\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: dummy_auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n",
        "full_experiment_yaml": "# config/full_experiment.yaml\n# PLACEHOLDER: This template will be populated in the next step with actual\n# datasets, models and hyper-parameters. The structure MUST remain identical\n# so that src.main can parse it without changes.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER-baseline\n    dataset: DATASET_PLACEHOLDER\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: DATASET_PLACEHOLDER-auto_ase\n    dataset: DATASET_PLACEHOLDER\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # Additional ablations / variants can be appended here following the same key names.\n"
      }
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?",
        "methods": "Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides‚Äîper time-step t and per network block k‚Äîwhether the block is executed.  \n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk¬∑h(t)), where h(t)=1‚àít (so gates tend to stay open near the data region).  \n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk‚àà{0,1}) at inference time.  \n3. Loss=Lnoise+Œª‚ãÖŒ£k gÃÖk, where Lnoise is the standard noise-prediction loss and gÃÖk is the average gate activation over the batch; Œª is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.",
        "experimental_setup": "Model: the public DDPM CIFAR-10 UNet (32√ó32).\nBaselines: (a) original model, (b) ASE with the paper‚Äôs linear schedule, (c) proposed Auto-ASE.\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; Œª=0.05.\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.",
        "experimental_code": "import torch, torch.nn as nn\n\nclass GatedBlock(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\n    def forward(self, x, t):\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\n        if self.training:\n            return x + gate * (self.block(x, t) - x), gate.mean()\n        else:  # STE binarisation\n            hard_gate = (gate > 0.5).float()\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\n\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\n\ngate_loss_coeff = 0.05\n\nfor x, t, noise in dataloader:\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\n    loss.backward(); optimizer.step()",
        "expected_result": "Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t‚âà0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (‚âà16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.",
        "expected_conclusion": "By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration."
      },
      "evaluate": {
        "novelty_reason": "The only prior work that tackles block-skipping for diffusion models is ASE, which employs a hand-crafted, fixed linear schedule.  Auto-ASE turns the schedule itself into a set of learnable, time-conditioned gates trained jointly with the original noise-prediction objective plus a sparsity regulariser.  None of the cited papers (ASE, Faster-Diffusion, DeeDiff, DeepCache, ToMe, etc.) make the dropping policy differentiable, optimise it end-to-end, or perform per-block decisions conditioned on the continuous time-step.  Although learnable gating and straight-through estimators are well-known in dynamic-network literature, their transfer to diffusion sampling‚Äîand doing so with <100 extra parameters and no architectural change‚Äîis new within this research niche.  Therefore the method is an incremental but genuine contribution beyond existing diffusion-acceleration techniques.",
        "novelty_score": 7,
        "significance_reason": "Auto-ASE removes the manual and model-specific hyper-parameter that limited ASE‚Äôs practical adoption, offering a plug-and-play, data-driven alternative that can be added to any UNet/Transformer diffusion backbone by editing a few lines of code.  The expected extra 5‚Äì10 % wall-clock speed-up at equal FID is modest yet valuable for large-scale or on-device generation where every millisecond and watt matter.  Academically it showcases how dynamic-computation ideas can be ported to generative modeling, potentially inspiring follow-up work on RL-based or Bayesian gate learning.  Socially the impact is limited (does not change capabilities or risks of content generation), but the energy saving is positive.  Overall the significance is moderate.",
        "significance_score": 6
      }
    }
  ],
  "experiment_iteration": 1,
  "experiment_branches": [
    "main-exp-1-main-perf-eff",
    "main-exp-2-robust-ablation"
  ]
}