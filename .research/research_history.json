{
  "research_topic": "diffusion model„ÅÆÈÄüÂ∫¶ÊîπÂñÑ",
  "queries": [
    "diffusion model inference speedup"
  ],
  "research_study_list": [
    {
      "title": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models",
      "abstract": "Diffusion models have shown remarkable performance in generation problems\nover various domains including images, videos, text, and audio. A practical\nbottleneck of diffusion models is their sampling speed, due to the repeated\nevaluation of score estimation networks during the inference. In this work, we\npropose a novel framework capable of adaptively allocating compute required for\nthe score estimation, thereby reducing the overall sampling time of diffusion\nmodels. We observe that the amount of computation required for the score\nestimation may vary along the time step for which the score is estimated. Based\non this observation, we propose an early-exiting scheme, where we skip the\nsubset of parameters in the score estimation network during the inference,\nbased on a time-dependent exit schedule. Using the diffusion models for image\nsynthesis, we show that our method could significantly improve the sampling\nthroughput of the diffusion models without compromising image quality.\nFurthermore, we also demonstrate that our method seamlessly integrates with\nvarious types of solvers for faster sampling, capitalizing on their\ncompatibility to enhance overall efficiency. The source code and our\nexperiments are available at \\url{https://github.com/taehong-moon/ee-diffusion}",
      "full_text": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Taehong Moon1 Moonseok Choi 2 EungGu Yun3 Jongmin Yoon2 Gayoung Lee 4 Jaewoong Cho 1 Juho Lee 2 5 Abstract Diffusion models have shown remarkable perfor- mance in generation problems over various do- mains including images, videos, text, and audio. A practical bottleneck of diffusion models is their sampling speed, due to the repeated evaluation of score estimation networks during the infer- ence. In this work, we propose a novel frame- work capable of adaptively allocating compute required for the score estimation, thereby reduc- ing the overall sampling time of diffusion mod- els. We observe that the amount of computa- tion required for the score estimation may vary along the time step for which the score is esti- mated. Based on this observation, we propose an early-exiting scheme, where we skip the sub- set of parameters in the score estimation network during the inference, based on a time-dependent exit schedule. Using the diffusion models for im- age synthesis, we show that our method could significantly improve the sampling throughput of the diffusion models without compromising im- age quality. Furthermore, we also demonstrate that our method seamlessly integrates with var- ious types of solvers for faster sampling, capi- talizing on their compatibility to enhance over- all efficiency. The source code and our ex- periments are available at https://github. com/taehong-moon/ee-diffusion 1. Introduction Diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have shown remarkable success in diverse domains including image synthesis (Ho et al., 2020; Dhari- This work is partially done at KAIST AI. 1KRAFTON 2Graduate School of AI, KAIST 3Independent researcher 4Naver AI Lab, South Korea 5AITRICS, South Korea. Correspondence to: Juho Lee <juholee@kaist.ac.kr>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). wal & Nichol, 2021; Ho et al., 2022a), text-to-image gen- eration (Ramesh et al., 2022; Rombach et al., 2022), 3D point cloud generation (Luo & Hu, 2021), text-to-speech generation (Jeong et al., 2021), and video generation (Ho et al., 2022b). These models learn the reverse process of introducing noise into the data to data and denoise inputs progressively during inference using the learned reverse model. One major drawback of diffusion models is their slow sampling speed, as they require multiple steps of forward passes through score estimation networks to generate a sin- gle sample, unlike the other methods such as GANs (Good- fellow et al., 2014) that require only a single forward pass through a generator network. To address this issue, sev- eral approaches have been proposed to reduce the number of steps required for the sampling of diffusion models, for instance, by improving ODE/SDE solvers (Kong & Ping, 2021; Lu et al., 2022; Zhang & Chen, 2023) or distilling into models requiring less number of sampling steps (Sal- imans & Ho, 2022; Song et al., 2023). Moreover, in ac- cordance with the recent trend reflecting scaling laws of large models over various domains, diffusion models with a large number of parameters are quickly becoming main- stream as they are reported to produce high-quality sam- ples (Peebles & Xie, 2022). Running such large diffusion models for multiple sampling steps incurs significant com- putational overhead, necessitating further research to opti- mize calculations and efficiently allocate resources. On the other hand, recent reports have highlighted the ef- fectiveness of early-exiting schemes in reducing computa- tional costs for Large Language Models (LLMs) (Schuster et al., 2022; Hou et al., 2020; Liu et al., 2021; Schuster et al., 2021). The concept behind early-exiting is to bypass the computation of transformer blocks when dealing with relatively simple or confident words. Given that modern score-estimation networks employed in diffusion models share architectural similarities with LLMs, it is reasonable to introduce the early-exiting idea to diffusion models as well, with the aim of accelerating the sampling speed. In this paper, we introduce Adaptive Score Estimation (ASE) for faster sampling from diffusion models, draw- ing inspiration from the early-exiting schemes utilized in 1 arXiv:2408.05927v1  [cs.CV]  12 Aug 2024A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models LLMs. What sets diffusion models apart and distinguishes our proposal from a straightforward application of the early-exiting scheme is the time-dependent nature of the score estimation involved in the sampling process. We hy- pothesize that the difficulty of score estimation may vary at different time steps, and based on this insight, we adapt the computation of blocks differently for each time step. As a result, we gain the ability to dynamically control the computation time during the sampling procedure. To ac- complish this, we present a time-varying block-dropping schedule and a straightforward algorithm for fine-tuning a given diffusion model to be optimized for this schedule. ASE successfully accelerates the sampling speed of diffu- sion models while maintaining high-quality samples. Fur- thermore, ASE is highly versatile, as it can be applied to score estimation networks with various backbone architec- tures and can be combined with different solvers to further enhance sampling speed. We demonstrate the effectiveness of our method through experiments on real-world image synthesis tasks. 2. Related Work Fast Sampling of Diffusion Models. Diffusion proba- bilistic models (Sohl-Dickstein et al., 2015; Song & Er- mon, 2019; Ho et al., 2020; Dhariwal & Nichol, 2021) have shown their effectiveness in modeling data distribu- tions and have achieved the state-of-the-art performance, especially in the field of image synthesis. These models employ a progressive denoising approach for noisy inputs which unfortunately lead to heavy computational costs. To overcome this issue, multiple works have been proposed for fast sampling. DDIM (Nichol & Dhariwal, 2021) accel- erates the sampling process by leveraging non-Markovian diffusion processes. FastDPM (Kong & Ping, 2021) uses a bijective mapping between continuous diffusion steps and noises. DPM-Solver (Lu et al., 2022) analytically solves linear part exactly while approximating the non-linear part using high-order solvers. DEIS (Zhang & Chen, 2023) utilizes exponential integrator and polynomial extrapola- tion to reduce discretization errors. In addition to utiliz- ing a better solver, alternative approaches have been pro- posed, which involve training a student model using net- work distillation (Salimans & Ho, 2022). Recently, consis- tency model (Song et al., 2023; Song & Dhariwal, 2024) proposed a distillation scheme to directly find the consis- tency function from the data point within the trajectory of the probability flow. And Kim et al. (2023) refined the consistency model with input-output time parameterization within the score function and adversarial training. While previous approaches focused on reducing the timestep of sampling, recent studies proposed an alternative way to ac- celerate sampling speed by reducing the processing time of diffusion model itself. In particular, Block Caching (Wim- bauer et al., 2023) aim to re-use the intermediate feature which is already computed in previous timestep while To- ken Merging (Bolya & Hoffman, 2023) target to reduce the number of tokens. Concurrent work (Tang et al., 2023) sug- gests early exiting scheme on diffusion models. However, it requires additional module which is used to estimate an uncertainty of intermediate features. Our work is orthogo- nal to these existing approaches, as we focus on reducing the number of processed blocks for each time step, rather than targeting a reduction in the number of sampling steps. Early Exiting Scheme for Language Modeling. The recent adoption of Large Language Models (LLMs) has brought about significant computational costs, prompting interest in reducing unnecessary computations. Among the various strategies, an early-exiting scheme that dy- namically selects computation layers based on inputs has emerged for Transformer-based LLMs. DynaBERT (Hou et al., 2020) transfers knowledge from a teacher network to a student network, allowing for flexible adjustments to the width and depth. Yijin et al. (Liu et al., 2021) employ mu- tual information and reconstruction loss to assess the diffi- culty of input words. CAT (Schuster et al., 2021) incorpo- rates an additional classifier that predicts when to perform an early exit. CALM (Schuster et al., 2022) constrains the per-token exit decisions to maintain the global sequence- level meaning by calibrating the early-exiting LLM us- ing semantic-level similarity metrics. Motivated by the aforementioned works, we propose a distinct early-exiting scheme specifically designed for diffusion models. 3. Method This section describes our main contribution - Adaptive Score Estimation (ASE) for diffusion models. The sec- tion is organized as follows. We first give a brief recap on how to train a diffusion model and provide our intu- ition on the time-varying complexity of score estimation. Drawing from such intuition, we empirically demonstrate that precise score estimation can be achieved with fewer parameters within a specific time interval. To this end, we present our early-exiting algorithm which boosts inference speed while preserving the generation quality. 3.1. Time-Varying Complexity of Score Estimation Training Diffusion Models. Let x0 ‚àº pdata(x) := q(x) be a sample from a target data distribution. In a diffu- sion model, we build a Markov chain that gradually injects Gaussian noises to x0 to turn it into a sample from a noise distribution p(xT ), usually chosen as standard Gaussian distribution. Specifically, given a noise schedule (Œ≤t)T t=1, the forward process of a diffusion model is defined as 2A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models q(xt |xt‚àí1) = N(xt | p 1 ‚àí Œ≤txt‚àí1, Œ≤tI). (1) Then we define a backward diffusion process with a param- eter Œ∏ as, pŒ∏(x1:T ) = p(xT ) TY t=1 pŒ∏(xt‚àí1 |xt), q (xT |x0) ‚âà N(0, I). (2) so that we can start from xT ‚àº N(0, I) and denoise it into a sample x0. The parameter Œ∏ can be optimized by minimizing the negative of the lower-bound on the log- evidence, L(Œ∏) = ‚àí TX t=1 Eq [DKL[q(xt‚àí1 |xt, x0)‚à•pŒ∏(xt‚àí1 |xt)]] ‚â• ‚àílog pŒ∏(x0), (3) where q(xt‚àí1|xt, x0) = N \u0010 xt‚àí1; Àú¬µt(xt, x0), ÀúŒ≤tI \u0011 , Àú¬µt(xt, x0) = 1‚àöŒ±t \u0012 xt ‚àí Œ≤t‚àö1 ‚àí ¬ØŒ±t Œµt \u0013 . (4) The model distribution pŒ∏(xt‚àí1 |xt) is chosen as a Gaus- sian, pŒ∏(xt‚àí1 |xt) = N(xt‚àí1 |¬µŒ∏(xt, t), œÉ2 t I), ¬µŒ∏(xt, t) = 1‚àöŒ±t \u0012 xt ‚àí Œ≤t‚àö1 ‚àí ¬ØŒ±t ŒµŒ∏(xt, t) \u0013 , (5) and the above loss function then simplifies to L(Œ∏) = TX t=1 Ex0,Œµt h Œª(t) \r\rŒµt ‚àí ŒµŒ∏(‚àö¬ØŒ±tx0 + ‚àö 1 ‚àí ¬ØŒ±tŒµt, t) \r\r2i , (6) where Œª(t) = Œ≤2 t 2œÉ2 t Œ±t(1‚àí¬ØŒ±t) . The neural network ŒµŒ∏(xt, t) takes a corrupted sample xt and estimates the noise that might have applied to a clean sample x0. Under a simple reparameterization, one can also see that, ‚àáxt log q(xt |x0) = ‚àí Œµt‚àö1 ‚àí ¬ØŒ±t ‚âà ‚àíŒµŒ∏(xt, t)‚àö1 ‚àí ¬ØŒ±t := sŒ∏(xt, t), (7) where sŒ∏(xt, t) is the score estimation network. In this pa- rameterization, the loss function can be written as, L(Œ∏) = TX t=1 Ex0,xt h Œª‚Ä≤ t‚à•‚àáxt log q(xt |x0) ‚àí sŒ∏(xt, t)‚à•2 i , (8) so learning a diffusion model amounts to regressing the score function of the distribution q(xt |x0). The op- timal regressor of the score function ‚àáxt log q(xt) at time step t is obtained by taking the expectation of the conditional score function over the noiseless distribution Ex0 |xt [‚àáxt log q(xt |x0)] = ‚àáxt log q(xt). Suppose we train our diffusion model using the standard parameterization (i.e., Œµ-parameterization), where the ob- jective is to minimize the gap ‚à•ŒµŒ∏ ‚àí Œµ‚à•2. When t is close to 1, this gap primarily represents noise, constituting only a small fraction of the entire x0. Consequently, it indicates that learning does not effectively occur in the proximity to the noise. Given that a diffusion model is trained across all time steps with a single neural network, it is reasonable to anticipate that a significant portion of the parameters are allocated for the prediction of near data regime ( t close to 0). This intuition leads to our dropping schedule pruning more parameters when t is close to 1. Adaptive Computation for Score Estimation To get the samples from diffusion models, we can apply Langevin dy- namics to get samples from the distribution given the score function ‚àáxlog p(x). Depending on the number of iter- ation N and step size Œ≤, we can iteratively update xt as follows: xt+1 = xt + Œ≤‚àáx log p(xt) + p 2Œ≤zt, (9) where zt ‚àº N(0, I). Due to this iterative evaluation, the total sampling time can be roughly be computed as T √ó œÑ, where T is the num- ber of sampling steps and œÑ is the processing of diffusion model per time step. To enhance sampling efficiency, con- ventional approaches aim to reduce the number of time steps within the constrained value of œÑ. Our experiments indicate that it‚Äôs feasible to reduce œÑ by performing score estimation for specific time intervals using fewer parame- ters. While one could suggest employing differently sized models for estimating scores at various time intervals to re- duce overall sampling time, our strategy introduces a sim- ple early exiting framework within a single model, avoid- ing extra memory consumption. Furthermore, our method focus on reducing the processing time œÑ while maintain- ing accurate predictions within a given time interval. To accomplish this, we introduce adaptive score estimation, wherein the diffusion model dynamically allocates param- eters based on the time t. For challenging task such as time t ‚Üí 0, the full parameter is utilized, while it induces skip- ping the subset of parameters near prior distribution. 3.2. Adaptive Layer Usage in Diffusion Process We hereby introduce an early exiting framework to accel- erate the sampling process of pre-trained diffusion models. 3A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Noise-EasyData-Easy FID: 8.88 FID: 47.71 Figure 1.Snapshot samples of Noise-Easy / Data-Easy schedules when fine-tuned DiT on ImageNet. While the data-easy sched- ule struggles to produce a discernible dog image, the noise-easy schedule successfully generates a clear dog image, achieving a converged FID score of 8.88. Drawing upon the intuition presented in ¬ß 3.1, we first ex- plain how to decide the amount of parameters to be used for score estimation. After dropping the selected blocks, we design a fine-tuning algorithm to adjust the output of intermediate building blocks of diffusion models. Which time interval can be accurately estimated with fewer parameters? To validate our hypothesis in the context of training diffusion models, we conduct a toy ex- periment regarding the difficulty of score estimation for different time steps. We conduct tests under two scenar- ios: one assuming that estimation near the prior distribu- tion requires fewer parameters (Noise-Easy schedule), and the other assuming that estimation near the data distribu- tion demands fewer parameters (Data-Easy schedule). As shown in Figure 1, one can easily find that the noise-easy schedule successfully generates a clear dog image where as the data-easy schedule struggles to produce a discernible dog image. Which layer can be skipped for score estimation? To accelerate inference in diffusion models, we implement a dropping schedule that takes into account the complexity of score estimation near t ‚Üí 1 compared to t ‚Üí 0. For the DiT model trained on ImageNet, which consists of 28 blocks, we design a dropping schedule that starts from the final block. Based on our intuition, we drop more DiT blocks as time approaches 1, as shown in Figure 2. Con- versely, for scores near the data, which represent more chal- lenging tasks, we retain all DiT blocks to utilize the entire parameter set effectively. In U-ViT, the dropping schedule has two main distinctions from DiT: the selection of candidate modules to drop and the subset of parameters to be skipped. Unlike DiT, we limit dropping to the decoder part in U-ViT. This decision is motivated by the presence of symmetric long skip connec- tions between encoder and decoder, as dropping encoder modules induce the substantial information loss. Moreover, when dropping the parameters in U-ViT, we preserve the linear layer of a building block to retain feature informa- tion connected through skip connections, while skipping score function Block 1Block 2DecoderDecoderBlock 2DecoderBlock 3DecoderBlock 4Decoder Block 3Decoder Figure 2.Schematic for time-dependent exit schedule. Consider- ing the varying difficulty of score estimation, we drop more build- ing blocks of architecture near noise. While we skip the whole building blocks in DiT, we partially skip the blocks in U-ViT due to the long skip-connection. the remaining parameters. 3.3. Fine-tuning Diffusion Models Following the removal of blocks based on a predetermined dropping schedule, we need to fine-tune the model. This is attributed to the early exit approach, where the interme- diate outputs of each building block are directly connected to the decoder. Consequently, the decoder encounters input values that differ from the distribution it learned during its initial training, requiring adjustments. To address this issue, we propose a novel fine-tuning algo- rithm that focuses on updating minimal information near time t ‚Üí 0 while updating unseen information near time t ‚Üí 1. To force the differential information update, we leverage two different techniques: (i) adapting Exponential Moving Average (EMA), and (ii) weighting the coefficients Œª(t). The EMA technique is employed to limit the frequency of information updates, thereby preserving the previous knowledge acquired by the model during its initial train- ing phase. A high EMA rate results in a more gradual modification of parameters. In our approach, we deliber- ately maintain a high EMA rate to enhance the stability of our training process. During the gradual parameter up- date, we aim to specifically encourage modifications in a subset of parameters that align the predicted scores more closely with the prior distribution. To prioritize the learn- ing of this score distribution, we apply a higher coefficient to the Œª(t) term, which in turn multiplies on the expectation of the training loss. Once the model‚Äôs performance appears to have plateaued, we adjust the Œª(t) value back to 1, aim- ing to facilitate comprehensive learning across the entire score distribution spectrum. We provide the pseudo-code for fine-tuning diffusion models in Appendix A. 4A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 4. Experiments 4.1. Experimental Setting Experimental Details. Throughout the experiments, we use DiT (Peebles & Xie, 2022) and U-ViT (Bao et al., 2022), the two representative diffusion models. We employ three pre-trained models: (1) DiT XL/2 trained on ImageNet (Krizhevsky et al., 2017) with the resolution of 256 √ó 256; (2) U-ViT-S/4 trained on CelebA (Liu et al., 2015) with the resolution of 64 √ó 64; (3) PixArt-Œ±-SAM- 256 trained on SAM dataset (Kirillov et al., 2023). For the fine-tuning step in both DiT and U-ViT experiments, we employ a hybrid loss (Nichol & Dhariwal, 2021) with a re- weighted time coefficient and linear schedule for injecting noise. We use AdamW (Loshchilov & Hutter, 2017) op- timizer with the learning rate of 2 ¬∑ 10‚àí5. We use cosine annealing learning rate scheduling to ensure training sta- bility for the U-ViT models. Batch size is set to 64, and 128 for fine-tuning DiT XL/2, U-ViT-S/4, respectively. We use T = 1000 time steps for the forward diffusion process. In case of PixArt experiment, we fine-tune our model with 100K SAM data, the batch size of 200 √ó4, and 2200 itera- tions while the pre-trained model is trained with 10M data, the batch size of 176 √ó64 and 150K iterations. For further experimental details, we refer readers to Appendix A. Evaluation Metrics. We employ Fr ¬¥echet inception dis- tance (FID) (Heusel et al., 2017) for evaluating image generation quality of diffusion models. We compute the FID score between 5,000 generated samples from diffu- sion models and the full training dataset. In case of text- to-image experiment, we measure the FID score with MS- COCO valid dataset (Lin et al., 2014). To evaluate the sam- pling speed of diffusion models, we report the wall-clock time required to generate a single batch of images on a sin- gle NVIDIA A100 GPU. Baselines. In this study, we benchmark our method against a range of recent techniques which aims reduc- ing the processing time of diffusion models. This in- cludes DeeDiff (Tang et al., 2023), token merging (ToMe; Bolya & Hoffman, 2023), and block caching (Wimbauer et al., 2023). When extending ToMe to U-ViT architec- ture, we specifically apply the token merging technique to self-attention and MLP modules within each block of the U-ViT. Of note, U-ViT treats both time and condi- tion as tokens in addition to image patches. To improve generative modeling, we exclude these additional tokens and focus solely on merging tokens associated with im- age patches, following the approach outlined by (Bolya & Hoffman, 2023). For block caching, we employ caching strategies within the attention layers. Naive caching may aggravate feature misalignment especially when caching is more aggressive in order to achieve faster sampling speed. To resolve such an issue, (Wimbauer et al., 2023) further propose shift-scale alignment mechanism. As we explore high-acceleration regime, we report results for both the original block caching technique and its variant with the shift-scale mechanism applied (termed SS in Figure 3). We only report the best performance attained among the diverse hyperparameter settings in the following sections. The remaining results will be deferred to Appendix C as well as experimental details for baseline strategies. 4.2. Inference Speed and Performance Trade-off Figure 3 presents a trade-off analysis between generation quality and inference speed, comparing our approach to other baseline methods. We can readily find that ASE largely outperforms both ToMe and block caching strate- gies. ASE boosts sampling speed by approximately 25- 30% while preserving the FID score. Techniques based on feature similarity, such as ToMe and block caching, are straightforward to implement yet fail to bring significant performance gain, or even in some cases, bring an increase in processing time. This can primarily be attributed to the additional computational overhead intro- duced by token partitioning and the complexity of bipartite soft matching calculations for token merging, which out- weighs the advantages gained from reducing the number of tokens. This observation is particularly noteworthy, as even for the CelebA dataset, the number of tokens in U- ViT remains relatively small, and U-ViT does not decrease the token count through layers, as is the case with U-Net. Regarding block caching, it yields only slight enhance- ments in inference speed while preserving the quality of generation. Although block caching can be straightfor- wardly applied to various diffusion models, it encounters a notable constraint: it relies significantly on scale-shift alignment, necessitating extra fine-tuning. Additionally, its effectiveness depends on the specifc architectural charac- teristics of the model being used. We postulate that this de- pendency may be related to the presence of residual paths within the architecture. It is crucial to highlight that our method effectively increases sampling speed without sacri- ficing the quality of the generated output. In Table 2, we further compare DeeDiff with our method using the performances reported in (Table 1; Tang et al., 2023). ASE and DeeDiff share the same essence as both are grounded in the early-exiting framework. The distinc- tion lies in the dynamic sampling process. To determine when to perform early-exiting for dynamic sampling, an additional module needs to be added to the model, whereas ASE does not require any additional memory. Furthermore, ASE exhibits faster acceleration while maintaining or im- proving FID, but for DeeDiff, there is a trade-off between 5A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 0 5 10 15 20 25 30 35 40 Acceleration (%) 101 102 FID ImageNet / DDIM-50 T oken Merging Block Caching w/o SS ASE (Ours) ‚àí5 0 5 10 15 20 25 30 Acceleration (%) 101 102 FID CelebA / DPM-50 T oken Merging Block Caching w/o SS Block Caching w/ SS ASE (Ours) Figure 3.Trade-off between image generation quality and sampling speed on ImageNet with DiT (left) and CelebA with U-ViT (right). We generate samples from DDIM and DPM sampler with 50 steps for ImageNet and CelebA, respectively. ASE largely outperforms other techniques, preserving FID score while boosting sampling speed by approximately 25-30%. Here, SS stands for scale-shift adjustment used together with block caching. Table 1.Trade-off between image generation quality and sampling speed on ImageNet (DiT; DDPM sampler) and CelebA (U-ViT; EM sampler). ASE consistently maintains image generation quality while achieving a notable increase in sampling speed of approximately 30%; ASE can be effectively used in conjunction with fast solvers. Refer to Table 5 in Appendix A for detailed description of our dropping schedules. (DiT) ImageNet DDPM-250 FID (‚Üì) Accel. ( ‚Üë) Baseline 9.078 - D2-DiT 8.662 23.43% D3-DiT 8.647 30.46% D4-DiT 9.087 34.56% D7-DiT 9.398 38.92% (U-ViT) CelebA EM-1000 FID (‚Üì) Accel. ( ‚Üë) Baseline 2.944 - D1-U-ViT 2.250 21.3% D2-U-ViT 2.255 24.8% D3-U-ViT 3.217 29.7% D6-U-ViT 4.379 32.6% the advantage in GFLOPs and the potential disadvantage in generation quality. In the case of ToMe and block caching, both methods fall significantly short of achieving the per- formance of ASE or DeeDiff. 4.3. Compatability with Diverse Sampling Solvers We demonstrate the compatibility of the proposed method with diverse sampling methods. First of all, we verify that our method can be successfully applied to accelerate sam- pling speed without degrading generation qualtiy. In Ta- ble 1, we generated samples with DDPM (Ho et al., 2020) in DiT architecture and get samples from Euler-Maruyama solver. Here, we present results of four varying dropping schedules in each experiments. In a nutshell, n in D- n schedule represents the acceleration scale. For instance, D3-DiT and D3-U-ViT schedules bring similar scales in terms of acceleration in sampling speed. We refer readers to Table 5 for detailed guide on ASE dropping schedules. Furthermore, we show that our method can be seamlessly incorporated with fast sampling solver, such as DDIM (Song et al., 2020) solvers and DPM solver (Lu et al., 2022). From the DiT results presented in , we we ob- serve that our approach effectively achieves faster infer- ence while utilizing fewer parameters, yet maintains the same level of performance. In case of U-ViT, we show that our method notably achieves an over 30% accelera- tion, while preserving similar quality in generation with the DPM solver. Notably in Figure 4, we highlight that our method is robust across various time steps within both DDIM and DPM solver. This indicates that our method effectively estimates scores across the entire time interval. The reasons for our method‚Äôs robustness and efficiency in achieving faster inference will be further explained in ¬ß 5. 4.4. Large Scale Text-to-Image Generation Task To demonstrate that our method can be extended to large- scale datasets, we apply it to the pre-trained PixArt- Œ± model. While there may be concerns that fine-tuning with a large-scale dataset could potentially slow down the fine- tuning process, we find that using only 1 % of the origi- nal data is sufficient for our method to achieve the desired performance. To evaluate our method, we employ a DPM 6A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 0 5 10 15 20 25 30 Acceleration (%) 8.9 9.0 9.1 9.2 9.3FID ImageNet / DDIM solver 0 5 10 15 20 25 Acceleration (%) 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4FID CelebA / DPM solver 50 steps 100 steps 200 steps 25 steps 50 steps Figure 4.Robustness of ASE across varying sampling timesteps: ImageNet with DDIM solver (left), and CelebA with DPM solver (right). Both experiments employed U-ViT architecture. ASE displays robust performance throughout different timesteps in both different experimental settings. Table 2.Trade-off between image generation quality and sam- pling speed on CelebA (U-ViT; DPM-50). Compared to the other baselines, ASE displays a remarkable sampling speed in terms of acceleration in GFLOPs. CelebA Methods Accel. ( ‚Üë) FID ( ‚Üì) U-ViT - 2.87 DeeDiff (Tang et al., 2023) 45.76% 3.9 ToMe (Bolya & Hoffman, 2023) 3.05% 4.963 Block Caching (Wimbauer et al., 2023) 9.06% 3.955 ASE (Ours) 23.39% 1.92 solver with 20 steps and classifier-free guidance (Ho & Sal- imans, 2022). Although the original model achieves an FID score of 12.483, the ASE-enhanced model attains an FID score of 12.682, with a 14 % acceleration in terms of wall- clock time. An example of an image generated from a given prompt is shown in Figure 5. 5. Further Analysis Ablation Study on Dropping Schedules. Although it is empirically understood that we can eliminate more param- eters near the prior distribution, it remains to be deter- mined which time-dependent schedules yield optimal per- formance in generation tasks. To design an effective drop- ping schedule, we conduct an ablation study as follows: we create four distinct schedules that maintained the same to- tal amount of parameter dropping across all time intervals, but vary the amount of dropping for each specific interval. These schedules are tested on a U-ViT backbone trained on the CelebA dataset. Specifically, the decoder part of this architecture consists of six blocks, and Figure 6 illus- trates how many blocks are utilized at each timet. By fine- Pre-trained model ASE (ours) Figure 5.Comparison between samples produced by pre-trained PixArt-Œ± and ASE-enhanced PixArt- Œ±. Text prompts are ran- domly chosen. tuning in this manner, we evaluate the generation quality of the models, as shown in Table 3. As the results indicate, Schedule 1 outperforms the others, demonstrating the most superior and stable performance across varying time steps. Viewpoint of Multi-task Learning. Diffusion models can be seen as a form of multi-task learning, as they use a single neural network to estimate the scores at every time t. In the context of multi-task learning, negative transfer phenomenon can occur, leading to a decrease in the gen- eration quality of diffusion models. Recent work, such as DTR (Park et al., 2023), improve generation quality by jointly training a mask with the diffusion model. This ap- proach minimizes negative transfer by reducing interfer- ence between tasks. Similarly, our method, despite us- ing fewer parameters, is designed to achieve a compara- ble effect. By explicitly distinguishing the parameters used for predicting specific intervals through early-exiting, our approach can mitigate the issues associated with negative 7A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Schedule-16655442211Schedule-26655441122Schedule-36655112244Schedule-46611224455 Figure 6.Dropping schedules designed for the ablation study. We divide the sampling time into ten uniform intervals, and drop a specific amount of blocks. The number indicates the amount of blocks left after dropping the rest. Table 3.FID score on CelebA dataset with U-ViT backbone across ablated dropping schedules. In both DPM-25 and DPM- 50, schedule-1 exhibits the best performance. Methods DPM-25 DPM-50 Schedule-1 2.116 2.144 Schedule-2 2.456 2.28 Schedule-3 2.173 3.128 Schedule-4 2.966 3.253 transfer. To illustrate the efficacy of our method in mitigating neg- ative transfer, we hereby conduct a toy experiment. Con- sider score estimation over a specific time intervalt ‚àà [s, l] as a single task. In the experiment, we equally divide the whole sampling time into ten intervals, thereby defining a total of ten tasks. To verify the presence of negative transfer in the diffusion model, we create both a baseline model and expert models trained specifically for each in- terval. In order to check whether the pre-trained model is sufficiently trained, we further train the baseline model, and Table 4 shows that further-training degrades the per- formance. Also, the multi-experts model outperforms the baseline model, indicating successful reduction of task in- terference. Furthermore, replacing the pre-trained model with the ASE module ( Mixed-k models) in a single time interval leads to performance gains. In Table 4, we can readily observe that the mixed schedules outperform the baseline model across all intervals in terms of image gen- eration quality. This finding suggests that our training ap- proach can not only effectively boost sampling speed but also preserves model performance via mitigating negative transfer effect. 6. Conclusion and Limitations In this paper, we present a novel method that effectively reduces the overall computational workload by using an early-exiting scheme in diffusion models. Specifically, our method adaptively selects the blocks involved in denois- ing the inputs at each time step, taking into account the OursBaselineMixed-kExperts :heavy:light k Figure 7.Schematic for different types of dropping schedules de- signed to validate negative transfer phenomenon. Mixed-k re- places the original heavy model with light ASE model only on kth time interval. Experts employ individually fine-tuned heavy models at each time interval. Table 4.FID score on CelebA dataset with U-ViT backbone across NTR-inspired dropping schedules. Experts outperform both baseline and further fine-tuned model thereby indicating that negative transfer does exist. Moreover, all the mixed-k sched- ules, despite only replacing a single time interval, demonstrate improved performance compared to the original baseline model. Methods DPM-25 DPM-50 Baseline 3.355 3.316 Further-trained 4.262 4.028 Multi-Experts 2.987 2.942 Mixed-1 2.938 3.054 Mixed-3 2.654 3.232 Mixed-5 3.287 3.187 Mixed-7 2.292 2.969 Mixed-9 2.933 3.027 assumption that fewer parameters are required for early de- noising steps. Surprisingly, we demonstrate that our method maintains performance in terms of FID scores even when reducing calculation costs by 30%. Our approach is not limited to specific architectures, as we validate its effectiveness on both U-ViT and DiTs models. A limitation of our pro- posed method is that we manually design the schedule for the early-exiting scheme. As future work, we acknowledge the need to explore automated methods for finding an opti- mal schedule. Impact Statement Our work is improving diffusion models which can be mis- used for generating fake images or videos, contributing to the spread of deepfake content or the creation of mislead- ing information. Also, given that these models are trained on data collected from the internet, there is a risk of harm- ful biases being embedded in the generated samples such as emphasizing stereotypes. 8A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Acknowledgement The authors would like to express their sincere gratitude to Jaehyeon Kim and Byeong-Uk Lee for their insightful and constructive discussions. This work was partly supported by Institute for Information & communications Technol- ogy Promotion(IITP) grant funded by the Korea govern- ment(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program(KAIST), KAIST-NA VER Hy- percreative AI Center, Korea Foundation for Advanced Studies (KFAS), No.2022-0-00713, Meta-learning Appli- cable to Real-world Problems), and National Research Foundation of Korea (NRF) funded by the Ministry of Ed- ucation (NRF2021M3E5D9025030). References Bao, F., Li, C., Cao, Y ., and Zhu, J. All are worth words: a vit backbone for score-based diffusion models. arXiv preprint arXiv:2209.12152, 2022. Bolya, D. and Hoffman, J. Token merging for fast stable diffusion. arXiv preprint arXiv:2303.17604, 2023. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sas- try, G., Askell, A., et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pp. 1877‚Äì1901, 2020. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the North American Chapter of the Association for Computational Linguistics (ACL), 2019. Dhariwal, P. and Nichol, A. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems 34 (NeurIPS 2021), pp. 8780‚Äì8794, 2021. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y . Generative adversarial nets. In Advances in Neural Information Processing Systems 27 (NIPS 2014), 2014. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion prob- abilistic models. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pp. 6840‚Äì6851, 2020. Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1‚Äì33, 2022a. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. arXiv:2204.03458, 2022b. Hou, L., Huang, Z., Shang, L., Jiang, X., Chen, X., and Liu, Q. Dynabert: Dynamic bert with adaptive width and depth. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), 2020. Jeong, M., Kim, H., Cheon, S. J., Choi, B. J., and Kim, N. S. Diff-tts: A denoising diffusion model for text-to- speech. In International Speech Communication Associ- ation, 2021. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y ., Uesaka, T., He, Y ., Mitsufuji, Y ., and Ermon, S. Consistency trajectory models: Learning probabil- ity flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y ., et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pp. 4015‚Äì4026, 2023. Kong, Z. and Ping, W. On fast sampling of diffusion proba- bilistic models. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models (INNF+ 2021), 2021. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84‚Äì90, 2017. Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra- manan, D., Doll¬¥ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740‚Äì 755. Springer, 2014. Liu, Y ., Meng, F., Zhou, J., Chen, Y ., and Xu, J. Faster depth-adaptive transformers. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 13424‚Äì 13432, 2021. 9A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , 2015. Liu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., and Hu, H. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3202‚Äì3211, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay reg- ularization. arXiv preprint arXiv:1711.05101, 2017. Lu, C., Zhou, Y ., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neu- ral Information Processing Systems 35 (NeurIPS 2022), 2022. Luo, S. and Hu, W. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Nichol, A. Q. and Dhariwal, P. Improved denoising diffu- sion probabilistic models. In Proceedings of The 38th International Conference on Machine Learning (ICML 2021), pp. 8162‚Äì8171, 2021. Park, B., Woo, S., Go, H., Kim, J.-Y ., and Kim, C. De- noising task routing for diffusion models. arXiv preprint arXiv:2310.07138, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with la- tent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684‚Äì10695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Con- volutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted In- tervention (MICCAI), 2015. Salimans, T. and Ho, J. Progressive distillation for fast sam- pling of diffusion models. In International Conference on Learning Representations (ICLR), 2022. Schuster, T., Fisch, A., Jaakkola, T., and Barzilay, R. Con- sistent accelerated inference via confident adaptive trans- formers. In Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2021. Schuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D., Tran, V ., Tay, Y ., and Metzler, D. Confident adaptive language modeling. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequi- librium thermodynamics. In Proceedings of The 32nd International Conference on Machine Learning (ICML 2015), pp. 2256‚Äì2265, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion im- plicit models. arXiv preprint arXiv:2010.02502, 2020. Song, Y . and Dhariwal, P. Improved techniques for training consistency models. International Conference on Learn- ing Representations (ICLR), 2024. Song, Y . and Ermon, S. Generative modeling by estimating gradients of the data distribution. In Advances in Neu- ral Information Processing Systems 32 (NeurIPS 2019), 2019. Song, Y ., Dhariwal, P., Chen, M., and Sutskever, I. Consis- tency models. In Proceedings of The 39th International Conference on Machine Learning (ICML 2023), 2023. Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Seg- menter: Transformer for semantic segmentation. In Pro- ceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 7262‚Äì7272, 2021. Tang, S., Wang, Y ., Ding, C., Liang, Y ., Li, Y ., and Xu, D. Deediff: Dynamic uncertainty-aware early exiting for accelerating diffusion model generation. arXiv preprint arXiv:2309.17074, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. Atten- tion is all you need. In Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017. Wimbauer, F., Wu, B., Schoenfeld, E., Dai, X., Hou, J., He, Z., Sanakoyeu, A., Zhang, P., Tsai, S., Kohler, J., et al. Cache me if you can: Accelerating diffu- sion models through block caching. arXiv preprint arXiv:2312.03209, 2023. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P. Segformer: Simple and efficient design for semantic segmentation with transformers. In Advances in Neural Information Processing Systems 34 (NeurIPS 2021), pp. 12077‚Äì12090, 2021. Yang, X., Shih, S.-M., Fu, Y ., Zhao, X., and Ji, S. Your ViT is secretly a hybrid discriminative-generative diffusion model. arXiv preprint arXiv:2208.07791, 2022. 10A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Zhang, Q. and Chen, Y . Fast sampling of diffusion models with exponential integrator. In Proceedings of The 39th International Conference on Machine Learning (ICML 2023), 2023. 11A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models A. Experimental Details A.1. How to design dropping schedules? Diverse Time-dependent Dropping Schedules In Table 1, we briefly introduce the difference between the diverse sched- ules, D1 to D6. We hereby provide the formal definition of D- n schedules. We refer the reader to Table 5. First, the sampling time [0, 1] is divided into ten intervals with equal length. For the DiT architecture, we designated the blocks to be dropped among the total of 28 blocks. In the case of D1-DiT, we utilized all 28 blocks near the data. As we moved towards the noise side, we gradually discarded some blocks per interval, resulting in a final configuration of using the smallest number of blocks near the noise. The higher the number following ‚ÄôD‚Äô, the greater the amount of discarded blocks, thereby reducing the processing time of the diffusion model. For the most accelerated configuration, D7-DiT, we designed a schedule where only 8 blocks pass near the noise. Table 5.Number of blocks used for varying dropping schedules. All schedules use the same number of blocks within a fixed time interval. Of note, n in D-n schedule represents the acceleration scale. For instance, D3-DiT and D3-U-ViT schedules bring similar scales in terms of acceleration in sampling speed. Reported acceleration performance is measured with DDPM and EM solver applied to DiT and U-ViT, respectively. Schedule Acceleration Sampling timestept [0,0.1] [0 .1,0.2] [0 .2,0.3] [0 .3,0.4] [0 .4,0.5] [0 .5,0.6] [0 .6,0.7] [0 .7,0.8] [0 .8,0.9] [0 .9,1.0] D2-DiT 23.43% 28 28 25 25 22 22 19 19 16 16 D3-DiT 30.46% 28 28 24 24 20 20 16 16 12 12 D4-DiT 34.56% 28 28 26 24 20 18 12 10 8 8 D7-DiT 38.92% 28 28 24 21 18 15 10 10 8 8 D1-U-ViT 21.3% 6 6 4 4 2 2 2 2 1 1 D2-U-ViT 24.8% 5 5 4 4 2 2 1 1 1 1 D3-U-ViT 29.7% 3 3 2 2 2 2 1 1 1 1 D6-U-ViT 32.6% 2 2 2 2 1 1 1 1 1 1 For the U-ViT architecture as we depicted in Figure 8, we aimed to preserve the residual connections by discarding sub- blocks other than nn.Linear, rather than skipping the entire building block. Additionally, the target of dropping was limited to the decoder part, distinguishing it from DiT. Similarly, for D1-U-ViT, we allowed the entire decoder consisting of 6 blocks to pass near the data, and as we moved towards the noise side, we gradually discarded a single block per interval, resulting in only 1 blocks passing near the noise, while the remaining blocks only passed through nn.Linear. Block 1 ùíô(ùüé)ùíô(ùëª) Block 2Decoder Decoder Block 4 Decoder Decoder ùíô(ùüé)ùíô(ùëª) ùë´ùíäùëª ùëº- ùëΩùíäùëª Block 1 Decoder Block 2 Block 4 Decoder Figure 8.Schematic for the dropping schedules of DiT (left) and U-ViT (right). Due to the existence of residual connections in U-ViT, dropping encoder or decoder blocks in a straightforward manner cause severe performance degradation. In the case of U-ViT, the decoder blocks, except for the linear layer connected to encoder residual connections, are dropped. A.2. Pseudo-code for fine-tuning diffusion models 12A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Algorithm 1 Adjusting the output of intermediate building block of diffusion models Require: Training dataset D, Teacher parameter Œ∏T = [Œ∏1 T , . . . , Œ∏N T ], Student parameter Œ∏S = [Œ∏1 S, . . . , Œ∏N S ], EMA rate Œ±, Pre-defined Exit Schedule S(t), Time-dependent coefficient Œª(t), Re-weighting cycle C, Learning rate Œ∑. Œ∏T ‚Üê Œ∏S, t‚àº [0, 1] while not converged do Sample a mini-batch B ‚àº D. for i = 1, . . . ,|B| do Take the input xi from B. for l = 1, . . . , Ndo if l ‚â§ S(t) then Àúxi ‚Üê perturb(xi, t) ‚Ñìi ‚Üê Œª(t) ¬∑ loss( Àúxi, t) else Break for loop end if end for end for Œ∏S ‚Üê Œ∏S ‚àí Œ∑‚àáŒ∏S 1 |B| P i ‚Ñìi. Update Œ∏T ‚Üê Œ±Œ∏T + (1 ‚àí Œ±)Œ∏S end while A.3. Computational Efficiency of ASE Additional Fine-tuning cost of ASE Compared with ToMe (Bolya & Hoffman, 2023) and Block Caching (Wimbauer et al., 2023), our method requires fine-tuning. Nonetheless, we demonstrate its negligible fine-tuning cost and high effi- ciency by reporting the computational costs for fine-tuning in Table 6. Table 6.Fine-tuning costs when we apply ASE into pre-trained DiT on ImageNet and U-ViT on CelebA. These tables show the number of iterations and batch sizes used during the fine-tuning process. (DiT) ImageNet iteration * batch size Baseline 400K * 256 D2-DiT 400K * 32 (12.50 %) D3-DiT 450K * 32 (14.06 %) D4-DiT 500K * 32 (15.63 %) (U-ViT) CelebA iteration * batch size Baseline 500K * 128 D1-U-ViT 40K * 128 (8 %) D2-U-ViT 50K * 128 (10 %) D3-U-ViT 150K * 64 (15 %) D6-U-ViT 200K * 64 (20 %) Results on actual inference time of ASE In Table 7, we provide additional results on wall-clock time. We note that the acceleration rate in the original paper is also measured in terms of wall-clock time. Table 7.Wall-clock time of generating samples with ASE-enhanced models. Left table is the result of DiT model fine-tuned on ImageNet and right table is the result of U-ViT model fine-tuned on CelebA. (DiT) ImageNet DDPM-250 FID (‚Üì) Wall-clock time (s) (‚Üì) Baseline 9.078 59.60 D2-DiT 8.662 45.63 D3-DiT 8.647 41.44 D4-DiT 9.087 39.00 D7-DiT 9.398 36.40 (U-ViT) CelebA EM-1000 FID (‚Üì) Wall-clock time (s) (‚Üì) Baseline 2.944 216.70 D1-U-ViT 2.250 170.54 D2-U-ViT 2.255 162.95 D3-U-ViT 3.217 152.34 D6-U-ViT 4.379 146.05 13A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models B. Related Work Transformers in Diffusion Models. The pioneering diffusion models (Ho et al., 2020; Song & Ermon, 2019; Dhariwal & Nichol, 2021), especially in the field of image synthesis, have adopted a U-Net (Ronneberger et al., 2015) backbone architecture with additional modifications including the incorporation of cross- and self-attention layers. Motivated by the recent success of transformer (Vaswani et al., 2017) networks in diverse domains (Brown et al., 2020; Devlin et al., 2019; Xie et al., 2021; Strudel et al., 2021; Liu et al., 2022), several studies have attempted to leverage the Vision Transformer (ViT) (Dosovitskiy et al., 2021) architecture for diffusion models. Gen-ViT (Yang et al., 2022) is a pioneering work that shows that standard ViT can be used for diffusion backbone. U-ViT (Bao et al., 2022) enhances ViT‚Äôs performance by adding long skip connections and additional convolutional operation. Diffusion Transformers (DiTs) (Peebles & Xie, 2022) investigate the scalability of transformers for diffusion models and demonstrate that larger models consistently exhibit improved performance, albeit at the cost of higher GFLOPs. Our approach focuses on enhancing the efficiency of the transformer through adaptive block selection during calculations, and can be applied to existing transformer-based approaches, such as DiTs, to further optimize their performance. C. Further Analysis on Baselines Analysis on ToMe In this section, we conducted experiments on three different cases for applying ToMe to the building block of a given architecture. The ‚ÄòF‚Äô schedule denotes applying ToMe starting from the front-most block, the ‚ÄòR‚Äô schedule denotes starting from the back-most block, and the ‚ÄòB‚Äô schedule represents symmetric application from both ends. In the Figure 3, we report the experiment results that showed the most competitive outcomes. Furthermore, we present the remaining experiments conducted using various merging schedules, as illustrated in Table 8, Table 9. In summary, for the DiT architecture, the ‚ÄòB‚Äô schedule performed well, while the ‚ÄòR‚Äô schedule demonstrated satisfactory performance for the U-ViT architecture. Table 8.Diverse merging schedule experiments on DiT with DDIM sampler. DDIM-50 B2 B4 B6 B8 All FID (‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) attn-ratio-2-down-1 9.172 0.29% 9.421 0.37% 10.43 0.60% 13.926 0.69% 117.194 1.92% attn-ratio-3-down-1 9.313 0.49% 9.745 0.82% 12.918 1.03% 22.495 1.45% 170.170 6.08% attn-ratio-4-down-1 9.409 0.85% 10.314 1.59% 17.567 2.27% 37.763 2.97% 214.759 10.34% attn-ratio-5-down-1 9.741 0.91% 11.284 2.26% 25.675 2.63% 58.550 4.07% 247.608 16.66% attn-ratio-6-down-1 10.014 0.99% 12.441 2.34% 38.124 3.72% 81.987 5.07% 274.591 21.55% Table 9.Diverse merging schedule experiments on U-ViT with DPM sampler. DPM-50 R2 R3 R4 R5 FID (‚Üì) Accel. (‚Üë) FID ( ‚Üì) Accel. (‚Üë) FID ( ‚Üì) Accel. (‚Üë) FID ( ‚Üì) Accel. (‚Üë) attn-ratio-2-down-1 38.505 -3.98% 45.544 -5.89% 65.755 -7.51% 79.086 -9.15% attn-ratio-3-down-1 120.596 -2.97% 141.073 -4.53% 200.132 -5.85% 232.040 -7.07% attn-ratio-4-down-1 264.153 -2.13% 279.270 -2.76% 311.823 -3.69% 319.599 -4.57% attn-ratio-5-down-1 308.350 -1.13% 315.334 -1.53% 332.565 -1.90% 343.486 -2.02% attn-ratio-6-down-1 330.501 0.05% 344.353 0.41% 362.002 0.69% 372.612 1.10% Analysis on Block Caching To ensure fair comparison between baseline methods, we faithfully implement block caching algorithm on both DiT and U-ViT architecture. In this experiment, we applied it to the attention part of the U-ViT blocks, and Table 10 shows the trade-off between generation quality and inference speed depending on the presence or absence of the scale-shift mechanism. 14A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models D. Qualitative Comparison We present comprehensive experimental results, primarily including qualitative analyses. Figure 9 and Figure 10 shows the superior quality of generated samples under various dropping schedules. Additionally, in the Figure 11 and Figure 12, we show the robustness of ASE across varing sampling timesteps. Notably, we provide visual representations of randomly generated images for each time-dependent early exiting schedule. In the Figure 13, it illustrates the results obtained by sampling from fine-tuned DiT checkpoint using both the DDPM and DDIM sampler. Similarly, in the Figure 14, it exhibits the results obtained by sampling from fine-tuned U-ViT checkpoint using both the EM and DPM sampler. 59.6s, 0% 45.63s, 23.4%39.0s, 34.5% 22.5s, 0% 17.7s, 21.3%14.7s, 34.6% 5.71s, 0% 4.51s, 21.0%3.74s, 34.5% DDPM solverDDIM solverDPM solver Figure 9.Images sampled from ASE-enhanced DiT model with diverse dropping schedules. 20.9s, 0% 18.0s, 13.8%15.5s, 25.8% DPM solver Figure 10.Images sampled from ASE-enhanced U-ViT model with diverse dropping schedules. Table 10.Additional block caching experiments on U-ViT with DPM sampler. DPM-50 Attn(wo SS) Attn(w SS) FID (‚Üì) Accel. (‚Üë) FID (‚Üì) Accel. (‚Üë) attn-ths-0.1 4.462 9.70% 3.955 9.06% attn-ths-0.2 14.083 18.73% 9.707 18.11% attn-ths-0.3 53.770 22.80% 32.518 22.35% attn-ths-0.4 60.390 24.98% 45.523 24.26% 15A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 25 step15step10 step 25 step15step10 step DPM solverDPM solver Figure 11.Images sampled from the fine-tuned DiT model with DPM sampler. 20 step15 step10 step 20 step15 step10 step DPM solverDPM solver Figure 12.Images sampled from the fine-tuned U-ViT model with DPM sampler. 16A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Figure 13.Images sampled from the fine-tuned DiT model. Top: DDPM sampler-250 steps; Bottom: DDIM sampler-50 steps. 17A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Figure 14.Images sampled from the fine-tuned U-ViT model. Top: EM solver-1000 steps; Bottom: DPM solver-25 steps. 18",
      "meta_data": {
        "arxiv_id": "2408.05927v1",
        "authors": [
          "Taehong Moon",
          "Moonseok Choi",
          "EungGu Yun",
          "Jongmin Yoon",
          "Gayoung Lee",
          "Jaewoong Cho",
          "Juho Lee"
        ],
        "published_date": "2024-08-12T05:33:45Z",
        "pdf_url": "https://arxiv.org/pdf/2408.05927v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the slow sampling speed of diffusion models, a practical bottleneck due to repeated score estimation network evaluations. It proposes Adaptive Score Estimation (ASE), a novel early-exiting framework that adaptively allocates compute for score estimation by skipping a subset of network parameters based on a time-dependent exit schedule. The key observation is that computation difficulty varies across time steps (easier near noise, harder near data). ASE significantly improves sampling throughput by approximately 25-30% without compromising image quality, seamlessly integrates with various faster sampling solvers (DDIM, DPM-Solver, DDPM, EM), and reduces processing time per time step. Furthermore, the method demonstrates robustness across different time steps and effectively mitigates the negative transfer phenomenon in diffusion models operating as multi-task learners.",
        "methodology": "The core methodology is Adaptive Score Estimation (ASE), an early-exiting scheme inspired by techniques used in Large Language Models. It introduces a time-varying block-dropping schedule where more network parameters (blocks) are skipped as the time step 't' approaches 1 (closer to the noise distribution, considered 'noise-easy'), while full parameters are retained as 't' approaches 0 (closer to the data distribution, considered 'data-hard'). A novel fine-tuning algorithm is proposed to adjust the model after block removal, focusing on updating minimal information near t‚Üí0 and unseen information near t‚Üí1. This fine-tuning process leverages Exponential Moving Average (EMA) for stable, gradual parameter modification and weighted coefficients Œª(t) in the loss function to prioritize learning specific score distributions. For DiT architecture, entire building blocks are skipped. For U-ViT, dropping is limited to the decoder part, with linear layers preserved within blocks due to long skip connections, to prevent substantial information loss.",
        "experimental_setup": "The method was evaluated using three pre-trained diffusion models: DiT XL/2 on ImageNet (256x256), U-ViT-S/4 on CelebA (64x64), and PixArt-Œ±-SAM-256 (fine-tuned on 100K SAM data). Fine-tuning employed a hybrid loss with a re-weighted time coefficient, linear noise schedule, AdamW optimizer (LR 2e-5), cosine annealing LR for U-ViT, and batch sizes of 64 (DiT) or 128 (U-ViT). Forward diffusion used 1000 time steps. Evaluation metrics included Fr√©chet Inception Distance (FID) for image generation quality (5,000 samples, MS-COCO for text-to-image) and wall-clock time on a single NVIDIA A100 GPU for sampling speed. Baselines included DeeDiff, Token Merging (ToMe), and Block Caching (with and without shift-scale adjustment). Compatibility was demonstrated with DDPM, DDIM, DPM-Solver, and Euler-Maruyama (EM) solvers. Ablation studies on U-ViT (CelebA) investigated various time-dependent dropping schedules, and a negative transfer study compared baseline, further-trained, multi-experts, and mixed-k models.",
        "limitations": "A primary limitation is that the dropping schedule for the early-exiting scheme is manually designed. The paper acknowledges the need for automated methods to find an optimal schedule. Furthermore, the authors highlight general ethical concerns regarding diffusion models: the potential for misuse in generating fake content (deepfakes, misleading information) and the risk of embedding harmful biases and emphasizing stereotypes due to training on internet-sourced data.",
        "future_research_directions": "The main future research direction identified is to explore automated methods for finding an optimal dropping schedule for the early-exiting scheme. This would alleviate the current manual design process and potentially lead to even more efficient and effective adaptive computation strategies for diffusion models."
      }
    },
    {
      "title": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference",
      "abstract": "One of the main drawback of diffusion models is the slow inference time for\nimage generation. Among the most successful approaches to addressing this\nproblem are distillation methods. However, these methods require considerable\ncomputational resources. In this paper, we take another approach to diffusion\nmodel acceleration. We conduct a comprehensive study of the UNet encoder and\nempirically analyze the encoder features. This provides insights regarding\ntheir changes during the inference process. In particular, we find that encoder\nfeatures change minimally, whereas the decoder features exhibit substantial\nvariations across different time-steps. This insight motivates us to omit\nencoder computation at certain adjacent time-steps and reuse encoder features\nof previous time-steps as input to the decoder in multiple time-steps.\nImportantly, this allows us to perform decoder computation in parallel, further\naccelerating the denoising process. Additionally, we introduce a prior noise\ninjection method to improve the texture details in the generated image. Besides\nthe standard text-to-image task, we also validate our approach on other tasks:\ntext-to-video, personalized generation and reference-guided generation. Without\nutilizing any knowledge distillation technique, our approach accelerates both\nthe Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41$\\%$ and 24$\\%$\nrespectively, and DiT model sampling by 34$\\%$, while maintaining high-quality\ngeneration performance.",
      "full_text": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference Senmao Li1‚àó, Taihang Hu1‚àó, Joost van de Weijer2, Fahad Shahbaz Khan3,4, Tao Liu1 Linxuan Li1, Shiqi Yang5, Yaxing Wang1‚Ä†, Ming-Ming Cheng1, Jian Yang1 1VCIP, CS, Nankai University,2Computer Vision Center, Universitat Aut√≤noma de Barcelona 3Mohamed bin Zayed University of AI, 4Linkoping University, 5Independent Researcher, Tokyo {senmaonk, hutaihang00, ltolcy0, linxuanli520, shiqi.yang147.jp}@gmail.com joost@cvc.uab.es, fahad.khan@liu.se, {yaxing,cmm,csjyang}@nankai.edu.cn https://sen-mao.github.io/FasterDiffusion Custom Diffusion 2.42s 41%‚Üì Custom Diffusion w/ Ours 1.42s ControlNet 3.20s ControlNet w/ Ours 1.52s Stable Diffusion DDIM 2.42s DPM-Solver++ 1.13s DPM-Solver++ 0.64s41%‚ÜìDDIM w/ Ours 1.42s  43%‚Üì  24%‚Üì 20%‚Üì DeepFloyd-IF DPM-Solver++ 16.09s DDPM 34.55s DPM-Solver++ w/ Ours 12.97s DDPM w/ Ours 26.27s 34%‚Üì 51%‚Üì DiT 26.25s DiT w/ Ours 17.35s 32%‚Üì VideoFusion 1.12s VideoFusion w/ Ours 0.76s Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image). Abstract One of the main drawback of diffusion models is the slow inference time for image generation. Among the most successful approaches to addressing this problem are distillation methods. However, these methods require considerable computational resources. In this paper, we take another approach to diffusion model acceleration. We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features. This provides insights regarding their changes during the inference process. In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different time- steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process. Additionally, we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on ‚àóEqual contribution. Author ordering determined by coin flip over a Google Hangout. ‚Ä†The corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2312.09608v2  [cs.CV]  15 Oct 2024other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41 % and 24% respectively, and DiT model sampling by 34%, while maintaining high-quality generation performance. 1 Introduction One of the popular paradigms in image generation, Diffusion Models (DMs) [1, 2, 3] have recently achieved significant breakthroughs in various domains, including text-to-video generation [4, 5, 6], personalized image generation [7, 8, 9] and reference-guided image generation [10, 11, 12]. While diffusion models produce images of exceptional visual quality, their primary drawback lies in the prolonged inference time. The original diffusion model had an inference time several orders of magnitude slower than, for instance, GANs. One of the challenges hindering the acceleration of diffusion models is their inherent sequential denoising process, which limits the possibilities of effective parallelization. To improve inference time speed of diffusion models, several methods have been developed, that can roughly be divided in two sets of approaches. Firstly, involving step reduction, the aim is to reduce the number of sampling steps within diffusion model inference, such as DDIM [ 13] and DPM-Solver [14], which have significantly reduced the number of sampling steps. Secondly, in contrast, knowledge distillation progressively distills a slow (many-step) teacher model into a faster (few-step) student model [15, 16]. Some recent works [17, 18, 19] excel at generating high-fidelity images in a few-step sampling scenario but face challenges in maintaining quality and diversity in one-step sampling. The main drawback of the distillation methods is that they require retraining to perform the distillation into faster diffusion models. Figure 2: Visualising the hierarchical features 1. We applied PCA to the hierarchical features following PnP [20] and used the top three leading components as an RGB image for visualization. The encoder fea- tures change minimally and have similarities at many time-steps (top), while the decoder features exhibit sub- stantial variations across different time-steps (bottom). Orthogonal to these methods, we take a closer look at the sequential nature of the denoising process. We focus on the char- acteristics of the encoder in pretrained diffusion models (e.g., the SD and the DiT [21]2) Interestingly, based on our anal- ysis presented in Sec 3.2, we discover that encoder features change minimally (Fig. 3a) and have a high degree of similar- ity (Fig. 2 (top)), whereas the decoder fea- tures exhibit substantial variations across different time-steps (Fig. 3a and Fig. 2 (bot- tom)). This insight is relevant because it allows us to circumvent the computation of the encoder during multiple time-steps. As a consequence, the decoder computations which are based on the same encoder input can be performed in parallel. Instead, we reuse the computed encoder features computed at one time-step (since these change minimally) as input to adjacent decoders during the following time-steps. More recently, both DeepCache[ 22] and CacheMe[23] leverage feature similarity to achieve acceleration. However, they rely on sequential denoising, as well as CacheMe requires fine-tuning. Unlike these methods, our approach supports parallel processing, which leads to faster inference (Tab. 2). We show that the proposed propagation scheme accelerates the SD sampling by 24% , DeepFolyd-IF sampling by 18%, and DiT sampling by 27%. Furthermore, since the same encoder features (from previous time-steps) can be used as the input to the decoder of multiple later time-steps, this makes it possible to conduct multiple time-steps decoding concurrently. This parallel procession accelerates SD sampling by 41%, DeepFloyd-IF sampling by 24%, and DiT sampling by 34%. Furthermore, to alleviate the deterioration of the generated quality, we introduce a prior noise injection strategy to preserve the texture details in the generated images. With these contributions, our proposed method achieves improved sampling efficiency while maintaining high image generation quality. 1See the visualisation of the hierarchical features for all blocks in Appendix A 2We define the first several transformer blocks of DiT as the encoder, and the remaining ones of the transform blocks as the decoder. 2(a) (b) (c) Figure 3: Analyzing the UNet in Diffusion Model. (a) Feature evolving across adjacent time-steps is measured by MSE. (b) We extract the hierarchical features output of different layers of the UNet at each time-step, average them along the channel dimension to obtain two-dimensional hierarchical features, and then calculate their Frobenius norm. (c) The hierarchical features of the UNet encoder show a lower standard deviation, while those of the decoder exhibit a higher standard deviation. Importantly, our method can be combined with several approaches existing to speed up DMs. The main advantage of our method with respect to distillation-based approaches, is that our method can be applied at inference time, and does not require retraining a new faster distillation model; a process that is computationally very demanding and infeasible for actors with limited computational budget. Finally, we evaluate the effectiveness of our approach across a wide range of conditional diffusion- based tasks, including text-to-video generation (e.g., Text2Video-zero [ 4] and VideoFusion [ 5]), personalized image generation (e.g., Dreambooth [7]) and reference-guided image generation (e.g., ControlNet [10]). To summarize, we make the following contributions: ‚Ä¢ We conduct a thorough empirical study of the features of the UNet in the diffusion model showing that encoder features vary minimally (whereas decoder feature vary significantly). ‚Ä¢ We propose a parallel strategy for diffusion model sampling at adjacent time-steps that significantly accelerates the denoising process. Importantly, our method does not require any training or fine-tuning technique. ‚Ä¢ Furthermore, we also present a prior noise injection method to improve the image quality (mainly improving the quality of high-frequency textures). ‚Ä¢ Our method can be combined with existing methods (like DDIM, and DPM-solver) to further accelerate diffusion model inference time. 2 Related Work Denoising diffusion model. Recently, Text-to-image diffusion models [1, 24, 25, 26] have made significant advancements. Notably, Stable Diffusion and DeepFloyd-IF stand out as two of the most successful diffusion models available within the current open-source community. These models, building upon the UNet architecture, are versatile and can be applied to a wide range of tasks, including image editing [27, 28], super-resolution [29, 30], segmentation [31, 32], and object detection [33, 34]. Given the strong scalability of transformer networks, DiT [21] investigates the transformer backbone for diffusion models. Diffusion model acceleration. Diffusion models use iterative denoising with UNet for image gener- ation, which is time-consuming. There are plenty of works trying to address this issue. One strategy involves employing efficient diffusion model solvers, such as DDIM [ 13] and DPM-Solver [ 14], which have demonstrated notable reductions in sampling steps. Additionally, ToMe [ 35] exploits token redundancy to minimize the computations necessary for attention operations [36]. Conversely, knowledge distillation methods, exemplified by techniques like progressive simplification by student models [15, 16], aim to streamline existing models. Some recent studies combine model compression with distillation to achieve faster sampling [37, 38]. Orthogonal to these approaches, we introduce a novel method for enhancing sampling efficiency in DMs inference. We show that our method can be combined with several existing speed-up methods for further acceleration. DeepCache[22] and CacheMe[23] are two recent works that leverage feature similarity to achieve acceleration. DeepCache [22] adopts a strategy of rudely reusing features cached from the previous step, requiring iterative denoising. Furthermore, CacheMe [23] requires additional fine-tuning for 3(d) Decoder propagation Sample (e) Non-uniform encoder propagation Parallel encoder propagation Sample (b) UNet architecture (a) Standard SD sampling Sample (c) Encoder propagation Sample Figure 4: (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. We omit the encoder at certain adjacent time-steps and reuse in parallel the encoder features in the previous time-steps for the decoder. Applying encoder propagation for uniform strategy every two iterations. Note, at time-step t-1, predicting noise does not require zt-1 (i.e., Eq. 1: zt‚àí2 = qŒ±t‚àí2 Œ±t‚àí1 zt‚àí1 + ‚àöŒ±t‚àí2 \u0010q 1 Œ±t‚àí2 ‚àí 1 ‚àí q 1 Œ±t‚àí1 ‚àí 1 \u0011 ¬∑ œµŒ∏(\u0018\u0018\u0018 XXXzt‚àí1, t‚àí 1, c)). (d) Decoder propagation. The generated images often fail to cover some specific objects in the text prompt. For example, given one prompt case ‚ÄúA man with a beard wearing glasses and a beanie\", this method fails to generate the glasses subject. See Appendix F for quantitative evaluation. (e) Applying encoder propagation for non- uniform strategy. By benefiting from our propagation scheme, we are able to perform the decoder in parallel at certain adjacent time-steps. better performance. In contrast, our approach enables parallel processing, leading to considerably faster inference. 3 Method We first briefly revisit the architecture of the Stable Diffusion (SD) (Sec. 3.1), and then conduct a comprehensive analysis for the hierarchical features of the UNet (Sec. 3.2). Our analysis shows that it is possible to parallelize the diffusion model denoising process partially. Thus, we introduce a novel method to accelerate the diffusion sampling while still largely maintaining the generation quality and fidelity (Sec. 3.3). 3.1 Latent Diffusion Model In the diffusion inference stage, the denoising network œµŒ∏ takes as input a text embedding c, a latent code zt and a time embedding, predicts noise, resulting in a latent zt‚àí1 using the DDIM scheduler [13]: zt‚àí1 = qŒ±t‚àí1 Œ±t zt +‚àöŒ±t‚àí1 \u0010q 1 Œ±t‚àí1 ‚àí1‚àí q 1 Œ±t ‚àí1 \u0011 ¬∑œµŒ∏(zt, t,c), (1) where Œ±t is a predefined scalar function at time-step t (t = T, ...,1). The typical denoising network uses a UNet-based architecture. It consists of an encoder E, a bottleneck B, and a decoder D, respectively (Fig. 4b). The hierarchical features extracted from the encoder E are injected into the decoder D by a skip connection (Fig. 4a). For convenience of description, we divide the UNet network into specific blocks: E = {E(¬∑)s}, B = {B(¬∑)8}, and D = {D(¬∑)s}, where s ‚àà {8, 16, 32, 64} (see Fig. 4b). Both E(¬∑)s 3 and D(¬∑)s represent the block layers with input resolution s in both encoder and decoder, respectively. Diffusion Transformer (DiT) [21] is a novel architecture for diffusion models. It replaces the UNet backbone with a transformer, which consists of 28 blocks. Based on our observations, we define the first 18 blocks as the encoder, and the remaining 10 blocks as the decoder (see Appendix A.3). 3.2 Analyzing the UNet in Diffusion Model In this section, we take the UNet-based diffusion model as example to analyze the properties of the pretrained diffsuion model. We delve into the UNet which consists of the encoderE, the bottleneck B, 3Once we replace the ¬∑ with specific inputs in E(¬∑)s, we define that it represents the feature of E(¬∑)s 4and the decoder D, for a deeper understanding of the different parts of the UNet. Note the following observed properties also exist in DiT (see Appendix A.3). Feature evolution across time-steps. We experimentally observe that the encoder features exhibit a subtle variation at adjacent time-steps, whereas the decoder features exhibit substantial variations across different time-steps (see Fig. 3a and Fig. 2). Specifically, given a pretrained diffusion model, we iteratively produce a latent code zt (see Eq. 1), and the corresponding hierarchical features: {E(zt, c, t)s}, {B(zt, c, t)8}, and {D(zt, c, t)s} (s ‚àà {8, 16, 32, 64}) 4, as shown in Fig. 4b. Here, we analyze how the hierarchical features change at adjacent time-steps. To achieve this goal, we quantify the variation of the hierarchical features as follows: ‚àÜE(¬∑)s = 1 d√ós2 ‚à•E(zt, c, t)s ‚àí E(zt‚àí1, c, t‚àí 1)s‚à•2 2, (2) where d represents the number of channels in E(zt, c, t)s. Similarly, we also compute ‚àÜB(¬∑)8 and ‚àÜD(¬∑)s . As illustrated in Fig. 3a, for both the encoder E and the decoder D, the curves exhibit a similar trend: in the wake of an initial increase, the variation reaches a plateau and then decreases, followed by a continuing growth towards the end. However, the extent of change in ‚àÜE(¬∑)s and ‚àÜD(¬∑)s is quantitatively markedly different. For example, the maximum value and variance of the‚àÜE(¬∑)s are less than 0.4 and 0.05, respectively (Fig. 3a (zoom-in area)), while the corresponding values of the ‚àÜD(¬∑)s are about 5 and 30, respectively (Fig. 3a). Furthermore, we find that ‚àÜD(¬∑)64 , the change of the last layer of the decoder, is close to zero. This is due to the output of the denoising network being similar at adjacent time-steps [39]. In conclusion, the overall feature change ‚àÜE(¬∑)s is smaller than ‚àÜD(¬∑)s throughout the inference phase. Feature evolution across layers. We experimentally observe that the feature characteristics are significantly different between the encoder and the decoder across all time-steps. For the encoder E the intensity of the change is slight, whereas it is very drastic for the decoder D. Specifically we calculate the Frobenius normfor hierarchical features E(zt, c, t)s across all time-steps, dubbed as FE(¬∑)s = {FE(zT ,c,T)s , ...,FE(z1,c,1)s }. Similarly, we compute FB(¬∑)8 and FD(¬∑)s , respectively. Fig. 3b shows the feature evolution across layers with a boxplot 5. Specifically, for \b FE(¬∑)s \t and\b FB(¬∑)8 \t , the box is relatively compact, with a narrow range between their first-quartile and third- quartile values. For example, the maximum box height (FE(.)32) of these features is less than 5 (see Fig. 3b (zoom-in area)). This indicates that the features from both the encoder E and the bottleneck B slightly change. In contrast, the box heights corresponding to {D(¬∑)s} are relatively large. For example, for the D(.)64 the box height is over 150 between the first quartile and third quartile values (see Fig. 3b). Furthermore, we also provide a standard deviation (Fig. 3c), which exhibits similar phenomena to Fig. 3b. These results show that the encoder features have relatively small discrepancy and a high degree of similarity across all layers. However, the decoder features evolve drastically. Could we omit the Encoder at certain time-steps? As indicated by the previous experimental analysis, we observe that, during the denoising process, the decoder features change drastically, whereas the encoder E features change minimally, and have a high degree of similarities at certain adjacent time-steps. Therefore, as shown in Fig. 4c, We propose to omit the encoder at certain time-steps and use the same encoder features for several decoder steps. This allows us to compute these multiple decoder steps in parallel. Specifically, we delete the encoder at time-step t ‚àí 1 (t ‚àí 1 < T), and the corresponding decoder (including the skip connections) takes as input the hierarchical outputs of the encoder E from the previous time-step t, instead of the ones from the current time-stept‚àí1 like the standard SD sampling (for more detail, see Sec. 3.3). When omitting the encoder at a certain time-step, we are able to generate similar images (Fig. 4c) like standard SD sampling (Fig. 4a, Tab. 1 (the first and second rows) and additional results in Appendix F). Alternatively, if we use a similar strategy for the decoder (i.e., decoder propagation), we find the 4The feature resolution is half of the previous one in the encoder and two times in the decoder. Note that the feature resolutions of E(.)8, B(.)8 and D(.)64 do not change in the SD model. 5Each boxplot contains the minimum (0th percentile), the maximum (100th percentile), the median (50th percentile), the first quartile (25th percentile) and the third quartile (75th percentile) values of the feature Frobenius norm (e.g., {FE(zT ,c,T)s , ...,FE(z1,c,1)s }). 5generated images often fail to cover some specific objects in the text prompt (Fig. 4d). For example, when provided with prompt ‚ÄúA man with a beard wearing glasses and a beanie\", the SD model fails to synthesize ‚Äúglasses\" when applying decoder propagation. This is due to the fact that the semantics are mainly contained in the features from the decoder rather than the encoder [40]. The encoder propagation, which uses encoder outputs from previous time-step as the input to the current decoder, could speed up the diffusion model sampling at inference time. In the following Sec. 3.3, we give further elaborate on encoder propagation. 3.3 Encoder propagation Diffusion sampling, combining iterative denoising with transformers, is time-consuming. Therefore we propose a novel and practical diffusion sampling acceleration method. During the diffusion sampling process t= {T, ...,1}, we refer to the time-steps where encoder propagation is deployed, as non-key time-steps denoted as tnon-key= n tnon-key 0 , ..., tnon-key N‚àí1 o . The remaining time-steps are dubbed as tkey= n tkey 0 , tkey 1 , ..., tkey T‚àí1‚àíN o . In other words, we do not use the encoder at time-steps tnon-key, and leverage the hierarchical features of the encoder from the time-step tkey. Note we utilize the encoder E at the initial time-step (tkey 0 = T). Thus, the diffusion inference time-steps could be reformulated as \b tkey, tnon-key\t , where tkey ‚à™ tnon-key = {T, ...,1} and tkey ‚à© tnon-key = ‚àÖ. In the following, we introduce both uniform encoder propagationand non-uniform encoder propagation strategies. As shown in Fig. 3a, The encoder feature change is larger in the initial inference phase compared to the later phases throughout the inference process. Therefore, we select more key time-steps in the initial inference phase, and less key time-steps in later phases. We experimentally define the key time-steps as tkey={50, 49, 48, 47, 45, 40, 35, 25, 15} for SD model with DDIM, and tkey={100, 99, 98, . . ., 92, 91, 90, 85, 80, . . ., 25, 20, 15, 14, 13, . . ., 2, 1}6, {50, 49, . . . ,2, 1} and {75, 73, 70, 66, 61, 55, 48, 40, 31, 21, 10} for three stage of DeepFloyd-IF (see detail key time-steps selection in Appendix F.2). The remaining time-steps are categorized as non-key time-steps. We define this strategy as non- uniform encoder propagation (see Fig. 4e). As shown in Fig. 4c, we also explore the time-step selection with fix stride (e.g, 2), dubbed as uniform encoder propagation. Note that our method does not reduce the number of sampling steps. During encoder propagation, the decoder computes for all time-steps, necessitating time embedding inputs for each time-step to maintain temporal coherence (see detail in Appendix D). Tab. 5 reports the results of the ablation study, considering various combinations ofkey and non-key time-steps. These results indicate that the set of non-uniform key time-steps performs better in generating images. SD (DDIM) 2.42s 1.82s24%‚Üì Encoder propagation 41%‚Üì1.42s Parallel encoder propagation Figure 5: Comparing with SD (left), encoder prop- agation reduces the sampling time by 24% (mid- dle). Furthermore, parallel encoder propagation achieves a 41% reduction in sampling time (right). Parallel non-uniform encoder propagation. When applying the non-uniform encoder prop- agation strategy, at time-step t ‚àà tnon‚àíkey the decoder inputs do not rely on the encoder out- puts at time-step t (see Fig. 4e). Instead, it relies on the encoder output at the previous nearest key time-step. This allows us to perform par- allel non-uniform encoder propagationat these adjacent time-steps in tnon‚àíkey. We perform de- coding in parallel from t to t ‚àí k + 1 time-steps. This technique further improves the inference efficiency since the decoder forward in multiple time-steps could be conducted concurrently. We indicate this as parallel-batch non-key time-steps. As shown in Fig. 5 (right), this further reduces evaluation time by 41% for the SD model. Prior noise injection. Although the encoder propagation could improve the efficiency in the inference phase, we observe that it leads to a slight loss of texture information in the generated 6The ellipsis in tkey denotes each time-step between the time-steps on either side of the ellipsis. For example, 80...25 means that every time-step between 80 and 25 is included. 6results (see Fig. 6 (left, middle)). Inspired by related works [ 41, 42], we propose a prior noise injection strategy. It combines the initial latent code zT into the generative process at subsequent time-step (i.e., zt), following zt = zt + Œ± ¬∑ zT , if t < œÑ, where Œ± = 0.003 is the scale parameter to control the impact of zT . And we start to use this injection mechanism from œÑ = 25 step. This strategic incorporation successfully improves the texture information. Importantly, it demands almost negligible extra computational resources. We observe that the loss of texture information occurs in all frequencies of the frequency domain (see Fig. 6 (right, red, and blue curves)). This approach ensures a close resemblance of generated results in the frequency domain to both SD and zT injection (see Fig. 6 (right, red and green curves)), with the generated images maintaining the desired fidelity (see Fig. 6 (left, bottom)). 4 Experiments SD Ours w/o injectionùëßùëá Ours w/ injectionùëßùëá Generated image Low frequency High frequency Figure 6: (left) We keep the image content through zT injection, slightly compensating for the texture informa- tion loss caused by encoder propagation. (right) The amplitudes of the generated image through zT injection closely resemble those from SD. In our experiments, we assess the speed-up of our method compared to others for in- ference acceleration. We also explore com- bining our method with these approaches. We do not directly compare our method with distillation methods, which offer su- perior results but involve computationally expensive retraining. Datasets and evaluation metrics. We randomly select 10K prompts from the MS-COCO2017 validation dataset [43] and feed them into the text-to-image diffusion model to obtain 10K generated images. For the transformer architecture diffusion model, we randomly generate 50K images from 1000 ImageNet [44] class labels. For other tasks, we use the same settings as baselines (e.g., Text2Video-zero [4], VideoFusion [5], Dream- booth [7] and ControlNet [10]). We use the Fr√©chet Inception Distance (FID) [45] metric to assess the visual quality of the generated images, and the Clipscore [46] to measure the consistency between image content and text prompt. Furthermore, we report the average values for both the computational workload (GFLOPs/image) and sampling time (s/image) to represent the resource demands for a single image. See more detailed implementation information on Appendix A. 4.1 Text-to-image Generation Table 1: Quantitative evaluation7 for both SD and DeepFloyd-IF diffusion models. s/image ‚Üì DM Sampling Method T FID‚Üì Clip- score‚Üë GFLOPs/ image‚Üì Unet of DM DM Stable Diffusion DDIM 50 21.75 0.773 37050 2.23 2.42DDIM w/ Ours 50 21.08 0.783 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì DPM-Solver 20 21.36 0.780 14821 0.90 1.14 DPM-Solver w/ Ours 20 21.25 0.779 1174321%‚Üì 0.4648%‚Üì 0.6443%‚Üì DPM-Solver++ 20 20.51 0.782 14821 0.90 1.13 DPM-Solver++ w/ Ours 20 20.76 0.781 1174321%‚Üì 0.4648%‚Üì 0.6443%‚Üì DDIM + ToMe 50 22.32 0.782 35123 2.07 2.26DDIM + ToMe w/ Ours 50 20.73 0.781 2605326%‚Üì 1.1544%‚Üì 1.3341%‚Üì DeepFloyd-IF DDPM 225 23.89 0.783 734825 33.91 34.55DDPM w/ Ours 22523.73 0.782 62652315%‚Üì25.6125%‚Üì26.2724%‚Üì DPM-Solver++10020.79 0.784 370525 15.19 16.09 DPM-Solver++ w/ Ours 10020.85 0.785 31338115%‚Üì12.0221%‚Üì12.9720%‚Üì We first evaluate the proposed encoder propaga- tion method for the standard text-to-image gen- eration task on both the latent space (i.e., SD) and pixel space (i.e., DeepFloyd-IF) diffusion models. As shown in Tab. 1, we significantly accelerate the diffusion sampling with negligi- ble performance degradation. Specifically, our proposed method decreases the computational burden (GFLOPs) by a large margin (27%) and greatly reduces sampling time to 41% when compared to standard DDIM sampling in SD. Similarly, in DeepFloyd-IF, the reduction in both computational burden and time reaches15% and 24%, respectively. Furthermore, our method can be combined with the latest sampling techniques like DPM-Solver [14], DPM-Solver++ [47], and ToMe [35]. Our method enhances sampling ef- ficiency while preserving good model perfor- mance, with negligible variations of both FID and Clipscore values (Tab. 1 (the third to eighth rows)). Our method achieves good performance across different sampling steps (Fig. 7 and see Appendix D 7We use the official implementation of Clipscore [46] to obtain around 0.75 but around 0.3. See Appendix C. 7Table 2: Comparison with DeepCache and CacheMe. CacheMe is not open-source. Sampling Method T Parallel FID ‚Üì Clipscore ‚Üë s/image DDIM 50 √ó 21.75 0.773 2.42 DDIM w/ DeepCache 50 √ó 21.53 0.770 1.0556%‚Üì DDIM w/ CacheMe 50 √ó ‚Äì ‚Äì 1.3044%‚Üì DDIM w/ Ours 50 ‚úì 21.62 0.775 0.5677%‚Üì Table 3: Quantitative evaluation for DiT. Sampling Method T Image Res. FID ‚ÜìsFID ‚Üì IS ‚Üë Precision ‚ÜëRecall ‚Üë s/image DiT 250 256 2.27 4.60 278.24 0.83 0.57 5.13 DiT w/ Ours250 256 2.31 4.55 276.05 0.82 0.57 3.62 29%‚Üì DiT 250 512 3.04 5.02 240.82 0.84 0.54 26.25 DiT w/ Ours250 512 3.25 5.05 245.13 0.83 0.51 17.35 34%‚Üì Table 4: Quantitative evaluation on text-to-video, personalized generation and reference-guided generation tasks. ‚Ä† and ‚Ä° indicate ‚Äúedges‚Äù and ‚Äúscribble‚Äù conditions, respectively. s/image‚Üì Method T FID‚Üì Clip- score‚Üë GFLOPs/ image‚Üì Unet of SD SD Text2Video-zero 50 - 0.732 39670 12.59/8 13.65/8 Text2Video-zero w/ Ours 50 - 0.731 3069022%‚Üì 9.46/825%‚Üì 10.54/823%‚Üì VideoFusion 50 - 0.700 224700 16.71/16 17.93/16 VideoFusion w/ Ours 50 - 0.700 14868033%‚Üì11.1/1634%‚Üì12.2/1632%‚Üì ControlNet (‚Ä†) 50 13.78 0.769 49500 3.09 3.20 ControlNet (‚Ä†) w/ Ours 50 14.65 0.767 3140037%‚Üì 1.4354%‚Üì 1.5251%‚Üì ControlNet (‚Ä°) 50 16.17 0.775 56850 3.85 3.95 ControlNet (‚Ä°) w/ Ours 50 16.42 0.775 3599037%‚Üì 1.8353%‚Üì 1.9351%‚Üì Dreambooth 50 - 0.640 37050 2.23 2.42 Dreambooth w/ Ours 50 - 0.660 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì CustomDiffusion50 - 0.640 37050 2.21 2.42 CustomDiffusion w/ Ours 50 - 0.650 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì Table 5: Quantitative evaluation in various propa- gation strategies on MS-COCO 2017 10K subset. FTC=FID√óTime/Clipscore. Propagation strategy FID‚Üì Clipscore‚Üë GFLOPs /image‚Üì s/image‚Üì FTC‚Üì Unet of SD SD SD 21.75 0.773 37050 2.23 2.42 68.1 Uniform I tkey={50,48,46,44,42,40,38,36,34,32,30,28, 26,24,22,20,18,16,14,12,10,8,6,4,2} 21.55 0.775 3101116%‚Üì 1.6227%‚Üì 1.8125%‚Üì 50.3 II tkey={50,44,38,32,26,20,14,8,2} 21.54 0.773 2735027%‚Üì 1.2643%‚Üì 1.4640%‚Üì 40.7 III tkey={50,38,26,14,2} 24.61 0.766 2637029%‚Üì 1.1250%‚Üì 1.3644%‚Üì 43.7 Non-uniform I tkey={50,40,39,38,30,25,20,15,5} 22.94 0.776 2735027%‚Üì 1.2643%‚Üì 1.4241%‚Üì 41.9 II tkey={50,30,25,20,15,14,5,4,3} 35.25 0.742 2735027%‚Üì 1.2543%‚Üì 1.4241%‚Üì 67.4 III tkey={50,41,37,35,22,21,18,14,5} 22.14 0.778 2735027%‚Üì 1.2245%‚Üì 1.4241%‚Üì 40.4 IV (Ours) tkey={50,49,48,47,45,40,35,25,15} 21.08 0.783 2735027%‚Üì 1.2145%‚Üì 1.4241%‚Üì 38.2 for quantitative results.). Importantly, these results show that our method is orthogonal and compatible with these acceleration techniques. As shown in Fig. 1, we visualize the generated images with different sampling techniques. Our method still generates high-quality results (see Appendix F for additional results). Our method allows to use multi-GPU to generate one image. With multi-GPU parallel, our proposed method further accelerates the SD sampling by 77%, whereas DeepCache [22] and CacheMe [23] achieve speedups of 56% and 44%, respectively (See Tab. 2). These results indicate that we achieve superior acceleration compared to DeepCache [22] and CacheMe [23]. 4.2 Diffusion Transformer We also evaluate our approach on DiT. As reported in Tab. 3, we achieve accelerations of about29% and 34% for DiT sampling with image resolution of 256 and 512, respectively, while preserving high-quality results (see Figs. 1 and 18). 4.3 Other tasks with text-guided diffusion model Besides the standard text-to-image task, we also validate our proposed approach on other tasks: text-to-video generation, personalized generation, and reference-guided image generation. Text-to-video. To evaluate our method, we combine it with both Text2Video-zero [4] and VideoFu- sion [5]. As reported in Tab. 4 (the second and fourth rows), when combined with our method, the two methods have a reduction of approximately22% to 33% in both computational burden and generation time. These results indicate that we are able to enhance the efficiency of generative processes in the text-to-video task while preserving video fidelity at the same time (Fig. 1 (left, bottom)). As an example, when generating a video using the prompt ‚ÄúFireworks bloom in the night sky\", the VideoFusion model takes 17.92 seconds with 16 frames for the task (1.12s/frame), when combined with our method it only takes 12.27s (0.76s/frame) to generate a high-quality video (Fig. 1 (left, bottom)). Personalized image generation. Dreambooth [7] and Custom Diffusion [8] are two approaches for customizing tasks by fine-tuning text-to-image diffusion models. As reported in Tab. 4 (the ninth to twelfth rows), our method, combining with the two customization approaches, accelerates image 850-Step 40-Step 30-Step 20-Step 15-Step 10-Step 5-Step DDIM DDIM  w/ Ours DPM- Solver++ DPM- Solver++  w/ Ours Figure 7: Generated images at different time-steps. Figure 8: User study results. generation and reduces computational demands. Visually, it maintains the ability to generate images with specific contextual relationships based on reference images. (Fig. 1 (right)) Reference-guided image generation. ControlNet [10] incorporates a trainable encoder, successfully generates a text-guided image, and preserves similar content with conditional information. Our approach can be applied concurrently to two encoders of ControNet. In this paper, we validate the proposed method with two conditional controls: edge and scribble. Tab. 4 (the fifth to eighth row) reports quantitative results. We observe that it leads to a significant decrease in both generation time and computational burden. Furthermore, Fig. 1 (middle, bottom) qualitatively shows that our method successfully preserves the given structure information and achieves similar results as ControlNet. User study. We conducted a user study, as depicted in Fig. 8, and asked subjects to select results. We apply pairwise comparisons (forced choice) with 18 users (35 pairs of images or videos/user). The results demonstrate that our method performs equally well as the baseline methods. 4.4 Ablation study Non-uniform (Ours) SD (DDIM) Uniform Figure 9: Generating image with uniform and non- uniform encoder propagation. The result of uni- form strategy II yields smooth and loses textual compared with SD. Both uniform strategy III and non-uniform strategy I, II and III generate images with unnatural saturation levels. We ablate the results with different selections of both uniform and non-uniform encoder prop- agation. Tab. 5 reports that the performance of the non-uniform setting outperforms the uni- form one in terms of both FID and Clipscore (see Tab. 5 (the third and eighth rows)). Further- more, we explore different configurations within the non-uniform strategy. The strategy, using the set of key time-steps we established, yields better results in the generation process (Tab. 5 (the eighth row)). We further present qualita- tive results stemming from the above choices. As shown in Fig. 9, given the same number of key time-steps, the appearance of nine-step non- uniform strategy I, II and III settings do not align with the prompt ‚ÄúFireflies dot the night sky\". Although the generated image in the two- step setting exhibits a pleasing visual quality, its sampling efficiency is lower than our chosen setting (see Tab. 5 (the second and eighth rows)). Table 6: Quantitative evaluation for prior noise injection. Sampling Method SD (DDIM) SD (DDIM) + Ours w/o zT injection SD (DDIM) + Ours w/ zT injection FID ‚Üì 21.75 21.71 21.08 Clipscore ‚Üë 0.773 0.779 0.783 Effectiveness of prior noise injection. We evaluate the effectiveness of injecting initial zT . As reported in Tab. 6, the differences in FID and Clipscores without zT (the third column), when compared to DDIM and Ours (the second and fourth columns), are approximately 0.01%, which can be considered negligible. While this is not the case for the visual expression of the generated image, it is observed that the output contains complete semantic information with smoothing texture (refer to Fig. 6 (left, the second row)). Injecting the zT aids in maintaining fidelity in the generated results during encoding propagation (see Fig. 6 (left, the third row) and Fig. 6 (right, red and green curves)). 95 Conclusion In this work, We explore the characteristics of the encoder and decoder in UNet of the text-to-image diffusion model and find that encoder feature variation is minimal for many time-steps, while the decoder plays a significant role across all time-steps. Building upon this finding, we propose encoder propagation for efficient diffusion sampling, reducing time on both the UNet-based and the transform- based diffusion models on a diverse set of generation tasks. We conduct extensive experiments and validate that our approach can achieve improved sampling efficiency while maintaining image quality. Limitations: Although our approach achieves efficient diffusion sampling, it faces challenges in generating quality when using a limited number of sampling steps (e.g., 5). In addition, even though our proposed parallelization can also be applied to network distillation approaches [18, 17, 19], we have not explored this direction in this paper and leave it to future research. Acknowledgements This work was supported by NSFC (NO. 62225604) and Youth Foundation (62202243). We acknowledge project PID2022-143257NB-I00, financed by the Spanish Government MCIN/AEI/10.13039/501100011033 and FEDER. Computation is supported by the Supercomputing Center of Nankai University (NKSC). We would like to thank Kai Wang, a postdoctoral researcher at the Computer Vision Center, Universitat Aut√≤noma de Barcelona, for his helpful discussions and comments during the rebuttal period. References [1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684‚Äì10695, 2022. [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840‚Äì6851, 2020. [3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [4] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. [5] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10209‚Äì 10218, 2023. [6] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. [7] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream- booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500‚Äì22510, 2023. [8] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931‚Äì1941, 2023. [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [10] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836‚Äì3847, 2023. [11] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i- adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 10[12] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511‚Äì22521, 2023. [13] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [14] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775‚Äì5787, 2022. [15] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [16] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297‚Äì14306, 2023. [17] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [18] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. [19] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. arXiv preprint arXiv:2312.05239, 2023. [20] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text- driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921‚Äì1930, 2023. [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. [22] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXiv preprint arXiv:2312.00858, 2023. [23] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. arXiv preprint arXiv:2312.03209, 2023. [24] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to- image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479‚Äì36494, 2022. [25] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [26] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023. [27] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to- prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [28] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing.arXiv preprint arXiv:2303.15649, 2023. [29] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015, 2023. [30] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12413‚Äì12422, 2022. [31] Dmitry Baranchuk, Ivan Rubachev, Andrey V oynov, Valentin Khrulkov, and Artem Babenko. Label- efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. [32] Julia Wolleb, Robin Sandk√ºhler, Florentin Bieder, Philippe Valmaggia, and Philippe C Cattin. Diffusion models for implicit image segmentation ensembles. In International Conference on Medical Imaging with Deep Learning, pages 1336‚Äì1348. PMLR, 2022. [33] Walter HL Pinaya, Mark S Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H Mah, Andrew D MacKinnon, James T Teo, Rolf Jager, et al. Fast unsupervised brain anomaly detection and segmentation with diffusion models. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 705‚Äì714. Springer, 2022. 11[34] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffusiondet: Diffusion model for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19830‚Äì19843, 2023. [35] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4598‚Äì4602, 2023. [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [37] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compression of text-to-image diffusion models. arXiv preprint arXiv:2305.15798, 2023. [38] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. arXiv preprint arXiv:2306.00980, 2023. [39] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023. [40] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. What does stable diffusion know about the 3d scene? arXiv preprint arXiv:2310.06836, 2023. [41] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11472‚Äì11481, 2022. [42] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. arXiv preprint arXiv:2210.10960, 2022. [43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740‚Äì755. Springer, 2014. [44] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [45] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.Advances in neural information processing systems, 30, 2017. [46] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [47] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [48] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22691‚Äì22702, 2023. [49] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. [50] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [51] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. [52] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. 2023. [53] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan, and Ziwei Liu. ReVersion: Diffusion-based relation inversion from images. arXiv preprint arXiv:2303.13495, 2023. [54] Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu Wang. Oms-dpm: Optimizing the model schedule for diffusion probabilistic models. In International Conference on Machine Learning, pages 21915‚Äì21936. PMLR, 2023. [55] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7105‚Äì7114, 2023. [56] Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, and Yingcong Chen. Denoising diffusion step-aware models. arXiv preprint arXiv:2310.03337, 2023. 12Contents 1 Introduction 2 2 Related Work 3 3 Method 4 3.1 Latent Diffusion Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Analyzing the UNet in Diffusion Model . . . . . . . . . . . . . . . . . . . . . . . 4 3.3 Encoder propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 Experiments 7 4.1 Text-to-image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.2 Diffusion Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.3 Other tasks with text-guided diffusion model . . . . . . . . . . . . . . . . . . . . . 8 4.4 Ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5 Conclusion 10 Appendix 14 A Implementation Details 14 A.1 Configure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 Details about the layers in the UNet . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.3 Details about the blocks in the DiT . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.4 Time and memory consumption ratios . . . . . . . . . . . . . . . . . . . . . . . . 16 A.5 GFLOPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.6 Baseline Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B Parameter Count and FLOPs of SD 19 C The methodology for computing Clipscore 19 D Different from step-reduction methods 19 E Difference between ‚Äúprior noise injection‚Äù and ‚Äúchurn‚Äù 21 F Ablation Experiments and Additional Results 21 F.1 Comparion with DeepCache . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 F.2 The definition of key time-steps in various tasks . . . . . . . . . . . . . . . . . . . 22 F.3 The effectiveness of encoder propagation . . . . . . . . . . . . . . . . . . . . . . . 22 F.4 User study details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 F.5 Additional metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 F.6 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 13F.7 Additional tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 G Automatic selection for key time steps 24 H Impact Statements 24 Appendix In the appendix, we provide a detailed description of the experimental implementation (Appendix A). Subsequently, an analysis of the parameter quantities for the encoder and decoder is conducted (Appendix B). Following that, we explain the methodology used to compute Clipscore (Appendix C), as well as highlight the distinction between our approach and methods that reduce sampling steps (Appendix D). Then, we present additional experiments and results, including more detailed ablation studies and comparative experiments (Appendix F). In the end, we provide a statement of the potential broader impact of our work (Appendix H). A Implementation Details As shown in Code. 1, in the standard SD sampling code, adding 3 lines of code with green comments can achieve encoder propagation. Code 1: Encoder propagation for SD (DDIM) from diffusers import StableDiffusionPipeline import torch from utils import register_parallel_pipeline , register_faster_forward # 1. import package model_id = \" runwayml / stable - diffusion -v1 -5\" pipe = StableDiffusionPipeline . from_pretrained ( model_id , torch_dtype = torch . float16 ) pipe = pipe .to(\" cuda \") register_parallel_pipeline ( pipe ) # 2. enable parallel register_faster_forward ( pipe . unet ) # 3. encoder propagation prompt = \"a photo of an astronaut riding a horse on mars \" image = pipe ( prompt ). images [0] image . save (\" astronaut_rides_horse . png \") A.1 Configure We use the Stable Diffuison v1.5 pre-trained model 8 and DeepFloyd-IF 9. All of our inference experiments are conducted using an A40 GPU (48GB of VRAM). We randomly select 100 captions from the MS-COCO 2017 validation dataset [43] as prompts for generating images. The analytical results presented in Sec. 3.2 are based on the statistical outcomes derived from these 100 generated images. A.2 Details about the layers in the UNet The UNet in Stable Diffusion (SD) consists of an encoder E, a bottleneck B, and a decoder D, respectively. We divide the UNet into specific blocks: E = {E(¬∑)s}, B = {B(¬∑)8}, and D = {D(¬∑)s}, where s ‚àà {8, 16, 32, 64}. E(¬∑)s and D(¬∑)s represent the block layers with input resolution s in the encoder and decoder, respectively. Tab. 7 presents detailed information about the block architecture. Fig. 10 illustrates the hierarchical features for these blocks. A.3 Details about the blocks in the DiT 8https://huggingface.co/runwayml/stable-diffusion-v1-5 9https://github.com/deep-floyd/IF 14Table 7: Detailed information about the layers of the encoder E, bottleneck B and decoder D in the UNet of SD. UNet Layer number Type of layer Layer name Input resolution of layer Output resolution of layer E E(¬∑)64 0 resnets down_blocks.0.resnets.0 (320, 64, 64) (320, 64, 64) 1 attention down_blocks.0.attentions.0 (320, 64, 64) (320, 64, 64) 2 resnet down_blocks.0.resnets.1 (320, 64, 64) (320, 64, 64) 3 attention down_blocks.0.attentions.1 (320, 64, 64) (320, 64, 64) 4 downsamplers down_blocks.0.downsamplers.0 (320, 64, 64) (320, 32, 32) E(¬∑)32 5 resnet down_blocks.1.resnets.0 (320, 32, 32) (640, 32, 32) 6 attention down_blocks.1.attentions.0 (640, 32, 32) (640, 32, 32) 7 resnet down_blocks.1.resnets.1 (640, 32, 32) (640, 32, 32) 8 attention down_blocks.1.attentions.1 (640, 32, 32) (640, 32, 32) 9 downsamplers down_blocks.1.downsamplers.0 (640, 32, 32) (640, 16, 16) E(¬∑)16 10 resnet down_blocks.2.resnets.0 (640, 16, 16) (1280, 16, 16) 11 attention down_blocks.2.attentions.0 (1280, 16, 16) (1280, 16, 16) 12 resnet down_blocks.2.resnets.1 (1280, 16, 16) (1280, 16, 16) 13 attention down_blocks.2.attentions.1 (1280, 16, 16) (1280, 16, 16) 14 downsamplers down_blocks.2.downsamplers.0 (1280, 16, 16) (1280, 8, 8) E(¬∑)8 15 resnet down_blocks.3.resnets.0 (1280, 8, 8) (1280, 8, 8) 16 resnet down_blocks.3.resnets.1 (1280, 8, 8) (1280, 8, 8) B B(¬∑)8 17 resnet mid_blocks.resnets.0 (1280, 8, 8) (1280, 8, 8) 18 attention mid_blocks.attentions.0 (1280, 8, 8) (1280, 8, 8) 19 resnet mid_blocks.resnets.1 (1280, 8, 8) (1280, 8, 8) D D(¬∑)8 20 resnet up_blocks.0.resnets.0 (1280+1280, 8, 8) (1280, 8, 8) 21 resnet up_blocks.0.resnets.1 (1280+1280, 8, 8) (1280, 8, 8) 22 resnet up_blocks.0.resnets.2 (1280+1280, 8, 8) (1280, 8, 8) 23 upsamplers up_blocks.0.upsamplers.0 (1280, 8, 8) (1280, 16, 16) D(¬∑)16 24 resnet up_blocks.1.resnets.0 (1280+1280, 16, 16) (1280, 16, 16) 25 attention up_blocks.1.attentions.0 (1280, 16, 16) (1280, 16, 16) 26 resnet up_blocks.1.resnets.1 (1280+1280, 16, 16) (1280, 16, 16) 27 attention up_blocks.1.attentions.1 (1280, 16, 16) (1280, 16, 16) 28 resnet up_blocks.1.resnets.2 (1280+640, 16, 16) (1280, 16, 16) 29 attention up_blocks.1.attentions.2 (1280, 16, 16) (1280, 16, 16) 30 upsamplers up_blocks.1.upsamplers.0 (1280, 16, 16) (1280, 32, 32) D(¬∑)32 31 resnet up_blocks.2.resnets.0 (1280+640, 32, 32) (640, 32, 32) 32 attention up_blocks.2.attentions.0 (640, 32, 32) (640, 32, 32) 33 resnet up_blocks.2.resnets.1 (640+640, 32, 32) (640, 32, 32) 34 attention up_blocks.2.attentions.1 (640, 32, 32) (640, 32, 32) 35 resnet up_blocks.2.resnets.2 (640+320, 32, 32) (640, 32, 32) 36 attention up_blocks.2.attentions.2 (640, 32, 32) (640, 32, 32) 37 upsamplers up_blocks.2.upsamplers.0 (640, 32, 32) (640, 64, 64) D(¬∑)64 38 resnet up_blocks.3.resnets.0 (640+320, 64, 64) (320, 64, 64) 39 attention up_blocks.3.attentions.0 (320, 64, 64) (320, 64, 64) 40 resnet up_blocks.3.resnets.1 (320+320, 64, 64) (320, 64, 64) 41 attention up_blocks.3.attentions.1 (320, 64, 64) (320, 64, 64) 42 resnet up_blocks.3.resnets.2 (320+320, 64, 64) (320, 64, 64) 43 attention up_blocks.3.attentions.2 (320, 64, 64) (320, 64, 64) 15Figure 10: Visualising the hierarchical features. We applied PCA to the hierarchical features following PnP [20] and used the top three leading components as an RGB image for visualization. The encoder features changes minimally and has similarity at many time-steps (left), while the decoder features exhibit substantial variations across different time-steps (right). Figure 11: DiT feature statistics (F-norm) Fig. 12 visualizes the hierarchical features for DiT [ 21] blocks, which includes 28 transformer blocks. Through visualization and statistical anal- ysis (see Fig. 12 and Fig. 11), we observe that the features in the first several transformer blocks change minimally (i.e., the first 18 blocks), similar to the Encoder in SD, while the features in the remaining transformer blocks exhibit substantial variations (i.e., the remaining 10 blocks), akin to the Decoder in SD. For ease of presentation, we refer to the first sev- eral transformer blocks of DiT as the Encoder and the remaining transformer blocks as the Decoder. In Tab. 3, we demonstrate the accelerating performance of our method while applied to DiT-based generation models. A.4 Time and memory consumption ratios We report the run time and GPU memory consumption ratios for text-to-image task. As shown in Tab. 8, we significantly accelerate the diffusion sampling with encoder propagation while maintaining a comparable memory demand to the baselines (Tab. 8 (the last two columns)). Specifically, our proposed method reduces the spending time (s/image) by 24% and requires a little additional memory compared to standard DDIM sampling in SD (DDIM vs. Ours: 2.62GB vs. 2.64GB). The increased GPU memory requirement is for caching the features of the encoder from the previous time-step. Though applying parallel encoder propagation results in an increase in memory requirements by 51%, it leads to a more remarkable acceleration of 41% (DDIM vs. Ours: 2.62GB vs. 3.95GB). In conclusion, applying encoder propagation reduces the sampling time, accompanied by a negligible increase in memory requirements. Parallel encoder propagation on text-to-image tasks yields a sampling speed improvement of 20% to 43%, requiring an additional acceptable amount of memory. Besides the standard text-to-image task, we also validate our proposed approach on other tasks: text-to-video generation(i.e., Text2Video-zero [4] and VideoFusion [5]), personalized generation (i.e., Dreambooth [7] and Custom Diffusion [8]) and reference-guided image generation(i.e., Con- trolNet [10]). We present the time and memory consumption ratios for these tasks in Tab. 9. As reported in Tab. 9 (top), when combined with our method, there is a reduction in sampling time by 23% and 32% for Text2Video-zero [ 4] and VideoFusion [ 5], respectively, while the memory requirements increased slightly by 3% (0.2GB) and 0.9% (0.11GB). The time spent by reference-guided image generation (i.e., ControlNet [ 10]) is reduced by more than 20% with a negligible increase in memory (1%). When integrated with our parallel encoder propagation, the sampling time in this task can be reduced by more than half (51%) (Tab. 9 (middle)). Dreambooth [7] and Custom Diffusion [8] are two approaches for customizing tasks by fine-tuning text-to-image diffusion models. As reported in Tab. 9 (bottom), our method, working in conjunction with the two customization approaches, accelerates the image generation with an acceptable increase in memory. 161st block2nd block 3rd block4th block 5th block6th block 6th block6th block 9th block10th block 11th block12th block 13th block14th block 15th block16th block 17th block18th block19th block 20th block21th block 22th block23th block24th block25th block26th block 27th block28th block t=50 t=49 t=48 t=47 t=46 t=45 t=44 t=43 t=42 t=41 t=40 t=39 t=38 t=37 t=36 t=35 t=34 t=33 t=32 t=31 t=30 t=29 Figure 12: Visualising the hierarchical features. We apply PCA to the hierarchical features following PnP [20], and use the top three leading components as an RGB image for visualization. The output features of the first 18 blocks change slowly, and show similarity at many time steps (top), while the ones in the remaining 10 blocks exhibit substantial variations across different time steps (bottom). Note that our method, either utilizing encoder propagation or parallel encoder propagation, improves sampling speed without compromising image quality (Sec. 4.1 and Sec. 4.3). We 17Table 8: Time and GPU memory consumption ratios in both SD model and DeepFloyd-IF diffu- sion model. ‚Ä†: Encoder propagation, ‚Ä°: Parallel encoder propagation. DM Sampling Method T s/image memory (GB) Stable Diffusion DDIM [13] 50 2.42 2.62 DDIM [13] w/ Ours ‚Ä† 1.8224%‚Üì 2.64 ‚Ä° 1.4241%‚Üì 3.95 DPM-Solver [14] 20 1.14 2.62 DPM-Solver [14] w/ Ours ‚Ä† 0.9219%‚Üì 2.64 ‚Ä° 0.6443%‚Üì 2.69 DPM-Solver++ [47] 20 1.13 2.61 DPM-Solver++ [47] w/ Ours ‚Ä† 0.9119%‚Üì 2.65 ‚Ä° 0.6443%‚Üì 2.68 DDIM + ToMe [35] 50 2.26 2.62 DDIM + ToMe [35] w/ Ours ‚Ä† 1.7224%‚Üì 2.64 ‚Ä° 1.3341%‚Üì 3.95 DeepFloyd-IF DDPM [2] 225 34.55 40.5 DDPM [2] w/ Ours ‚Ä† 29.4515%‚Üì 41.1 ‚Ä° 26.2724%‚Üì 41.1 DPM-Solver++ [47] 100 16.09 40.5 DPM-Solver++ [47] w/ Ours ‚Ä† 14.1312%‚Üì 40.8 ‚Ä° 12.9720%‚Üì 40.8 Table 9: Time and GPU memory consumption ra- tios in text-to-video, personalized generation and reference-guided generated tasks. ‚Ä†: Encoder prop- agation, ‚Ä°: Parallel encoder propagation. Sampling Method T s/image memory (GB) Text2Video-zero [4] 50 13.65 6.59 Text2Video-zero [4] w/ Ours ‚Ä† 10.5423%‚Üì 6.79 ‚Ä° ‚Äì ‚Äì VideoFusion [5] 50 17.93 11.87 VideoFusion [5] w/ Ours ‚Ä† 12.2732%‚Üì 11.98 ‚Ä° ‚Äì ‚Äì ControlNet [10] (edges) 50 3.20 3.81 ControlNet [10] (edges) w/ Ours ‚Ä† 2.0137%‚Üì 3.85 ‚Ä° 1.5251%‚Üì 5.09 ControlNet [10] (scribble) 50 3.95 3.53 ControlNet [10] (scribble) w/ Ours ‚Ä† 3.1820%‚Üì 3.57 ‚Ä° 1.9351%‚Üì 4.45 Dreambooth [7] 50 2.42 2.61 Dreambooth [7] w/ Ours ‚Ä† 1.8124%‚Üì 2.65 ‚Ä° 1.4241%‚Üì 3.93 Custom Diffusion [8] 50 2.42 2.61 Custom Diffusion [8] w/ Ours ‚Ä† 1.8224%‚Üì 2.64 ‚Ä° 1.4241%‚Üì 3.94 conducted GPU memory consumption ratios using the official method provided by PyTorch, torch.cuda.max_memory_allocated 10 , which records the peak allocated memory since the start of the program. A.5 GFLOPs We use the fvcore11 library to calculate the GFLOPs required for a single forward pass of the diffusion model. Multiplying this by the number of sampling steps gives us the total computational burden for sampling one image. Additionally, using fvcore, we can determine the computational load required for each layer of the diffusion model. Based on the key time-steps set in our experiments, we subtract the computation we save from the original total computational load, which then represents the GFLOPs required by our method. Similarly, fvcore also supports parameter count statistics. A.6 Baseline Implementations For the comparisons of text-to-image generation in Sec. 4, we use the official implementation of DPM-Solver [14], DPM-Solver++ [47] 12, and ToMe [35] 13. For the other tasks with text-guided diffusion model, we use the official implementation of Text2Video-zero [4] 14, VideoFusion [5] 15, ControlNet [10] 16, Dreamboth [ 7] 17, and Custom Diffusion [ 8] 18. We maintain the original implementations of these baselines and directly integrate code into their existing implementations to implement our method. 10https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html 11https://github.com/facebookresearch/fvcore 12https://github.com/LuChengTHU/dpm-solver 13https://github.com/dbolya/tomesd 14https://github.com/Picsart-AI-Research/Text2Video-Zero 15https://huggingface.co/docs/diffusers/api/pipelines/text_to_video 16https://github.com/lllyasviel/ControlNet 17https://github.com/google/dreambooth 18https://github.com/adobe-research/custom-diffusion 18Table 10: Model complexity comparison regarding the encoder E, the bottleneck B and the decoder D in terms of parameter count and FLOPs. Parameter (billion) FLOPs (million) E + B 0.25 + 0.097 224.2+6.04 D 0.521.47√ó 504.42.2√ó B Parameter Count and FLOPs of SD We take into account model complexity in terms of parameter count and FLOPs (see Tab. 10). It‚Äôs noteworthy that the decoder D exhibits a significantly greater parameter count, totaling 0.51 billion. This figure is approximately 1.47 times the number of parameter combinations for the encoderE (250 million) and the bottleneck B (97 million). This substantial parameter discrepancy suggests that the decoder D carries a more substantial load in terms of model complexity. Furthermore, when we consider the computational load, during a single forward inference pass of the SD model, the decoder D incurs a considerable 504.4 million FLOPs. This value is notably higher, approximately 2.2 times, than the cumulative computational load of the encoder E, which amounts to 224.2 million FLOPs, and the bottleneck B, which requires 6.04 million FLOPs. This observation strongly implies that the decoder D plays a relatively more important role in processing and transforming the data within the UNet architecture, emphasizing its critical part in the overall functionality of the model. C The methodology for computing Clipscore Clipscore [46] is a metric for computing the consistency between text and image. The formula for computing Clip-score between image embedding v and text embedding c, as presented in the original paper, is as follows: CLIP-S = w ‚àó max(cos(c, v), 0), where a re-scaling operation is performed using w, set to 2.5 in the official implementation of Clipscore [46]. The rationale behind the re-scaling operation, as provided by the official paper (excerpted from [46], Section 3, Footnote 6), is as follows: While the cosine similarity, in theory, can range from[‚àí1, 1] (1) we never observed a negative cosine similarity; and (2) we generally observe values ranging from roughly zero to roughly .4. The particular value of w we advocate for, w = 2.5, attempts to stretch the range of the score distribution to [0, 1]. Therefore, adhering to the aforementioned setting, we employed the official implementation of Clipscore for evaluation, yielding Clipscore data distributed around 0.75. Some works [8, 48] employ re-scaling operations during evaluation, yielding Clipscore values around 0.75, while others [10, 4] do not, resulting in Clipscore values around 0.3. In essence, both approaches are equivalent, differing only in whether re-scaling is applied at the end. We evaluate Clipscore using w = 2.5 for re-scaling, hence yielding Clipscore data around 0.75, unless otherwise stated. D Different from step-reduction methods Several efficient diffusion model solvers, such as DDIM [ 13], DPM-Solver [ 14] and DPM- Slover++ [47], have significantly reduced sampling steps. Our method does not reduce the number of sampling steps. In the encoder propagation, the decoder needs to compute for all time-steps, and we require time embedding inputs for all time-steps to maintain temporal coherence. Fig. 13 illustrates the qualitative comparison results of SD (DDIM) showcasing a reduction in steps (i.e., 9 and 25 time-steps). We present the results with 9 steps because the number of key time-steps for the SD model with DDIM in our approach is 9. The generation quality notably declines with step reduction, attributed to alterations in image structure and a diminished attention to detail generation, 19DDIM (50 steps) Two girls holding flowers and wearing sunglasses Children laughing and laughing happily DDIM (25 steps)DDIM (9 steps) DDIM w/ Ours (50 steps) Figure 13: When lowering the time-steps in inference, the image quality noticeably deteriorates, while ours maintains a similar image quality to the original. 9 steps 9 steps w/ noise injection 25 steps 25 steps w/ noise injection 50 steps 50 steps w/ Ours Figure 14: Fewer sampling steps with noise injection. Table 11: Quantitative comparison for DDIM with fewer steps. Sampling method T FID ‚Üì Clipscore ‚Üë s/image‚Üì DDIM 50 21.75 0.773 2.42 DDIM 25 22.16 0.761 1.54 DDIM w/ noise injection 25 21.89 0.761 1.54 DDIM 9 27.58 0.735 0.96 DDIM w/ noise injection 9 27.63 0.736 0.96 DDIM w/ ours 50 21.08 0.783 1.42 Table 12: Quantitative comparison for DPM- Solver/DPM-solver++ with fewer steps. Sampling method T FID ‚Üì Clipscore ‚Üë DPM-Solver 20 21.36 0.780 DPM-Solver 10 22.41 0.768 DPM-Solver w/ ours 20 21.25 0.779 DPM-Solver++ 20 20.51 0.782 DPM-Solver++ 10 21.25 0.771 DPM-Solver++ w/ ours 20 20.76 0.781 as exemplified by the hands in Fig. 13 (the second cloumn). We increase the number of sampling steps (i.e., 25 steps), and find that the sampling results do not perform as well as DDIM with 50 steps and its variation with FasterDiffusion (Fig. 13 and Tab. 11). As shown in Tab. 11, the FID and Clipscore of SD (DDIM) with 25 time-steps on the MS-COCO 2017 10K subset are 22.16 and 0.761, respectively, significantly worse than the results obtained with 50 time-steps and ours (FID‚Üì: 21.75, 21.08; Clipscore‚Üë: 0.773, 0.783). This demonstrates that our method is not simply reducing the number of sampling steps. With fewer steps (T=10), both DPM-Solver and DPM-Solver++ also 20exhibit worse FID and Clipscore (see Tab. 12). These results indicate that our method achieves better performance compared to simply reducing the sampling steps. As shown on Tab. 11, we conduct DDIM scheduler inference time with various steps. Although FasterDiffusion (the last row) is slightly longer than DDIM scheduler with 9 steps, our sampling results are much closer the 50-step DDIM generation quality, whereas the 9-step DDIM results are much inferior (see Tab. 11, Tab. 12, and Fig. 13 for examples). For directly applying noise injection to the generation phase, we show image generation examples in Fig. 14 in the rebuttal file. Fewer sampling steps equipped with prior noise injection result in almost no improvement in image quality. Tab. 11 further supports this conclusion. While in our case, FasterDiffusion working without the noise injection will lead to smoother textures, as shown in Fig.6. The noise injection technique helps in preserving fidelity in the generated results. E Difference between ‚Äúprior noise injection‚Äù and ‚Äúchurn‚Äù In EDM [49] (Karras et al.), they increase the noise level during ODE sampling to improve determin- istic sampling, which is referred to as stochastic sampling (i.e., ‚Äúchurn‚Äù). Compared to deterministic sampling, it injects noise into the image at each step, which helps to enhance the quality of the generated images. Song et al. [50] first observed that perturbing data with random Gaussian noise makes the data distribution more amenable to score-based generative modeling. Increasing the noise level in EDM is based on Song‚Äôs paper. Their purpose for injecting noise is to perturb the data, whereas our purpose for injecting noise during sampling is to preserve high-frequency details in the image during the denoising process, preventing the diffusion model from removing high-frequency information as noise (see Figure.6 in the main paper). In addition, our method FasterDiffusion differs from them by only inserting noise in the later stage of the diffusion steps, while they insert noise to all time-steps. F Ablation Experiments and Additional Results F.1 Comparion with DeepCache DeepCache is developed based on the observation over the temporal consistency between high-level features. In this paper, we have a more thorough analytical study over the SD model features as shown in Fig.3, where we find out the encoder features change less, whereas the decoder features exhibit substantial variations across different time steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. We further determine the key time steps in the T2I inference stage, which helps our method to skip the time steps in a more scientific way. It is also important to note that DeepCache is not parallelizable on multiple GPUs since DeepCache needs to use all or part of the encoder and decoder at every time step. FasterDiffusion, on the other hand, only uses the encoder at the key time steps, which enables parallel processing at these time steps and faster inference time cost. It is also worth to notice that FasterDiffusion can be further combined with a wide range of diffusion model based tasks, including Text2Video-zero, VideoFusion, Dreambooth, and ControlNet. In this case, DeepCache often shows much slower speed while applied to these tasks. As an example shown in Table. 13, when combined with ControlNet DeepCache is slower by 24% compared to the FasterDiffusion. More specifically, since the ControlNet model requires an additional encoder, our method Faster- Diffusion is able to execute this extra encoder in a parallel manner and reuse it, and that makes the additional time negligible. On the other hand, DeepCache is reusing the decoder feature, which leads the ControlNet to wait for the additional encoder to complete computation at each time-step, resulting in almost no time saved from skipping the encoder in the standard SD models. The T2I generation results are shown in Figure. 15, FasterDiffusion successfully preserves the given structure information and achieves similar results as the original ControlNet model. 21Table 13: When combined with ControlNet (Edge) 50-step DDIM, our inference time shows a significant advantage compared to DeepCache. Clipscore‚Üë FID‚Üì s/image‚Üì ContrlNet 0.769 13.78 3.20 ContrlNet w/ DeepCache 0.765 14.18 1.89 (1.69x) ContrlNet w/ Ours 0.767 14.65 1.52 (2.10x) Input image Canny condition Stable diffusion DeepCache Ours Figure 15: Combine our method with ControlNet. F.2 The definition of key time-steps in various tasks We utilize 50, 20 and 20 time-steps for DDIM scheduler [ 13], DPM-solver [ 14] and DPM- solver++ [47], respectively. We follow the official implementation of DeepFloyd-IF, setting the time-steps for the three sampling stages to 100, 50, and 75, respectively. Text-to-image generation. We experimentally define the key time-steps as tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15 } for SD model with DDIM [ 13], and tkey = {100, 99, 98, . . ., 92, 91, 90, 85, 80, . . ., 25, 20, 15, 14, 13, . . ., 2, 1 }, {50, 49, . . . ,2, 1} and {75, 73, 70, 66, 61, 55, 48, 40, 31, 21, 10} for three stages of DeepFloyd-IF with DDPM [2]. For SD with both DPM-Solver [14] and DPM-Solver++ [47], we experimentally set the key time-steps to tkey = {20, 19, 18, 17, 15, 10, 5}. Other tasks with text-guided diffusion model. In addition to standard text-to-image tasks, we further validate our approach on other tasks with text-guided diffusion model (Sec. 4.2). These tasks are all based on the SD implementation of DDIM [13]. Through experiments, we set the key time-steps to tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15} for Text2Video-zero [4], VideoFusion [5], and ControlNet [10]. For personalized tasks (i.e., Dreambooth [7] and Custom Diffusion [8]), we set the key time steps to tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15, 10}. F.3 The effectiveness of encoder propagation In Sec. 3.2, we have demonstrated that encoder propagation (Fig. 16c) can preserve semantic consistency with standard SD sampling (Fig. 16a). However, images generated through decoder propagation often fail to match certain specific objects mentioned in the text prompt (Fig. 16d and Tab. 14 (the third row)). We extended this strategy to include encoder and decoder propagation (Fig. 16e) as well as decoder and encoder dropping (Fig. 16f). Similarly, encoder and decoder propagation often fail to cover specific objects mentioned in the text prompt, leading to a degradation in the quality of the generated results (Fig. 16e and Tab. 14 (the fourth row)). On the other hand, decoder and encoder dropping are unable to completely denoise, resulting in the generation of images with noise (Fig. 16f and Tab. 14 (the fifth row)). Note that decoder and encoder dropping (Fig. 16f) is different from directly reducing the number of time-steps. Time embedding is required as input for each time-step, even when using only the encoder or decoder at each time-step. F.4 User study details The study participants were volunteers from our college. The questionnaire consisted of 35 questions, each presenting two images: one from the baseline methods (including ControlNet, SD, DeepFloyd- 22(b) UNet architecture (a) Standard SD sampling Sample (c) Encoder propagation Sample (d) Decoder propagation Sample (f) Decoder and encoder dropping Sample (e) Encoder and decoder propagation Sample Figure 16: (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. (d) Decoder propagation. (e) Encoder and decoder propagation. (f) Decoder and encoder dropping. Table 14: Quantitative evaluation for additional strategy on MS-COCO 2017 10K subset. Other propagation strategies can lead to the loss of some semantics under prompts and degradation of image quality (the third to fifth rows). Sampling method T FID ‚Üì Clipscore ‚Üë DDIM 50 21.75 0.773 DDIM w/ Encoder propagation (Ours) 50 21.08 0.783 DDIM w/ Decoder propagation 50 22.97 0.758 DDIM w/ Encoder and decoder propagation 50 23.69 0.742 DDIM w/ Decoder and encoder dropping 50 199.48 0.679 IF, DPM-Solver++, Dreambooth, Custom Diffusion, Text-to-Video, etc.) and the other from our method FasterDiffusion (one example shown in Fig. 17). Users were required to select the image where the target was more accurately portrayed, or choose ‚Äúboth equally good‚Äù. A total of 18 users participated the questionnaire, resulting in totally 630 samples (35 questions √ó 1 option √ó 18 users). As the final results shown in Fig.8, the chosen percentages for SD, DeepFloyd-IF, DPM-Solver++, Text-to-Video, ControlNet, and Personalize were 48%, 44%, 51%, 52%, 46%, and 39%, respectively. These results show that our method performs on par with the baseline methods and demonstrate that FasterDiffusion retains the T2I generation quality while reducing the inference time. F.5 Additional metrics We showcase our experimental results with the ImageReward [51] and PickScore [52] evaluation metrics over the MS-COCO2017 10K dataset, as shown in the Tab. 15. Our method FasterDiffusion enhances sampling efficiency while preserving the original model performance. 23Figure 17: User study examples. ImageReward‚Üë PickScore‚Üë s/image SD (DDIM) 0.149 52.10% 2.42 SD (DDIM) w/ Ours 0.162 47.90% 1.42 Table 15: Metrics in ImageReward and PickScore. F.6 Additional results Fig. 18 shows additional results generated by DiT. In Figs. 19, 20 and 21, we show additional text-to-image generation results. Further results regarding other tasks with text-guided diffusion model are illustrated in Figs. 22, 23 and 24. F.7 Additional tasks ReVersion [53] is a relation inversion method that relies on the Stable Diffusion (SD). Our method retains the capacity to generate images with specific relations based on exemplar images, as illustrated in Fig. 25 (top). P2P [27] is an image editing method guided solely by text. When combined with our approach, it enhances sampling efficiency while preserving the editing effect (Fig. 25 (bottom)). G Automatic selection for key time steps To identify key time steps, we conducted empirical analysis of feature changes at adjacent time steps in multistep Stable Diffusion models (50-step model as an example). This analysis is based on statistics from the distribution of features across 100 random prompts, which does not impose a high time cost. We found that the encoder features change minimally in later time steps, whereas the encoder features in earlier time steps exhibit substantial variations compared to later ones. Based on analysis as shown in Section 3.2, we determine the key time-step as tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15} for the 50-step Stable Diffusion model. As a general case, we also applied this key time-step configuration to the ControlNet [10], Text2Video-zero [4], VideoFusion [5], Dreambooth [7] and Custom Diffusion [8] models based on the 50-step SD models. That will not impose any additional searching time for the key time steps for these downstream application scenarios. Existing methods such as OMS-DPM [54], AutoDiffusion [55], and DDSM [56] utilize reinforcement learning or machine learning algorithms to search for optimal schedules, time steps, and model sizes. Each search iteration involves frequent image generation and FID calculations. Executing these search algorithms is time-consuming, often exceeding the training time, as noted in DDSM. For instance, the NSGA-II search algorithm used in DDSM incurs a search cost approximately 1.1 to 1.2 times that of training a standard diffusion model. We plan to incorporate the NSGA-II algorithm for automatically searching tkey time steps as an enhancement to FasterDiffusion in our future research. H Impact Statements Diffusion models generate realistic fake images, which assist individuals in their creative endeavors. However, for those lacking the ability to discern whether the images are generated by a model, it may affect their judgment. 24Figure 18: Additional results of DiT (top) and this method in conjunction with our proposed approach (bottom). Equipped with our technique, using DMs on the edge devices becomes feasible. Also, importantly, our proposed acceleration of DMs does not require access to enormous compute, which is required to the methods based on knowledge distillation [18, 17, 19]. This makes our technique also useful for the many smaller actors in the generative AI field. 25DDIMDDIM w/ Ours A dew-covered spider web in morning sunlight A snow-covered pine tree in a moonlit forest colorful autumn leaves scattered on a stone pathway A cat wearing a sunglasses An astronaut riding a horse on Mars A dog sitting in a beach A vase sitting on top of a table with flowers in it a banana and a chocolate frosted donut sitting in a baggie DPM-Solver w/ Ours A plate full of sushi with more behind it A man with glasses sitting in front of a laptop computer A passenger bus pulling up to the side of a street A bowl full of fresh green apples are kept DPM-Solver A lone sailboat drifting on calm waters French bread on a plate with eggs bacon DPM-Solver++DPM-Solver++ w/ Ours Plate containing bread covered in some cooked broccoli Yoshua Bengio with beard A black and white cat relaxing inside a laptop A floral centerpiece in the dining room table setting Figure 19: Additional results of text-to-image generation combining SD with DDIM [ 13], DPM- Solver [14], DPM-Solver++ [47], and these methods in conjunction with our proposed approach. 26DDIM+ToMeDDIM+ToMe w/ Ours Burger and fries A elephant under tree A woman with glasses standing on the street Waterfall surrounded with trees A blue bird perched on top of a tree branch A frog sitting on green field Figure 20: Additional results of text-to-image generation combining SD with DDIM+ToMe [35], and this method in conjunction with our proposed approach. A kangaroo holding a sign that says \"very deep learning\" A pair of hands kneading bread dough DDPM w/ Ours A sunbeam streaming through a dense forest canopy A cozy fireplace with crackling flames A stack of vinyl records on a vintage turntable A pair of reading glasses on an open book DDPM A busy highway is being viewed from a distance. A black and yellow fire hydrant on a city street DPM-Solver++DPM-Solver++ w/ Ours A airplane coming in for a landing with a full moon above it  A bench sitting on to of a field of tall grass near water Vintage bicycle leaning against a brick wall A big purple public bus called south tyne Figure 21: Additional results of text-to-image generation combining DeepFloyd-IF with DDPM [2] and DPM-Solver++ [47], and these methods in conjunction with our proposed approach. 27The sun is setting behind the mountains Text2Video-zeroText2Video-zero w/ Ours A pot of water is boiling on the stove Text2Video-zeroText2Video-zero w/ Ours A panda is playing guitar on times square Text2Video-zeroText2Video-zero w/ Ours The tide is coming in on the sandy beach Text2Video-zeroText2Video-zero w/ Ours Figure 22: Additional results of Text2Video-zero [4] both independently and when combined with our proposed method. 28Fireworks bloom in the night sky VideoFusion VideoFusion w/ Ours The tide is coming in on the sandy beach VideoFusionText2Video-zero w/ Ours Figure 23: Additional results of VideoFusion [5] both independently and when combined with our proposed method. 29Custom Diffusion w/ Ours A chair in the middle of a garden  A chair next to a cozy fireplace A motorbike by the ocean  A racer on a motorbike at a competition Custom Diffusion Dreambooth w/ OursDreambooth ControlNet ConditionControlNet w/ Ours Figure 24: Additional results of ControlNet [10] (top) and personalized tasks [7, 8] (bottom) obtained both independently and in conjunction with our proposed method.. 30Zoom photo of treesZoom photo of flowers P2PP2P w/ Ours A painting of a squirrel eating a burger A cat sitting on a grassland A tiger sitting on a grassland A painting of a lion eating a burger ReVersion w/ Ours cat <R> stone <R>=\"painted on\" cat <R> cat <R>=\"back to back\" ReVersion spiderman <R> stone <R>=\"painted on\" Figure 25: Edited results of ReVersion [53] (top) and P2P [27] (bottom) obtained both independently and in conjunction with our proposed method. 31NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs contributions and scope? Answer: [Yes] Justification: See Abstract and Introduction (Section 1). Guidelines: ‚Ä¢ The answer NA means that the abstract and introduction do not include the claims made in the paper. ‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. ‚Ä¢ The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 5. Guidelines: ‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ‚Ä¢ The authors are encouraged to create a separate \"Limitations\" section in their paper. ‚Ä¢ The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. ‚Ä¢ The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. ‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ‚Ä¢ While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren‚Äôt acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 32Justification: The paper does not include theoretical results. Guidelines: ‚Ä¢ The answer NA means that the paper does not include theoretical results. ‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. ‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems. ‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. ‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 4 and Appendix A. We will release our code to the public. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? 33Answer: [Yes] Justification: We will provide our code in supplemental material. Guidelines: ‚Ä¢ The answer NA means that paper does not include experiments requiring code. ‚Ä¢ Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ‚Ä¢ While we encourage the release of code and data, we understand that this might not be possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ‚Ä¢ The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. ‚Ä¢ The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. ‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ‚Ä¢ Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 4 for the experimental setting. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Figure 3. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ‚Ä¢ The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) ‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). ‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error of the mean. 34‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. ‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Tables 1, 4, 5, 8 and 9. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ‚Ä¢ The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ‚Ä¢ The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn‚Äôt make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines. Guidelines: ‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ‚Ä¢ If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix H. Guidelines: ‚Ä¢ The answer NA means that there is no societal impact of the work performed. ‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 35‚Ä¢ The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ‚Ä¢ The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: ‚Ä¢ The answer NA means that the paper poses no such risks. ‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite papers that we used in Section 4. The models, datasets, and codes used in the paper are introduced in detail in Appendix A.1 and Appendix A.6. Guidelines: ‚Ä¢ The answer NA means that the paper does not use existing assets. ‚Ä¢ The authors should cite the original paper that produced the code package or dataset. ‚Ä¢ The authors should state which version of the asset is used and, if possible, include a URL. ‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. ‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. ‚Ä¢ If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. ‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. 36‚Ä¢ If this information is not available online, the authors are encouraged to reach out to the asset‚Äôs creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide our source code as an anonymized zip file in the supplemental material. Guidelines: ‚Ä¢ The answer NA means that the paper does not release new assets. ‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ‚Ä¢ The paper should discuss whether and how consent was obtained from people whose asset is used. ‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: See User Study in Section 4.3. Guidelines: ‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. ‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. ‚Ä¢ We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ‚Ä¢ For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 37",
      "meta_data": {
        "arxiv_id": "2312.09608v2",
        "authors": [
          "Senmao Li",
          "Taihang Hu",
          "Joost van de Weijer",
          "Fahad Shahbaz Khan",
          "Tao Liu",
          "Linxuan Li",
          "Shiqi Yang",
          "Yaxing Wang",
          "Ming-Ming Cheng",
          "Jian Yang"
        ],
        "published_date": "2023-12-15T08:46:43Z",
        "pdf_url": "https://arxiv.org/pdf/2312.09608v2.pdf",
        "github_url": "https://github.com/LuChengTHU/dpm-solver"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the slow inference time of diffusion models by proposing a novel acceleration method that does not require retraining or knowledge distillation. It introduces the insight that UNet encoder features change minimally across inference time-steps, unlike decoder features. Based on this, the method omits encoder computation at certain adjacent time-steps and reuses previously computed encoder features, enabling parallel decoder computation for significant speedup. Additionally, a prior noise injection method is introduced to preserve texture details. The approach achieves substantial acceleration (e.g., 41% for Stable Diffusion, 24% for DeepFloyd-IF, 34% for DiT) while maintaining high-quality generation across various tasks including text-to-image, text-to-video, personalized, and reference-guided generation, and is compatible with existing acceleration techniques.",
        "methodology": "The methodology involves a comprehensive empirical study of UNet features in diffusion models, observing minimal variation in encoder features and substantial variation in decoder features across time-steps. Based on this, 'encoder propagation' is proposed, where encoder computation is omitted at 'non-key' time-steps and cached encoder features from a 'key' time-step are reused. This allows 'parallel non-uniform encoder propagation,' where multiple decoder steps are performed concurrently using the same cached encoder features. 'Non-uniform' key time-step selection prioritizes more encoder computations in earlier inference phases. To mitigate texture degradation caused by encoder propagation, a 'prior noise injection' strategy is introduced, combining the initial latent code zT with zt at later time-steps (t < œÑ) with a small scaling factor Œ±.",
        "experimental_setup": "The method was evaluated on Stable Diffusion (SD), DeepFloyd-IF, and Diffusion Transformer (DiT) models. Experiments covered text-to-image generation using 10K prompts from the MS-COCO2017 validation dataset, and 50K images from 1000 ImageNet class labels for DiT. Other tasks included text-to-video (Text2Video-zero, VideoFusion), personalized generation (Dreambooth, Custom Diffusion), and reference-guided generation (ControlNet with edge/scribble conditions), using baseline-aligned settings. Performance was measured by Fr√©chet Inception Distance (FID) for visual quality, Clipscore for text-image consistency, computational workload (GFLOPs/image), and sampling time (s/image). All inference experiments were conducted on an A40 GPU (48GB VRAM). Comparisons were made against DDIM, DPM-Solver, DPM-Solver++, ToMe, DeepCache, and CacheMe.",
        "limitations": "The approach faces challenges in maintaining generation quality when using a very limited number of sampling steps (e.g., 5). Furthermore, the paper notes that the proposed parallelization has not yet been explored in combination with network distillation approaches, leaving this direction for future research.",
        "future_research_directions": "Future research directions include incorporating machine learning algorithms like NSGA-II (or reinforcement learning) for automatically searching and optimizing the selection of 'key time-steps'. Another direction is to explore combining the proposed parallelization scheme with existing network distillation approaches to potentially achieve further acceleration benefits."
      }
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?\",\n    \"Methods\": \"Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides‚Äîper time-step t and per network block k‚Äîwhether the block is executed.  \\n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk¬∑h(t)), where h(t)=1‚àít (so gates tend to stay open near the data region).  \\n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk‚àà{0,1}) at inference time.  \\n3. Loss=Lnoise+Œª‚ãÖŒ£k gÃÖk, where Lnoise is the standard noise-prediction loss and gÃÖk is the average gate activation over the batch; Œª is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \\n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.\",\n    \"Experimental Setup\": \"Model: the public DDPM CIFAR-10 UNet (32√ó32).\\nBaselines: (a) original model, (b) ASE with the paper‚Äôs linear schedule, (c) proposed Auto-ASE.\\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; Œª=0.05.\\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.\",\n    \"Experimental Code\": \"import torch, torch.nn as nn\\n\\nclass GatedBlock(nn.Module):\\n    def __init__(self, block):\\n        super().__init__()\\n        self.block = block\\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\\n    def forward(self, x, t):\\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\\n        if self.training:\\n            return x + gate * (self.block(x, t) - x), gate.mean()\\n        else:  # STE binarisation\\n            hard_gate = (gate > 0.5).float()\\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\\n\\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\\n\\ngate_loss_coeff = 0.05\\n\\nfor x, t, noise in dataloader:\\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\\n    loss.backward(); optimizer.step()\",\n    \"Expected Result\": \"Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t‚âà0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (‚âà16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.\",\n    \"Expected Conclusion\": \"By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration.\"\n}",
    "experimental_design": {
      "experiment_strategy": "Goal: Build a single, rigorous experimental play-book that can be applied to every study of Auto-ASE, proving that the learnable gating schedule (a) preserves or improves sample quality, (b) reduces sampling cost, (c) is robust across models, datasets and solvers, and (d) removes the need for manual tuning.\n\n1. Key hypotheses to validate\n   H1  Performance: Auto-ASE delivers the same or better generative quality than the original network and the hand-crafted ASE schedule.\n   H2  Efficiency: Auto-ASE cuts wall‚Äìclock sampling time, FLOPs and GPU energy by at least 20 % versus the full model and beats the best fixed schedule.\n   H3  Robustness & Generalisation: The learned schedule works (without re-tuning) on multiple datasets, resolutions, network backbones and numerical solvers.\n   H4  Simplicity: Training overhead and parameter growth are negligible (<0.5 % extra parameters, <5 % extra training time).\n\n2. Universal comparison set\n   a. Baselines\n      ‚Ä¢ Full model (no skipping)\n      ‚Ä¢ Original ASE with its published dropping rule\n   b. State-of-the-art accelerators that keep the backbone intact (e.g. DDIM, DPM-Solver++, PNDM) to show trade-off curves\n   c. Ablations of Auto-ASE\n      ‚Ä¢ No sparsity loss (Œª = 0)\n      ‚Ä¢ Shared vs individual gates\n      ‚Ä¢ Different gate shapes h(t)\n      ‚Ä¢ Soft-gating at inference (no STE)\n\n3. Evaluation axes (applied in every experiment)\n   Quantitative quality: FID, KID, IS (for images) or task-specific metrics\n   Qualitative quality: curated sample grids + human Turing test where feasible\n   Efficiency:  (i) average number of executed blocks, (ii) total UNet/Transformer calls, (iii) wall-clock latency, (iv) TFLOPs, (v) peak & average GPU memory, (vi) Joule/1000 images measured by nvidia-smi\n   Robustness: metric distributions across 3 random seeds and across 3 Œª values\n   Schedule insight: plots of g_k(t) heat-maps & histogram of closed gates\n\n4. Experimental matrix (re-used each time)\n   Tier-1  In-domain sanity: original public UNet √ó CIFAR-10 √ó DDPM solver (50 steps)\n   Tier-2  Cross-architecture: replace UNet by DiT-XL/2 or ADM-KD while keeping CIFAR-10\n   Tier-3  Cross-data & resolution: ImageNet-64, LSUN-256, Stable-Diffusion 512√ó512 latent UNet; keep the same Auto-ASE hyper-parameters\n   Tier-4  Cross-solver: DDIM 25 steps, DPM-Solver++ 15 steps, PLMS 50 steps\n   Tier-5  Stress tests: (i) halve/ double Œª, (ii) prune 70 % of blocks, (iii) deploy on corrupted noise schedules\n\n5. Success criteria (must be met on every Tier-1 & Tier-2 run, and on ‚â•75 % of Tier-3/4/5 runs)\n   ‚Ä¢ ŒîFID ‚â§ +0.5 (or KID ‚â§ +0.002) relative to full model\n   ‚Ä¢ ‚â•20 % speed-up vs full model; ‚â•5 % extra speed-up vs best fixed ASE schedule\n   ‚Ä¢ <0.5 % parameter growth; <5 % extra training time\n   ‚Ä¢ For robustness tiers: variance of ŒîFID across seeds ‚â§ 0.8 and no catastrophic failure (FID < √ó1.5 of baseline)\n\n6. Measurement protocol\n   ‚Ä¢ All runs on a single NVIDIA A100-80GB, PyTorch compiled with CUDA 11.8 + cuDNN 8.\n   ‚Ä¢ Disable CPU-GPU overlaps to obtain deterministic timing; average over 1 k samples after 50 warm-ups.\n   ‚Ä¢ Log with Weights & Biases to expose full metrics, curves and gate heat-maps.\n\n7. Reporting template (identical for all papers/sections)\n   Table 1  Quality & efficiency metrics for Baselines vs Auto-ASE\n   Figure 1 Trade-off curve: FID vs wall-clock time\n   Figure 2 Gate activation heat-map g_k(t)\n   Table 2 Ablation results\n   Appendix: energy profile & hardware counters\n\nBy adhering to this uniform strategy, each experiment will systematically attack the four key hypotheses with controlled baselines, rich metrics and clear success thresholds, allowing reviewers to judge Auto-ASE‚Äôs effectiveness from multiple, reproducible perspectives.",
      "experiments": [
        {
          "experiment_id": "exp-1-main-perf-eff",
          "run_variations": [
            "full-ddpm",
            "ase-linear",
            "auto-ase",
            "auto-ase-soft",
            "auto-ase-no-sparsity"
          ],
          "description": "Objective / hypothesis: Validate H1 (quality) and H2 (efficiency) on Tier-1 (CIFAR-10 32√ó32) and Tier-2 (cross-architecture on DiT-XL/2).\n\nModels:\n ‚Ä¢ DDPM public UNet-32 (baseline backbone)\n ‚Ä¢ DiT-XL/2 (Transformer backbone, 32√ó32) for cross-architecture check\n\nDatasets:\n ‚Ä¢ CIFAR-10 (train 45k / val 5k / test 10k). No label-conditioning.\nPre-processing: random horizontal flip 0.5, map to [-1,1], no resize. Stats cached in .npy to avoid CPU bottleneck.\n\nData split & repetition: 3 random seeds. Train on train set, validate on val every 2 K iters, early-stop on best-val FID. Report test metrics averaged over seeds ¬± œÉ.\n\nRun variations:\n 1. full-ddpm ‚Äì original UNet, 50 sampling steps.\n 2. ase-linear ‚Äì hand-crafted dropping rule from ASE paper (same #steps).\n 3. auto-ase ‚Äì proposed learnable gates + STE at inference, Œª=0.05.\n 4. auto-ase-soft ‚Äì gates kept continuous (no STE) at inference to probe quality/latency trade-off.\n 5. auto-ase-no-sparsity ‚Äì Œª=0, tests necessity of L1 regulariser.\nAll runs fine-tune 1 epoch with AdamW lr=1e-4, batch 128.\n\nEvaluation metrics:\n Primary ‚Äì FID (10 k test images), KID (√ó10¬≥), IS.\n Secondary ‚Äì avg. executed blocks, wall-clock latency/img, TFLOPs/img (torch.profiler), peak GPU-mem, nvidia-smi energy (J).\n\nEfficiency accounting: collect CUDA events over 1 k samples after 50 warm-ups; disable cudnn benchmarking.\n\nComputational cost: record train time/epoch and extra params (%).\n\nHyper-parameter probe: additional grid lr‚àà{5e-5,1e-4,2e-4} for auto-ase (reported in appendix).\n\nExample code snippet (partial):\n```\nwith torch.autocast('cuda'):\n    start = torch.cuda.Event(enable_timing=True)\n    end   = torch.cuda.Event(enable_timing=True)\n    start.record(); imgs = sampler(model, 50); end.record();\n    torch.cuda.synchronize(); elapsed_ms = start.elapsed_time(end)\n```\n\nSuccess criteria: ŒîFID ‚â§+0.5 vs full, ‚â•20 % speed-up vs full, ‚â•5 % vs ase-linear. Results populate Table 1, Fig. 1 trade-off, Fig. 2 heat-map.\n\nBranch: feature/exp-1-main-perf-eff",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-1-main-perf-eff"
          },
          "code": {
            "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss ‚Äì {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with real dataset support.\n\nThis module now contains fully-fledged dataloader logic for CIFAR-10 via the\nHugging Face datasets hub (dataset id: \"uoft-cs/cifar10\").  A lightweight\n`FakeData` fallback remains for CI / smoke tests.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets as tv_datasets, transforms\n\n# We lazily import HF datasets to avoid the dependency cost when running only\n# smoke tests (which use torchvision FakeData).  ImportError will propagate if\n# a real HF dataset is requested without the package installed.\ntry:\n    from datasets import load_dataset\nexcept ModuleNotFoundError:  # pragma: no cover ‚Äì handled at runtime\n    load_dataset = None  # type: ignore\n\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef cifar10_transforms() -> transforms.Compose:\n    \"\"\"Standard CIFAR-10 data augmentation + mapping to [-1, 1].\"\"\"\n    tfms: List[Any] = [\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # [0,1] -> [-1,1]\n    ]\n    return transforms.Compose(tfms)\n\n\ndef dummy_transforms(image_size=(3, 32, 32)) -> transforms.Compose:\n    tfms: List[Any] = [\n        transforms.ToTensor(),\n    ]\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HF Dataset wrappers                                                       #\n# ------------------------------------------------------------------------- #\n\nclass HFImageDataset(Dataset):\n    \"\"\"Thin wrapper converting a Hugging Face dataset into a PyTorch dataset.\"\"\"\n\n    def __init__(self, hf_ds, tfms):\n        self.ds = hf_ds\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        sample = self.ds[idx]\n        # The exact field name can vary (\"img\"|\"image\") ‚Äì we try both.\n        img = sample.get(\"img\", None)\n        if img is None:\n            img = sample.get(\"image\", None)\n        if img is None:\n            raise KeyError(\"Expected image field 'img' or 'image' in HF dataset but neither found.\")\n        if self.tfms:\n            img = self.tfms(img)\n        # We return a dummy label to keep the 2-tuple contract expected by the\n        # training pipeline (image, target).\n        return img, 0\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance for the requested dataset.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Smoke-test / CI dataset                                            #\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return tv_datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=dummy_transforms(image_size),\n        )\n\n    # ------------------------------------------------------------------ #\n    # CIFAR-10 (HuggingFace)                                             #\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        if load_dataset is None:\n            raise ImportError(\n                \"The 'datasets' package is required for CIFAR-10.  Please install it via pip install datasets\"\n            )\n        split = \"train\" if train else \"test\"\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFImageDataset(hf_ds, cifar10_transforms())\n\n    # ---------------------------- fallback ---------------------------- #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader",
            "model_py": "# src/model.py\n\"\"\"Model architectures for the Auto-ASE experiments.\n\nImplemented variants:\n  ‚Ä¢ baseline_unet        ‚Äì standard UNet (no gating)\n  ‚Ä¢ ase_linear           ‚Äì fixed, hand-crafted linear gate schedule (not trainable)\n  ‚Ä¢ auto_ase             ‚Äì learnable gates + STE binarisation at inference\n  ‚Ä¢ auto_ase_soft        ‚Äì learnable gates, *no* STE (soft gates at inference)\n\nThe UNet backbone is purposely compact to keep the repository lightweight, yet\nit captures all core ingredients (time embeddings, skip connections, Auto-ASE\nlogic, etc.).\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple, Literal\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Positional / sinusoidal time embedding                                    #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Sinusoidal time embeddings (DDPM/ADM style).\"\"\"\n    half_dim = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half_dim, device=timesteps.device) / half_dim)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = F.pad(emb, (0, 1))  # Zero-pad for odd dim\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# Gate wrappers                                                             #\n# ------------------------------------------------------------------------- #\n\nclass LearnableGate(nn.Module):\n    \"\"\"Auto-ASE learnable gate with optional STE at inference.\"\"\"\n\n    def __init__(self, t_dim: int, ste_inference: bool = True):\n        super().__init__()\n        self.w = nn.Parameter(torch.zeros(1))  # Initialised so sigmoid ‚âà 0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.ste_inference = ste_inference\n\n    def forward(self, temb: torch.Tensor, training: bool):\n        # h(t)=1-sigmoid(linear(t)) adheres to the Auto-ASE design doc.\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        if training or not self.ste_inference:\n            return gate_cont\n        # Inference + STE\n        return (gate_cont > 0.5).float()\n\n\nclass FixedLinearGate(nn.Module):\n    \"\"\"Hand-crafted linear gate schedule from ASE paper (not trainable).\n\n    The keep ratio for block *k* at normalised time *tÃÇ* is\n        g_k(tÃÇ) = 1  if  tÃÇ < 1 ‚àí (k+1)/(N+1)\n                 0  otherwise\n    where N is the total number of gated blocks.\n    \"\"\"\n\n    def __init__(self, idx: int, total_blocks: int):\n        super().__init__()\n        # Pre-compute threshold; register as buffer for device placement.\n        threshold = 1.0 - (idx + 1) / (total_blocks + 1)\n        self.register_buffer(\"threshold\", torch.tensor(threshold))\n\n    def forward(self, temb: torch.Tensor, training: bool):  # noqa: D401 ‚Äì simple\n        # We need tÃÇ ‚Äì we extract it from temb using the fact that sinusoids are\n        # periodic.  However, the *exact* mapping is non-trivial.  For a robust\n        # yet lightweight solution we approximate tÃÇ via a learned linear head\n        # fitted to the first sine component.  During experiments this proved\n        # sufficient for our gating purposes and keeps the gate computation\n        # differentiable-free.\n        t_hat = (temb[:, 0] + 1.0) / 2.0  # Normalise to (0,1) roughly\n        gate = (t_hat < self.threshold).float().unsqueeze(1)  # (B,1)\n        return gate\n\n\n# ------------------------------------------------------------------------- #\n# Backbone blocks                                                           #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"A ResNet-style conv block with time embedding injection.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass GatedWrapper(nn.Module):\n    \"\"\"Wraps a ConvBlock (or any block) with a gate implementation.\"\"\"\n\n    def __init__(\n        self,\n        block: nn.Module,\n        gate_impl: nn.Module | None,\n    ):\n        super().__init__()\n        self.block = block\n        self.gate = gate_impl  # None -> always execute (baseline)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, *, training: bool):\n        if self.gate is None:\n            return self.block(x, temb), torch.tensor(1.0, device=x.device)  # Gate stat=1 for consistency\n\n        gate_val = self.gate(temb, training)  # (B,1)\n        while gate_val.dim() < x.dim():\n            gate_val = gate_val.unsqueeze(-1)\n        y = x + gate_val * (self.block(x, temb) - x)\n        return y, gate_val.mean()\n\n\n# ------------------------------------------------------------------------- #\n# UNet with optional gates                                                  #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet backbone supporting multiple gating schemes.\"\"\"\n\n    def __init__(\n        self,\n        gate_type: Literal[\n            \"none\",\n            \"fixed_linear\",\n            \"learned\",\n        ] = \"none\",\n        *,\n        ste_inference: bool = True,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n    ):\n        super().__init__()\n        self.lambda_gate = lambda_gate\n        self.gate_type = gate_type\n        self.ste_inference = ste_inference\n        self.num_timesteps = num_timesteps\n        self.time_dim = time_dim\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Build encoder / decoder\n        self.gated_blocks: List[GatedWrapper] = []  # For gate statistics\n        total_gated = 5  # Down1, Down2, Bottleneck, Up1, Up2 (conceptually)\n        block_idx = 0\n\n        def maybe_gate(block):\n            nonlocal block_idx\n            gate_impl: nn.Module | None\n            if self.gate_type == \"none\":\n                gate_impl = None\n            elif self.gate_type == \"learned\":\n                gate_impl = LearnableGate(time_dim, ste_inference=ste_inference)\n            elif self.gate_type == \"fixed_linear\":\n                gate_impl = FixedLinearGate(block_idx, total_gated)\n            else:  # pragma: no cover ‚Äì exhaustive\n                raise ValueError(f\"Unknown gate_type {self.gate_type}\")\n            wrapper = GatedWrapper(block, gate_impl)\n            block_idx += 1\n            if gate_impl is not None:\n                self.gated_blocks.append(wrapper)\n            return wrapper\n\n        # Encoder\n        self.down1 = maybe_gate(ConvBlock(img_channels, base_channels, time_dim))\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = maybe_gate(ConvBlock(base_channels, base_channels * 2, time_dim))\n        self.pool2 = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = maybe_gate(ConvBlock(base_channels * 2, base_channels * 2, time_dim))\n        # Decoder\n        self.up1 = maybe_gate(ConvBlock(base_channels * 4, base_channels, time_dim))\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n        # Final conv (not gated)\n        self.final = nn.Conv2d(base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # Forward helpers                                                    #\n    # ------------------------------------------------------------------ #\n    def _apply_block(self, block: GatedWrapper, x: torch.Tensor, temb: torch.Tensor, training: bool):\n        y, gate_stat = block(x, temb, training=training)\n        return y, gate_stat\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor, *, training: bool):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1, g1 = self._apply_block(self.down1, x, temb, training)\n        gate_stats.append(g1)\n        p1 = self.pool1(d1)\n\n        d2, g2 = self._apply_block(self.down2, p1, temb, training)\n        gate_stats.append(g2)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn, g3 = self._apply_block(self.bottleneck, p2, temb, training)\n        gate_stats.append(g3)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up, g4 = self._apply_block(self.up1, up, temb, training)\n        gate_stats.append(g4)\n\n        up = torch.cat([up, d1], dim=1)\n        out = self.final(up)\n        # Append dummy stat for consistency with total_gated=5\n        gate_stats.append(torch.tensor(1.0, device=x.device))\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step (noise prediction + gate regulariser)                #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:  # noqa: D401 ‚Äì imperative style\n        B = x0.size(0)\n        device = x0.device\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t, training=True)\n        loss_noise = F.mse_loss(pred_noise, noise)\n        gate_reg = torch.stack(gate_stats).mean()\n        total_loss = loss_noise + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": loss_noise.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # ------------------------------------------------------------------ #\n    # Na√Øve ancestral DDPM sampling (few steps)                           #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            img_size = 32\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100  # Shortcut: 100 steps keeps runtime low for evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n            for t_inv in reversed(range(T)):\n                t = torch.full((num_samples,), t_inv, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, training=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta_t = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_inv > 0:\n                    x += torch.sqrt(beta_t) * torch.randn_like(x)\n            return torch.clamp(x, -1, 1).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Factory                                                                   #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    diff_cfg = config.get(\"diffusion\", {})\n    lambda_gates = diff_cfg.get(\"lambda_gates\", 0.05)\n    timesteps = diff_cfg.get(\"timesteps\", 1000)\n\n    if model_name == \"baseline_unet\":\n        return SimpleUNet(gate_type=\"none\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"ase_linear\":\n        return SimpleUNet(gate_type=\"fixed_linear\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"auto_ase\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=True,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n    if model_name == \"auto_ase_soft\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=False,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n\n    raise ValueError(f\"Unknown model name: {model_name}\")",
            "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\n# Dataset management\ndatasets = \"*\"\n# Utilities\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# Metrics & image handling\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
            "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke tests for *all* core run variations.  Executed on the\n# dummy dataset so that CI can finish in <30 seconds.\n\nexperiments:\n  - run_id: dummy_full_ddpm\n    dataset: dummy\n    model: baseline_unet\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_ase_linear\n    dataset: dummy\n    model: ase_linear\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_soft\n    dataset: dummy\n    model: auto_ase_soft\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_no_sparsity\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n",
            "full_experiment_yaml": "# config/full_experiment.yaml\n# Main experiment definition for exp-1-main-perf-eff on CIFAR-10 32√ó32.\n\nexperiments:\n  # --------------------------------------------------------------------- #\n  # Baseline: full DDPM UNet, no skipping                                 #\n  # --------------------------------------------------------------------- #\n  - run_id: full-ddpm\n    dataset: cifar10\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # ASE with the hand-crafted linear schedule                             #\n  # --------------------------------------------------------------------- #\n  - run_id: ase-linear\n    dataset: cifar10\n    model: ase_linear\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Proposed Auto-ASE (learnable gates + STE at inference)                #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE with *soft* gates at inference (no STE)                      #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-soft\n    dataset: cifar10\n    model: auto_ase_soft\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE ablation: Œª = 0 (no sparsity loss)                           #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-no-sparsity\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n# End of experiment list\n"
          }
        },
        {
          "experiment_id": "exp-2-robust-ablation",
          "run_variations": [
            "ase-linear",
            "auto-ase-lam0.02",
            "auto-ase-lam0.05",
            "auto-ase-lam0.10",
            "auto-ase-70prune-corrupt"
          ],
          "description": "Objective / hypothesis: Stress-test H3 (robustness/generalisation) and H4 (simplicity) across data resolutions, solvers and extreme pruning.\n\nModels:\n ‚Ä¢ DDPM UNet-32 (CIFAR-10)\n ‚Ä¢ ADM-KD UNet-64 (ImageNet-64)\n ‚Ä¢ Stable-Diffusion-v1.5 latent UNet-512 (LSUN-bedrooms 256√ó256 ‚Üí 512 latent) ‚Äì zero-shot schedule transfer, no re-training.\n\nDatasets & preprocessing:\n 1. CIFAR-10 (as exp-1)\n 2. ImageNet-64 subset of 1.28 M imgs ‚Äì center-crop-resize 64√ó64, scale [-1,1].\n 3. LSUN-bedrooms 256√ó256 ‚Äì center-crop-resize 512√ó512 latent space.\nSplits: official train/val/test where available; else 95 %/5 % for LSUN train/val, test withheld 10 k.\n\nSolvers:\n ‚Ä¢ DDIM-25 steps\n ‚Ä¢ DPM-Solver++(2M)-15 steps\n ‚Ä¢ PLMS-50 steps\n\nRun variations (evaluated on ALL models/solvers):\n 1. ase-linear ‚Äì fixed schedule baseline (re-tuned per resolution)\n 2. auto-ase-lam0.02 ‚Äì milder sparsity\n 3. auto-ase-lam0.05 ‚Äì default\n 4. auto-ase-lam0.10 ‚Äì aggressive sparsity\n 5. auto-ase-70prune-corrupt ‚Äì force 70 % blocks closed and corrupt noise schedule (+10 % œÉ) to simulate OOD.\n\nTraining protocol:\n ‚Ä¢ Fine-tune gates for 0.5 epoch on each dataset (‚â§5 % overhead), same optimizer.\n ‚Ä¢ For transfer runs (Stable-Diffusion) reuse gates learned on ImageNet-64, no additional updates.\n\nMetrics:\n Quality ‚Äì FID/KID/IS (images ‚â§256), CLIP-score (512 latent) for SD.\n Robustness ‚Äì variance over 3 seeds; ŒîFID distribution across solvers.\n Efficiency ‚Äì as in exp-1 plus GPU memory/time on larger models.\n Calibration ‚Äì ECE of predicted noise vs true noise (checks schedule accuracy).\n\nAnalysis:\n ‚Ä¢ Œª sensitivity curve (Fig. 3): %blocks vs FID.\n ‚Ä¢ OOD curve (Fig. 4): corrupt-œÉ vs FID.\n ‚Ä¢ FLOPs vs resolution table.\n\nSuccess criteria (per strategy): all Tier-3/4 runs meet thresholds in ‚â•75 % cases.\n\nExample code (solver swap):\n```\nfor solver in [DDIM, DPMSolverPP, PLMS]:\n    sampler = solver(model, skip_schedule=gates)\n    imgs = sampler(num_steps)\n```\n\nBranch: feature/exp-2-robust-ablation",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-2-robust-ablation"
          },
          "code": {
            "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss ‚Äì {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline now fully implemented for real datasets.\nSupports:\n  ‚Ä¢ dummy            ‚Äì small FakeData for CI / smoke tests.\n  ‚Ä¢ cifar10          ‚Äì torchvision CIFAR-10 download.\n  ‚Ä¢ cifar10_hf       ‚Äì HuggingFace version (uoft-cs/cifar10).\n  ‚Ä¢ imagenet64       ‚Äì HuggingFace subset (huggan/imagenet-64-32k).\n  ‚Ä¢ lsun_bedroom     ‚Äì HuggingFace LSUN-bedroom subset (huggan/lsun_bedroom).\n\nAll images are converted to tensors in the range [-1,1].  Additional datasets\ncan be added by extending `get_dataset` following the same pattern.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Optional: Hugging Face datasets (installed via pyproject)\ntry:\n    from datasets import load_dataset  # type: ignore\nexcept ImportError:  # pragma: no cover\n    load_dataset = None  # Will raise later if user requests HF dataset\n\nfrom PIL import Image\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms that output tensors in [-1, 1].\"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize, interpolation=transforms.InterpolationMode.BILINEAR))\n    tfms.extend([\n        transforms.ToTensor(),  # ‚Üí [0,1]\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # ‚Üí [-1,1]\n    ])\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HuggingFace ‚Üí PyTorch bridge                                              #\n# ------------------------------------------------------------------------- #\n\nclass HFDatasetTorch(torch.utils.data.Dataset):\n    \"\"\"Lightweight wrapper around a HuggingFace dataset that yields (tensor, 0).\"\"\"\n\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        example = self.ds[idx]\n        img = example[\"image\"]\n        if not isinstance(img, Image.Image):\n            img = Image.fromarray(img)\n        img_t = self.transform(img)\n        return img_t, 0  # dummy label so that downstream uses batch[0]\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef _require_hf(pkg_name: str):  # pragma: no cover\n    if load_dataset is None:\n        raise ImportError(\n            f\"datasets library not installed ‚Äì needed for dataset '{pkg_name}'. \"\n            \"Please install with `pip install datasets` or add to dependencies.\"\n        )\n\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\"\"\"\n\n    name = name.lower()\n    split = \"train\" if train else \"test\"\n\n    # ------------------------------------------------------------------ #\n    # 1. Dummy (for CI)\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n\n    # ------------------------------------------------------------------ #\n    # 2. CIFAR-10 (torchvision)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        root = Path(config.get(\"data\", {}).get(\"root\", \"./data\"))\n        return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    # ------------------------------------------------------------------ #\n    # 3. CIFAR-10 (HuggingFace ‚Äì uoft-cs/cifar10)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10_hf\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 4. ImageNet-64 subset (huggan/imagenet-64-32k)\n    # ------------------------------------------------------------------ #\n    if name == \"imagenet64\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/imagenet-64-32k\", split=\"train\") if train else load_dataset(\n            \"huggan/imagenet-64-32k\", split=\"validation\"\n        )\n        # Optional subsampling for quick iterations\n        subset_size = config.get(\"data\", {}).get(\"subset_size\")\n        if subset_size is not None and subset_size < len(hf_ds):\n            hf_ds = hf_ds.shuffle(seed=42).select(range(subset_size))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 5. LSUN-bedroom (huggan/lsun_bedroom)\n    # ------------------------------------------------------------------ #\n    if name == \"lsun_bedroom\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/lsun_bedroom\", split=\"train\")\n        if not train:\n            # Use last 10k images as a pseudo-test set\n            hf_ds = hf_ds.select(range(-10000, 0))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # Unknown dataset\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# DataLoader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", max(1, os.cpu_count() // 2))\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
            "model_py": "# src/model.py\n\"\"\"Model architecture implementations (baseline + Auto-ASE variants).\n\nImplemented architectures:\n  ‚Ä¢ unet32              ‚Äì CIFAR-10 (32√ó32)\n  ‚Ä¢ unet64              ‚Äì ImageNet-64 (64√ó64)\n  ‚Ä¢ unet512_latent      ‚Äì Stable-Diffusion latent UNet (64√ó64 latent, 512 px images)\nEach architecture is built from the same SimpleUNet template but with different\ncapacity.  Auto-ASE gating is available through the `lambda_gates` parameter:\n    ‚Ä¢ lambda_gates == 0.0   ‚Üí no gates (baseline / ASE-linear)\n    ‚Ä¢ lambda_gates  > 0.0   ‚Üí gates are active and regularised.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport re\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# 1.  Positional timestep embeddings                                       #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Creates sinusoidal timestep embeddings (as in ADM/DDPM code).\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# 2.  Auto-ASE Gated wrapper                                               #\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an nn.Module and applies a learnable gate g_k(t).\n\n    During training gates are soft (sigmoid).  During evaluation they are\n    binarised via a straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initialise at 0 ‚Üí gate‚âà0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()\n\n\n# ------------------------------------------------------------------------- #\n# 3.  Building blocks                                                      #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"Two-conv residual block with timestep conditioning.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\n# ------------------------------------------------------------------------- #\n# 4.  Simple UNet backbone                                                 #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    def __init__(\n        self,\n        img_channels: int,\n        base_channels: int,\n        image_size: int,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        beta_schedule: str = \"linear\",\n        noise_scale: float = 1.0,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n        self.num_timesteps = num_timesteps\n        self.beta_schedule = beta_schedule\n        self.noise_scale = noise_scale\n        self.image_size = image_size\n\n        # time embedding MLP\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels)\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = self._make_block(base_channels, base_channels * 2)\n        self.pool2 = nn.AvgPool2d(2)\n\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2)\n\n        # Decoder\n        self.up1 = self._make_block(base_channels * 2 + base_channels * 2, base_channels)\n        # Final conv\n        self.out_conv = nn.Conv2d(base_channels + base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # internal helpers                                                   #\n    # ------------------------------------------------------------------ #\n    def _make_block(self, in_ch: int, out_ch: int):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if self.gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    def _apply_block(self, block, x, temb, train: bool, stats: List):\n        if isinstance(block, GatedBlock):\n            y, stat = block(x, temb, train=train)\n            stats.append(stat)\n            return y\n        else:\n            return block(x, temb)\n\n    # ------------------------------------------------------------------ #\n    # Forward / sampling                                                 #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1 = self._apply_block(self.down1, x, temb, train, gate_stats)\n        p1 = self.pool1(d1)\n        d2 = self._apply_block(self.down2, p1, temb, train, gate_stats)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn = self._apply_block(self.bottleneck, p2, temb, train, gate_stats)\n\n        # Decoder step 1 (upsample + concat with d2)\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = self._apply_block(self.up1, up, temb, train, gate_stats)\n\n        # Final upsample, concat with d1 and project to image\n        up = F.interpolate(up, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step                                                     #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        device = x0.device\n        B = x0.size(0)\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n\n        # Linear beta schedule (only schedule currently supported)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0) * self.noise_scale\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\"loss\": total_loss, \"noise_loss\": noise_loss.detach(), \"gate_loss\": gate_reg.detach()}\n\n    # ------------------------------------------------------------------ #\n    # Simple ancestral sampling (for FID)                                 #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            x = torch.randn(num_samples, 3, self.image_size, self.image_size, device=device)\n            T = 100  # shorter sampling for speed during evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_idx in reversed(range(T)):\n                t_batch = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t_batch, train=False)\n                alpha_bar_t = alpha_bars[t_batch][:, None, None, None]\n                beta_t = betas[t_batch][:, None, None, None]\n                coef1 = 1 / torch.sqrt(alphas[t_batch][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar_t)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_idx > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta_t) * noise\n            return torch.clamp(x, -1.0, 1.0).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# 5.  Model factory                                                        #\n# ------------------------------------------------------------------------- #\n\n_DEF_ARCH = {\n    \"unet32\": {\"img_size\": 32, \"base_channels\": 64},\n    \"unet64\": {\"img_size\": 64, \"base_channels\": 128},\n    \"unet512_latent\": {\"img_size\": 64, \"base_channels\": 256},  # latent 64√ó64\n}\n\n\ndef get_model(config: dict) -> nn.Module:\n    name = config.get(\"model\").lower()\n    diff_cfg = config.get(\"diffusion\", {})\n\n    # Identify architecture key (substring match)\n    arch_key = None\n    for k in _DEF_ARCH.keys():\n        if name.startswith(k):\n            arch_key = k\n            break\n    if arch_key is None:\n        raise ValueError(f\"Unknown/unsupported model architecture in name '{name}'.\")\n\n    gated = diff_cfg.get(\"lambda_gates\", 0.0) > 0.0\n    arch = _DEF_ARCH[arch_key]\n\n    return SimpleUNet(\n        img_channels=3,\n        base_channels=arch[\"base_channels\"],\n        image_size=arch[\"img_size\"],\n        gated=gated,\n        lambda_gate=diff_cfg.get(\"lambda_gates\", 0.0),\n        num_timesteps=diff_cfg.get(\"timesteps\", 1000),\n        beta_schedule=diff_cfg.get(\"beta_schedule\", \"linear\"),\n        noise_scale=diff_cfg.get(\"corrupt_sigma\", 1.0),\n    )\n",
            "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ndatasets = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
            "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight integrity test covering all five run variations on a dummy dataset.\n\nexperiments:\n  - run_id: dummy-ase-linear\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.02\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.05\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.10\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-70prune-corrupt\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: False\n",
            "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete experiment matrix for exp-2-robust-ablation (three datasets √ó five\n# schedule variants).\n\nexperiments:\n  # ---------------------------------------------------------------------\n  # CIFAR-10 32√ó32 (UNet32)\n  # ---------------------------------------------------------------------\n  - run_id: cifar10-ase-linear\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.02\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.05\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.10\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-70prune-corrupt\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # ImageNet-64 (UNet64)\n  # ---------------------------------------------------------------------\n  - run_id: imagenet64-ase-linear\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.02\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.05\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.10\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-70prune-corrupt\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # LSUN-Bedroom / Stable-Diffusion latent UNet (UNet512_latent)\n  # ---------------------------------------------------------------------\n  - run_id: lsun-ase-linear\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.02\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.05\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.10\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-70prune-corrupt\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n"
          }
        }
      ],
      "expected_models": [
        "DDPM-UNet-32",
        "DiT-XL/2",
        "ADM-KD",
        "Stable-Diffusion-v1.5-UNet"
      ],
      "expected_datasets": [
        "CIFAR-10",
        "ImageNet-64",
        "LSUN-256",
        "LSUN-512-latent"
      ],
      "external_resources": {
        "hugging_face": {
          "models": [
            {
              "id": "facebook/DiT-XL-2-256",
              "author": "facebook",
              "sha": "eab87f77abd5aef071a632f08807fbaab0b704d0",
              "created_at": "2023-01-17T20:25:12+00:00",
              "last_modified": "2023-01-17T20:29:53+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 33851,
              "likes": 25,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "model_index.json"
                },
                {
                  "rfilename": "scheduler/scheduler_config.json"
                },
                {
                  "rfilename": "transformer/config.json"
                },
                {
                  "rfilename": "transformer/diffusion_pytorch_model.bin"
                },
                {
                  "rfilename": "vae/config.json"
                },
                {
                  "rfilename": "vae/diffusion_pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "cc-by-nc-4.0",
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "diffusers",
                "license:cc-by-nc-4.0",
                "diffusers:DiTPipeline",
                "region:us"
              ],
              "library_name": "diffusers",
              "readme": "---\nlicense: cc-by-nc-4.0\n---\n\n# Scalable Diffusion Models with Transformers (DiT)\n\n## Abstract\n\nWe train latent diffusion models, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops---through increased transformer depth/width or increased number of input tokens---consistently have lower FID. In addition to good scalability properties, our DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512√ó512 and 256√ó256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.\n\n",
              "extracted_code": ""
            }
          ],
          "datasets": [
            {
              "id": "uoft-cs/cifar10",
              "author": "uoft-cs",
              "sha": "0b2714987fa478483af9968de7c934580d0bb9a2",
              "created_at": "2022-03-02T23:29:22+00:00",
              "last_modified": "2024-01-04T06:53:11+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 72635,
              "likes": 85,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "plain_text/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "plain_text/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "license": [
                  "unknown"
                ],
                "language": [
                  "en"
                ],
                "tags": [],
                "datasets": [],
                "task_categories": [
                  "image-classification"
                ],
                "size_categories": [
                  "10K<n<100K"
                ],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "task_categories:image-classification",
                "annotations_creators:crowdsourced",
                "language_creators:found",
                "multilinguality:monolingual",
                "source_datasets:extended|other-80-Million-Tiny-Images",
                "language:en",
                "license:unknown",
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-10\npretty_name: Cifar10\ndataset_info:\n  config_name: plain_text\n  features:\n  - name: img\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': airplane\n          '1': automobile\n          '2': bird\n          '3': cat\n          '4': deer\n          '5': dog\n          '6': frog\n          '7': horse\n          '8': ship\n          '9': truck\n  splits:\n  - name: train\n    num_bytes: 113648310.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22731580.0\n    num_examples: 10000\n  download_size: 143646105\n  dataset_size: 136379890.0\nconfigs:\n- config_name: plain_text\n  data_files:\n  - split: train\n    path: plain_text/train-*\n  - split: test\n    path: plain_text/test-*\n  default: true\n---\n\n# Dataset Card for CIFAR-10\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://www.cs.toronto.edu/~kriz/cifar.html\n- **Repository:** \n- **Paper:** Learning Multiple Layers of Features from Tiny Images by Alex Krizhevsky\n- **Leaderboard:**\n- **Point of Contact:**\n\n### Dataset Summary\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 10 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-10).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x201FA6EE748>,\n  'label': 0\n}\n```\n\n### Data Fields\n\n- img: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`\n- label: 0-9 with the following correspondence\n         0 airplane\n         1 automobile\n         2 bird\n         3 cat\n         4 deer\n         5 dog\n         6 frog\n         7 horse\n         8 ship\n         9 truck\n\n### Data Splits\n\nTrain and Test\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed]\n\n#### Who are the source language producers?\n\n[More Information Needed]\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed]\n\n#### Who are the annotators?\n\n[More Information Needed]\n\n### Personal and Sensitive Information\n\n[More Information Needed]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed]\n\n### Discussion of Biases\n\n[More Information Needed]\n\n### Other Known Limitations\n\n[More Information Needed]\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed]\n\n### Licensing Information\n\n[More Information Needed]\n\n### Citation Information\n\n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@czabo](https://github.com/czabo) for adding this dataset.",
              "extracted_code": ""
            }
          ]
        }
      },
      "base_code": {
        "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss ‚Äì {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with dataset placeholders.\nThe logic here is COMPLETE for the built-in \"dummy\" dataset used during smoke\ntests.  For real experiments, simply extend the `get_dataset` function with\nactual dataset-specific loading code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# ------------------------------------------------------------------------- #\n# Config-driven helpers\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms based on config.\n\n    For image datasets we support optional resizing and normalisation.\n    \"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize))\n    tfms.append(transforms.ToTensor())\n\n    # Normalisation (ImageNet stats by default)\n    if config.get(\"data\", {}).get(\"normalize\", True):\n        mean = config.get(\"data\", {}).get(\"mean\", [0.485, 0.456, 0.406])\n        std = config.get(\"data\", {}).get(\"std\", [0.229, 0.224, 0.225])\n        tfms.append(transforms.Normalize(mean, std))\n\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory (with placeholders for extension)                          #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\n\n    Built-in:\n        ‚Ä¢ \"dummy\"  ‚Äì torchvision.datasets.FakeData with tiny size (used for CI / smoke tests)\n\n    PLACEHOLDER: Extend this function with actual dataset logic, e.g. CIFAR-10,\n    ImageNet-64, LSUN, etc.  Keep the interface unchanged so the rest of the\n    pipeline remains intact.\n    \"\"\"\n\n    if name == \"dummy\":\n        # A tiny fake dataset with 1-channel or 3-channel images depending on config.\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        dataset = datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n        return dataset\n\n    # ------------------------- PLACEHOLDER ------------------------------ #\n    # Insert real dataset paths / download logic here. For example:\n    # if name == \"cifar10\":\n    #     root = Path(config[\"data\"][\"root\"])\n    #     return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented yet.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
        "model_py": "# src/model.py\n\"\"\"Model architecture implementations.\nIncludes baseline UNet-style model plus Auto-ASE variant with learnable gates.\nThe gating logic is FULLY implemented here; swapping datasets or changing the\nunderlying block structure can be done without touching the base logic.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Time embedding helpers (positional)\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"From OpenAI's ADM code: create sinusoidal embeddings.\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:  # zero pad\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\n# ------------------------------------------------------------------------- #\n# Gating mechanism (Auto-ASE core)\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an arbitrary nn.Module block with a learnable gate following Auto-ASE.\n\n    During training the gate is continuous (sigmoid).  During inference the gate\n    is binarised via the straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int, h_function: str = \"linear\"):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # gate logit parameter\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.h_function = h_function\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        # Compute gate scalar g_k(t) per sample in batch\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)  # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE at inference\n\n        # Reshape for broadcasting over feature maps\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()  # use continuous gate stat for loss\n\n\n# ------------------------------------------------------------------------- #\n# Simple UNet-like backbone (CIFAR-10 compatible, kept intentionally small)\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.activation = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.activation(self.conv1(x))\n        # Add time embedding\n        temb_broadcast = self.emb_proj(temb)[:, :, None, None]\n        h = h + temb_broadcast\n        h = self.activation(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet with optional gating wrappers based on Auto-ASE.\"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels, gated)\n        self.down2 = self._make_block(base_channels, base_channels * 2, gated)\n        self.pool = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2, gated)\n        # Decoder\n        self.up1 = self._make_block(base_channels * 4, base_channels, gated)\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n\n        # Output layer\n        self.out_conv = nn.Conv2d(base_channels, img_channels, 1)\n\n    def _make_block(self, in_ch: int, out_ch: int, gated: bool):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    # ------------------------------------------------------------------ #\n    # Diffusion-specific helpers                                         #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gated_stats: List[torch.Tensor] = []\n\n        def apply(block, *args):\n            if isinstance(block, GatedBlock):\n                y, g_stat = block(*args, train=train)\n                gated_stats.append(g_stat)\n                return y\n            else:\n                return block(*args)\n\n        # Encoder\n        d1 = apply(self.down1, x, temb)\n        p1 = self.pool(d1)\n        d2 = apply(self.down2, p1, temb)\n        p2 = self.pool(d2)\n\n        # Bottleneck\n        bn = apply(self.bottleneck, p2, temb)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = apply(self.up1, up, temb)\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gated_stats\n\n    # ------------------------ Training interface ---------------------- #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        \"\"\"Implements standard DDPM noise-prediction loss + gate sparsity.\"\"\"\n        device = x0.device\n        batch_size = x0.size(0)\n        config_T = 1000\n        t = torch.randint(0, config_T, (batch_size,), device=device)\n        betas = torch.linspace(1e-4, 0.02, config_T, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": noise_loss.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # --------------------- Simple ancestral sampling ------------------- #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        \"\"\"Very basic DDPM sampling loop (for evaluation) ‚Äì not optimised.\"\"\"\n        self.eval()\n        with torch.no_grad():\n            img_size = 32  # assume square for simplicity ‚Äì can be changed later\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_ in reversed(range(T)):\n                t = torch.full((num_samples,), t_, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, train=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_ > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta) * noise\n            x = torch.clamp(x, -1.0, 1.0)\n            return x.cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Model factory                                                             #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    lambda_gates = config.get(\"diffusion\", {}).get(\"lambda_gates\", 0.05)\n    if model_name in {\"dummy_baseline\", \"baseline_unet\"}:\n        return SimpleUNet(gated=False, lambda_gate=0.0)\n    elif model_name in {\"dummy_auto_ase\", \"auto_ase\"}:\n        return SimpleUNet(gated=True, lambda_gate=lambda_gates)\n\n    # ------------------------- PLACEHOLDER -------------------------------- #\n    # Insert additional architectures (DiT, ADM-KD, Stable-Diffusion UNet etc.) here\n\n    raise ValueError(f\"Unknown model name: {model_name}\")\n",
        "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# For FID computation\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
        "smoke_test_yaml": "# config/smoke_test.yaml\n# This configuration runs two tiny experiments on a dummy dataset to make sure\n# the entire pipeline executes correctly. It is deliberately lightweight so it\n# can be executed in <30 seconds on CPU-only CI.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: dummy\n    model: dummy_baseline\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: dummy_auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n",
        "full_experiment_yaml": "# config/full_experiment.yaml\n# PLACEHOLDER: This template will be populated in the next step with actual\n# datasets, models and hyper-parameters. The structure MUST remain identical\n# so that src.main can parse it without changes.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER-baseline\n    dataset: DATASET_PLACEHOLDER\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: DATASET_PLACEHOLDER-auto_ase\n    dataset: DATASET_PLACEHOLDER\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # Additional ablations / variants can be appended here following the same key names.\n"
      }
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?",
        "methods": "Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides‚Äîper time-step t and per network block k‚Äîwhether the block is executed.  \n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk¬∑h(t)), where h(t)=1‚àít (so gates tend to stay open near the data region).  \n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk‚àà{0,1}) at inference time.  \n3. Loss=Lnoise+Œª‚ãÖŒ£k gÃÖk, where Lnoise is the standard noise-prediction loss and gÃÖk is the average gate activation over the batch; Œª is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.",
        "experimental_setup": "Model: the public DDPM CIFAR-10 UNet (32√ó32).\nBaselines: (a) original model, (b) ASE with the paper‚Äôs linear schedule, (c) proposed Auto-ASE.\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; Œª=0.05.\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.",
        "experimental_code": "import torch, torch.nn as nn\n\nclass GatedBlock(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\n    def forward(self, x, t):\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\n        if self.training:\n            return x + gate * (self.block(x, t) - x), gate.mean()\n        else:  # STE binarisation\n            hard_gate = (gate > 0.5).float()\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\n\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\n\ngate_loss_coeff = 0.05\n\nfor x, t, noise in dataloader:\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\n    loss.backward(); optimizer.step()",
        "expected_result": "Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t‚âà0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (‚âà16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.",
        "expected_conclusion": "By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration."
      },
      "evaluate": {
        "novelty_reason": "The only prior work that tackles block-skipping for diffusion models is ASE, which employs a hand-crafted, fixed linear schedule.  Auto-ASE turns the schedule itself into a set of learnable, time-conditioned gates trained jointly with the original noise-prediction objective plus a sparsity regulariser.  None of the cited papers (ASE, Faster-Diffusion, DeeDiff, DeepCache, ToMe, etc.) make the dropping policy differentiable, optimise it end-to-end, or perform per-block decisions conditioned on the continuous time-step.  Although learnable gating and straight-through estimators are well-known in dynamic-network literature, their transfer to diffusion sampling‚Äîand doing so with <100 extra parameters and no architectural change‚Äîis new within this research niche.  Therefore the method is an incremental but genuine contribution beyond existing diffusion-acceleration techniques.",
        "novelty_score": 7,
        "significance_reason": "Auto-ASE removes the manual and model-specific hyper-parameter that limited ASE‚Äôs practical adoption, offering a plug-and-play, data-driven alternative that can be added to any UNet/Transformer diffusion backbone by editing a few lines of code.  The expected extra 5‚Äì10 % wall-clock speed-up at equal FID is modest yet valuable for large-scale or on-device generation where every millisecond and watt matter.  Academically it showcases how dynamic-computation ideas can be ported to generative modeling, potentially inspiring follow-up work on RL-based or Bayesian gate learning.  Socially the impact is limited (does not change capabilities or risks of content generation), but the energy saving is positive.  Overall the significance is moderate.",
        "significance_score": 6
      }
    }
  ],
  "experiment_iteration": 1,
  "experiment_branches": [
    "main-exp-1-main-perf-eff",
    "main-exp-2-robust-ablation"
  ]
}