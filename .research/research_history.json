{
  "research_topic": "analyze movie of mouses and identify and annotate actions",
  "queries": [
    "mouse behavior annotation"
  ],
  "research_study_list": [
    {
      "title": "MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior",
      "abstract": "We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the quality of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 million frames of pose tracking data). Accompanying these data, we introduce a panel of real-life downstream analysis tasks to assess the quality of learned representations by evaluating how well they preserve information about the experimental conditions (e.g. strain, time of day, optogenetic stimulation) and animal behavior. We test multiple state-of-the-art self-supervised video and trajectory representation learning methods to demonstrate the use of our benchmark, revealing that methods developed using human action datasets do not fully translate to animal datasets. We hope that our benchmark and dataset encourage a broader exploration of behavior representation learning methods across species and settings.",
      "full_text": "MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Jennifer J. Sun * 1 Markus Marks * 1 Andrew W. Ulmer2 Dipam Chakraborty 3 Brian Geuther 4 Edward Hayes Heng Jia 5 Vivek Kumar4 Sebastian Oleszko 6 Zachary Partridge 7 Milan Peelman 8 Alice Robie 9 Catherine E. Schretter 9 Keith Sheppard 4 Chao Sun 5 Param Uttarwar 10 Julian M. Wagner1 Erik Werner6 Joseph Parker 1 Pietro Perona1 Yisong Yue1 Kristin Branson 9 Ann Kennedy 2 Website: https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset Abstract We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the qual- ity of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 mil- lion frames of pose tracking data). Accompa- nying these data, we introduce a panel of real- life downstream analysis tasks to assess the qual- ity of learned representations by evaluating how well they preserve information about the exper- imental conditions (e.g. strain, time of day, op- togenetic stimulation) and animal behavior. We test multiple state-of-the-art self-supervised video and trajectory representation learning methods to demonstrate the use of our benchmark, reveal- ing that methods developed using human action datasets do not fully translate to animal datasets. We hope that our benchmark and dataset encour- age a broader exploration of behavior representa- tion learning methods across species and settings. 1. Introduction The study of interacting agents is important for a range of scientific and engineering applications, from designing safer *Equal contribution 1Caltech 2Northwestern University 3AICrowd 4JAX Labs 5Zhejiang University 6IRLAB Thera- peutics 7University of New South Wales 8Ghent University 9Janelia 10Saarland University. Correspondence to: Ann Kennedy <ann.kennedy@northwestern.edu>. Proceedings of the40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). Animal Interaction Datasets (Ours) Mouse Triplet (Video) 8 tasks Fruit Fly Group (Trajectory) 50 tasks Chase Strain  Light Cycle Huddle … … Experimental  context tasks Frame-by-frame  Expert-annotated  behaviors Beetle Interactions (Video) 14 tasks Broad Task Types from Scientiﬁc Experiments Previous Video Datasets Human Action Recognition  Animal Action Recognition Shaking Hands (Kuehne et al. 2011)  Eating (Ng. et al., 2022) Figure 1.MABe22 consists of animal interactions in laboratory experiments. We propose a dataset to benchmark representa- tion learning methods that focus on multi-agent behavior. Our benchmark includes a large video and trajectory library depicting interactions of mice, beetles, ants, and fruit flies alongside a large suite of downstream tasks to measure representation quality. Tasks differ across model organisms and include the classification of experimental conditions (e.g. species strain, light cycle, optoge- netic activations, interaction duration) as well as expert-annotated actions (e.g. chase, huddle, and sniffs for mice). autonomous vehicles (Chang et al., 2019), to understand- ing player behavior in virtual worlds (Hofmann, 2019), to uncovering the biological underpinnings of neurological dis- orders (Segalin et al., 2020; Wiltschko et al., 2020). Across disciplines, there is a need for new techniques to character- ize the structure of multi-agent behavior with greater preci- sion, sensitivity, and detail. Traditionally, behavior analysis models are trained with full supervision (Burgos-Artizzu 1 arXiv:2207.10553v2  [cs.LG]  30 Jun 2023MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior et al., 2012; Hong et al., 2015; Bohnslav et al., 2021), which subjects users to a heavy burden of video annotation. Efforts to learn behavioral representations without manual anno- tation (Berman et al., 2014; Wiltschko et al., 2015; Hsu & Yttri, 2020; Sun et al., 2021b) promise to bypass this labor bottleneck, but are difficult to evaluate systematically. To support the development of learned behavioral representa- tions, and to better evaluate their performance, we need benchmark datasets for behavior. These benchmarks should cover a broad range of experimental conditions, to avoid overfitting on the statistics of a particular dataset. Further- more, when representations are learned without supervision, there is no obvious metric to evaluate the quality of the representation. Yet, a metric is needed for quantitative com- parisons. These two challenges inspired our work. We have collected and curated a large dataset and benchmark from biology experiments for evaluating learned represen- tations of social behavior (Figure 1). We chose to focus on videos of laboratory animals for several reasons: • Animal behavioral experiments are collected against a uni- form uninformative background, such as (Segalin et al., 2020; Eyjolfsdottir et al., 2014; Pereira et al., 2020), and thus behavior classifiers are forced to focus on the dy- namic and pictorial cues of the action. In contrast, video of human behavior, e.g., actions in different sports, are usually pictorially informative, meaning that the action itself can be classified from the appearance of a single or a few frames rather than considering motion over long periods of time. • Animal behavior is often recorded under various experi- mental manipulations that impact the behavior (Figure 1). Identifying those experimental manipulations provides an objective task that may be used to evaluate the quality of a representation. This complements evaluation based on reproducing human annotations of behavior, which have shorter temporal structure but can be subjective (Anderson & Perona, 2014). • The biologists who provided us with videos of their ex- periments are engaged in analyzing specific aspects of the animals’ behavior. Using a given representation to automate their analysis provides us with an objective performance criterion that is defined outside the field of Computer Vision. Evaluation methods based on down- stream tasks, i.e. tasks where the representation is used to analyze specific aspects of the signal, have been used in other domains, e.g. for evaluating visual representa- tions (Van Horn et al., 2021) or neural mechanistic mod- els (Schrimpf et al., 2020). • Our dataset is from real-world neuroscience and evolu- tionary biology experiments, and progress on this dataset will enable biologists to use the representations gener- ated to study how behavior changes as a function of other experimental variables. We make three contributions: 1. A large and richly anno- tated video and trajectory dataset, Multi-Agent Behavior 2022 (MABe22), of social behavior in three species: labora- tory mice (Mus musculus) triplets, rove beetles (Sceptobius lativentris) paired with their symbiotic host species or with other beetles, and vinegar flies (Drosophila melanogaster). 2. A large and diverse set of downstream evaluation tasks based on the classification of experimental conditions (op- togenetic activation, animal strain, time-of-day) and expert- annotated behavior labels. 3. A baseline benchmark of state- of-the-art self-supervised video and trajectory representa- tion learning, as well as community-contributed methods solicited from an open challenge. To the best of our knowl- edge, our dataset is the first to provide non-annotation-based downstream tasks from scientific experiments for represen- tation evaluation (Table 1). Our dataset and related code is available at: https://sites.google.com/view/computational-behavior/our- datasets/mabe2022-dataset. 2. Related Work Related Animal Datasets. The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments. There are several existing datasets for studying animal social behavior, including CRIM13 (Burgos-Artizzu et al., 2012), Fly vs. Fly (Eyjolfsdottir et al., 2014), and CalMS21 (Sun et al., 2021a). These datasets contain video or pose data from interacting animals, as well as human-annotated behavior labels (Table 1); they all focus on a single species and setting. AnimalKingdom (Ng et al., 2022) is another recent animal behavior dataset that includes social and nonsocial behavior from multiple species, but is focused on human annotation- based action recognition only. Our dataset is unique in that it defines a range of downstream tasks for each organism; these tasks are motivated by scientific experiments, with the goal of to driving scientific discovery in biology. Related Human Datasets. While animal video datasets remain comparatively rate, there are many video datasets designed for work in human action recognition. Human datasets typically have very different visual characteristics from animal datasets. Most notably, many human datasets that are used to benchmark self-supervised video represen- tation learning, such as Kinetics (Kay et al., 2017), UCF101 (Soomro et al., 2012) and HMDB51 (Kuehne et al., 2011), contain ’spatially heavy’ visual information that informs downstream action classification– that is, different actions have different backgrounds. Because of these differences in the visual appearance, agents’ actions can be partly dis- tinguished by these visual features alone, without models having to learn any temporal features of the agents’ behav- 2MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Dataset Number of Annotation Action Downstream Sizespecies frequency classes tasks Kinetics400 (Kay et al., 2017) 1 (human) clip 400 x 306k clips HMDB (Kuehne et al., 2011) 1 (human) clip 51 x 6776 clips UCF (Soomro et al., 2012) 1 (human) clip 101 x 13320 clips Animal Kingdom (Ng et al., 2022) 850 frame 140 x 4.5M frames CalMS21 (Sun et al., 2021a) 1 frame 7 x 1M frames +6M unlabelled Fly vs. Fly (Eyjolfsdottir et al., 2014) 1 frame 10 x 1.5M frames CRIM13 (Burgos-Artizzu et al., 2012) 1 frame 13 x 8M frames Our Dataset 4 frame 16 56 15M frames video + from experiments 14M frames traj Table 1. Comparison with commonly used, public video and trajectory datasets. While existing datasets can be used for behavioral representation learning, the downstream evaluation focuses on a single type of task (detection and classification of human-annotated actions) or a single species. Our benchmark introduces a rich set of downstream analysis tasks that we obtain from scientific experiments on multiple species. ior. In contrast, our animal videos are all acquired against a stationary, neutral background, forcing models to use the temporal structure of the data to distinguish between actions. Related Problems in Multi-Agent Behavior. While our dataset is composed of multi-agent data from biology, there are also multi-agent behavior datasets from other domains, such as from autonomous driving (Chang et al., 2019; Sun et al., 2020), sports analytics (Yue et al., 2014; Decroos et al., 2018), and video games (Samvelyan et al., 2019; Guss et al., 2019). These datasets often focus on forecasting, motion planning, and reinforcement learning, whereas our dataset is used for tasks from scientific applications, such as distinguishing animal strains via observed behaviors. Work in Animal Behavior Analysis. In biology and neu- roscience, computational models of behavior have the po- tential to significantly reduce human data annotation efforts, and to provide more detailed descriptions of the behavior in question (Anderson & Perona, 2014; Pereira et al., 2020). Automated characterizations of animal behavior have been used to study the relationship between neural activity and behavior (Markowitz et al., 2018), to characterize behavioral differences between species and between different strains within a species (Hern´andez et al., 2020), and to quantify the effect of functional or pharmacological perturbations (Ro- bie et al., 2017; Wiltschko et al., 2020). The input to these models may be video (Bohnslav et al., 2021) or trajectory data (Sun et al., 2021b; Segalin et al., 2020). Supervised behavior models have been trained to identify human-defined behaviors-of-interest (Hong et al., 2015; Se- galin et al., 2020; Marks et al., 2022; Kabra et al., 2013), often using frame-by-frame behavior annotations from do- main experts. Another body of work discovers behaviors without human annotations, using unsupervised and self- supervised methods (Berman et al., 2014; Wiltschko et al., 2015; Hsu & Yttri, 2020; Luxem et al., 2020; Calhoun et al., 2019) that learn the latent structure of behavioral data. The learned representation may be continuous (Sun et al., 2021b), or discrete, such as when discovering behavior mo- tifs (Berman et al., 2014; Wiltschko et al., 2015; Hsu & Yttri, 2020). There currently does not exist a unified behavioral representation learning dataset that can compare these mod- els across a broad range of behavior analysis settings. Here, we propose MABe 2022 for evaluating the performance of these representation learning methods. Work in Representation Learning. Representation learn- ing for visual (Gidaris et al., 2018; Chen et al., 2020b; Oord et al., 2018; Kolesnikov et al., 2019; Han et al., 2019) and trajectory data (Sun et al., 2021b; Zhan et al., 2021) has been applied to a variety of tasks, such as for image classifi- cation (Chen et al., 2020b), speech recognition (Oord et al., 2018), and behavior classification (Sun et al., 2021b). In these works, many different unsupervised / self-supervised methods have been developed, employing various pretext tasks to pre-train a model, such as classifying image ro- tations (Gidaris et al., 2018), predicting future observa- tions (Oord et al., 2018), contrastive learning with image augmentations (Chen et al., 2020b), and decoding program- matic attributes (Sun et al., 2021b). The quality of learned representations is often evaluated on downstream tasks. Behavioral Representation Learning. For behavior anal- ysis, applications of representation learning include dis- covering behavior motifs (Berman et al., 2014; Wiltschko et al., 2015; Hsu & Yttri, 2020; Luxem et al., 2020), iden- tifying internal states (Calhoun et al., 2019), and improv- ing sample-efficiency of supervised classifiers (Sun et al., 2021b). These works use methods such as variational au- toencoders (Kingma & Welling, 2014), autoregressive hid- den Markov models (Wiltschko et al., 2015), and Uniform Manifold Approximation and Projection (UMAP) (McInnes et al., 2018) to characterize the latent structure of behavior. 3MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Notably, many groups have proposed methods for unsu- pervised behavior discovery (Berman et al., 2014; Klibaite et al., 2017; Wiltschko et al., 2015; Luxem et al., 2020; Hsu & Yttri, 2020; Marques et al., 2018). These works use dif- ferent methods to model the temporal structure of behavior, including wavelet transforms (Berman et al., 2014), autore- gressive hidden Markov models (Wiltschko et al., 2015), and recurrent NNs (Luxem et al., 2020), as well as different methods for segmenting behavior, such as Gaussian mixture models (Hsu & Yttri, 2020), k-means clustering (Luxem et al., 2020), and watershed transforms (Berman et al., 2014). Our goal is to develop a standardized dataset for evaluating these methods on a common set of behavior analysis tasks. 3. Dataset Design and Collection We designed and curated MABe22, a multi-agent behavior dataset for the purpose of studying behavioral representa- tion learning. Our dataset consists of data from multiple model organisms in neuroscience/biology: mice, beetles, and flies. For each dataset, we constructed a collection of tasks based on real-world scientific applications, including determining the experimental context of the organisms and capturing expert-annotated behaviors. There are 72 tasks in total: 8 for mice, 14 for beetles, and 50 for flies. For the purpose of establishing a benchmark, we define a ”good” learned representation of animal behavior that can decode biologically meaningful hidden labels as well as annotations by experts. Some tasks apply to all frames of the recording (e.g. strain of mice), but not all tasks are apply to all frames (e.g. sniffing, since experts may annotate only a subset of the videos). More details are available in the datasheet for our dataset (Appendix B). The mouse dataset (Section 3.1) consists of 2614 clips of video and trajectory data (1 minute each at 30 Hz) curated from longer videos of a triplet of interacting mice over mul- tiple recording days. The video and trajectory datasets are from the same clips, and the mice are tracked using (Shep- pard et al., 2022). We additionally release a larger set of 5336 clips of trajectory data for evaluating community- contributed methods (only used in Appendix F). The beetle dataset (Section 3.2) consists of 11536 clips of video (30 sec- onds each at 30 Hz) curated from paired interactions of rove beetles (Sceptobius lativentris) with intact or manipulated members of their symbiotic host species, the velvety tree ant (Liometopum occidentale), or with other beetle species. The fly dataset (Section 3.3) consists of 968 clips of tra- jectory data (30-second clips at 150 Hz) of groups of 8-11 interacting flies, tracked using (Kabra et al., 2022). 3.1. Mouse Triplets Data Description. The mouse dataset consists of a set of videos and trajectories from three interacting mice, recorded from an overhead camera in an open field arena measuring 52cm x 52cm, with a grate located at the northern wall of the arena giving access to food and water. Animals were introduced to the arena one by one over the first ten minutes of recording and were recorded continuously for four days at a framerate of 30 Hz and a camera resolution of 800 x 800 pixels. Illumination was provided by an overhead light on a 24-hour reverse light cycle (lights off during the day and on at night); mice are nocturnal and thus are most active during the dark. Behavior was recorded using an IR-pass filter so that light status could not be detected by the eye in the recorded videos. Animals’ posture was tracked using a pose estimation model (Sheppard et al., 2022) based on HRNet (Sun et al., 2019) with an identity embedding network to track long-term identity. Tasks. Representations of the mouse dataset are evalu- ated on 8 tasks that capture information about animals’ ge- netic background, environment, and expert-annotated be- haviors. These tasks were selected based on their relevance to common scientific applications such as identifying the behavioral effects of differences in animals’ genetic back- grounds or experimenter-imposed changes in their environ- ment. We examined capacity of learned representations to determine animal strain, as well as environmental fac- tors such as whether room lights were on or off (a proxy for day/night cycles, which modulate animal behavior). We also included two tasks to predict the day of the trajectory rela- tive to the start of recording (animal behavior changes across days as they habituate to a new environment (Klibaite et al., 2022)), and the time of day of the trajectory (animal behav- ior changes over the course of a day, driven by circadian rhythms). A learned representation of behavior should also be rich enough to recapitulate human-produced labels of ani- mals’ moment-to-moment actions. Therefore our evaluation tasks include the detection of expert-annotated behaviors: huddling, chasing, face sniffing, and anogenital sniffing. A detailed description of the tasks is listed in Appendix C.2. 3.2. Beetle Interactions Dataset description. The beetle dataset consists of a rove beetle (Sceptobius lativentris) interacting one-on-one with its host ant ( Liometopum occidentale), manipulated host ant (e.g., with pheromones stripped off) or with other insects (e.g., a nitidulid beetle). The original experiment consisted of two-hour interaction trials, from which we ex- tracted a collection of 30-second clips. These recordings were made in 8-well behavioral interaction chambers (2cm diameter circles) in the dark and illuminated with inferred lights from the side/top. A top-mounted machine vision camera sensitive to IR light monitored the two-hour behav- ioral trials at 60 Hz. For this dataset, individual circular wells were cropped/parsed from the multi-well video and saved at 800x800 resolution with downsampling to 30 Hz. 4MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Mouse Triplet Experimental Context Manually annotated behaviors Species time of day mouse strain lightsexperiment day chase huddle face sniﬀ anal sniﬀ Ant Beetle grooming exploring idleinteractor type  duration Fly neuronal subpopulations optogenetic manipulation thermogenetic  manipulation  aggression  courtship  chase Figure 2.Summary of tasks and actions in our dataset. Our dataset includes three different species: mice, beetles with an intractor (an ant or other another beetle), and flies. The mouse dataset has both video and trajectory available, the beetle dataset is video-based, and the fly dataset is trajectory based. Classification of experimental conditions is used as a performance metric (examples depicted on the left for each dataset). Additionally, we collected conventionally expert-annotated actions (examples depicted on the right for each dataset), with frame-by-frame labels, e.g., as ”chase”, ”huddle”, ”face sniff”, and ”anogenital sniff” for mice. Overall, there are 72 behavior analysis tasks: 8 for mice, 14 for beetles and 50 for flies. Tasks. The beetle dataset includes tasks based on environ- mental conditions as well as expert-annotated behaviors. Labels for environmental conditions include the interactor type (the species of insect the rove beetle interacts with, and any experimental manipulations applied) as well as how long into the two-hour assay the observed clip occurred. The interactors represent a range of cue types, from the host organism with which the beetle should interact exten- sively to other insects that the beetle will likely ignore. We also provide expert annotations for six behaviors across the seven different types of one-on-one interactions. Generat- ing a meaningful representation that extracts information of interest about the different behaviors adopted by the beetle in response to these disparate cues is crucial for insight into how species interact in nature. Details about the interaction tasks are described in Appendix C.3. 3.3. Fly Groups Data Description. The fly dataset consists of trajectories of groups of 8 to 11 vinegar flies (Drosophila melanogaster) interacting in a 5cm-diameter dish. The trajectories were derived from 96 videos of length 50k-75k frames, collected at 1024x1024 pixels and 150 frames per second. The flies’ bodies and wings were tracked using FlyTracker (Eyjolfs- dottir et al., 2014), and landmarks on the body were tracked using the Animal Part Tracker (APT) (Kabra et al., 2022) producing a total of 19 keypoints per tracked animal (details in Appendix C.1) As the brain controls behavior, a good representation of behavior should change with neural activity. Thanks to its tractable genetics, precise neural activity manipulations are straightforward in Drosophila. We thus chose to per- form experiments using optogenetic (light-activated neural activity via Chrimson) (Klapoetke et al., 2014) and ther- mogenetic (heat activated, via TrpA) (Robie et al., 2017) activation of selected sets of neurons. We chose neurons (and the associated GAL4 lines) previously identified as controlling social behaviors, including courtship, avoidance (Robie et al., 2017), and female aggression (Schretter et al., 2020). For thermogenetic experiments, neural activation is constant and continuous for the entire video. Our optoge- netic experiments consisted of activation for short periods of time at weak and strong intensities interspersed with periods of no activation. We combined these neural manipulations with genetic mutations and rearing conditions. Specifically, we selected populations of flies with the norpA mutation, 5MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior which induces blindness (Bloomquist et al., 1988), and ei- ther raised groups of flies together or separated by sex. Tasks. The representations of the fly dataset are evaluated on a set of 50 tasks. Many of these tasks differentiate which populations of neurons are activated and how they are ac- tivated. For example, Task 5 indicates the activation of courtship neurons targeted by the R71G01 GAL4 line in groups of 5 male and 5 female flies. Task 31 compares how neurons were activated – it compares strong and weak acti- vation of aIPg neurons, which regulate female aggression. Besides neural activation, tasks also differentiate flies based on sex, how the flies were raised, which strain they are from, and genetic mutations. A full list of tasks and the types of flies used are in Appendix C.1. Besides biological differences, we also include tasks based on manual annotations of the flies’ behavior for the fol- lowing social behaviors: any aggressive behavior toward another fly, chasing another fly, any courtship behavior to- ward another fly, high fencing, wing extension, and wing flick. We annotated behaviors sparsely across all videos with human experts using JAABA (Kabra et al., 2013), with the goal of including annotations in a wide variety of flies and videos. 4. Benchmarking & Methods We study how well behavioral representations generated by state-of-the-art self-supervised video representation learn- ing methods are suited for decoding our hidden downstream biological tasks and human annotations (Section 4.1). We also solicit community-contributed methods for video and trajectory representation learning through an open competi- tion (Section 4.2). The representation learned by the models is a mapping from each video frame/trajectory entry to a lower dimensional vector of fixed size. Here, we assume the evaluation tasks are hidden during representation learning. We then use this representation of the data to train a linear model to classify or regress to target values of the hidden downstream task (Appendix D). 4.1. Self-supervised Video Representation Learning Self-supervised video representation learning methods rely on designing pretext tasks that make use of prior knowledge about spatial and temporal information in videos to design pretext tasks such as temporal coherence (Goroshin et al., 2015), temporal ordering (Misra et al., 2016), the motion of an object (Agrawal et al., 2015), future prediction (Walker et al., 2016). Contrastive learning (Chen et al., 2020b; He et al., 2020) has been used for learning good visual repre- sentations for instance discrimination. Another line of work has been introducing methods that solely rely on positive samples (Grill et al., 2020; Caron et al., 2020). In a recent comparison, the video version of Bootstrap Your Own La- tent (BYOL) (Grill et al., 2020) has been shown to perform very well on the classic human benchmarks (Feichtenhofer et al., 2021), with increased performance for an increased number of positive samples. Masked Visual Modeling. Transformers (Vaswani et al., 2017) set the state-of-the-art across many AI fields, bridg- ing language and vision models. Inspired by pretext tasks for language transformer models, such as masking in BERT (Devlin et al., 2018), (He et al., 2022) recently introduced the Masked Auto-Encoder (MAE) for images, an effective pre-training method, by which an image is split into patches, and about 70 percent of the patches are masked. Based on the remaining patches, the task for the transformer is to reconstruct the masked patches. (Feichten- hofer et al., 2022; Tong et al., 2022) extended this framework to video, demonstrating transformers can be effectively pre- trained by masking 90 percent of the spatio-temporal vol- ume. MaskFeat (Wei et al., 2022) showed that using HOG features (Dalal & Triggs, 2005) as reconstruction targets of masked patches is an effective pre-text task. 4.2. Community-Contributed Methods In addition to studying state-of-the-art methods, our bench- marking efforts include community-contributed methods from an open competition. Our competition was hosted in two stages, where stage 1 consisted of the trajectory datasets from mouse and fly, and stage 2 consisted of video datasets from mouse and beetle. The test sets were private during the competition phase, and are now released as part of MABe22. We obtained around 1500 submissions in total at the end of the competition, and we summarize the top-performing method for the mouse, fly, and beetle datasets from this process for both video and trajectory data, with details for all methods in Appendix Section A. 5. Experiments We perform a large set of experiments to evaluate the perfor- mance of representation learning methods on MABe 2022 (Sections 5.1, 5.2). As video representation methods are more common, we focus on state-of-the-art video represen- tation learning methods in this section. We additionally compare both community contributed video and trajectory representation learning methods. For each video representa- tion learning method, we perform an ablation study on the key hyperparameter for the respective method and its effect on downstream task performance (Sections 5.3, 5.4), as well as pre-training on human datasets (Section 5.5). Finally, we present results from community-contributed methods on all datasets (Section 5.6), with additional results for the trajectory methods in Appendix F. 6MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Mouse Triplets Exp. Time of Strain Lights Manual Day↓ Day↓ ↑ ↑ Behaviors↑ ρBYOL (R-50 (Slow Pathway) 8x8 (Feichtenhofer et al., 2021).0152 .0913 .9997 .9701 0.1832 Maskfeat (MViTv2-S 16x4) (Wei et al., 2022) .0393 .0948 .9925 .7309 0.1627 MAE (ViT-B 16x4) (Feichtenhofer et al., 2022) .0102 .0816 1.0000 .9758 0.2309 (pretrained)ρBYOL (-, R-50 (Slow Pathway) 8x8 .0176 .0910 .9994 .7967 0.2688 (pretrained) Maskfeat (-, MViTv2-S 16x4) .0456 .0889 .9998 .7892 0.1896 (pretrained) MAE (-, ViT-B 16x4) .0218 .0925 1.0000 .9391 0.2301 Ant Beetle Duration Interactor Manual Manual ↓ Type↑ Behaviors↑ Behaviors (same)↑ ρBYOL (50 (Slow Pathway) 8x8 (Feichtenhofer et al., 2021) .0257 .9999 .6178 .6457 Maskfeat (MViTv2-S 16x4) (Wei et al., 2022) .0291 1.0000 .6212 .6574 MAE (ViT-B 16x4) (Feichtenhofer et al., 2022) .0283 1.0000 .6444 .6874 (pretrained)ρBYOL (R-50 (Slow Pathway) 8x8 .0300 .9981 .6967 .7334 (pretrained) Maskfeat (MViTv2-S 16x4) .0297 .9999 .6057 .6463 (pretrained) MAE (ViT-B 16x4) .0300 .9999 .6879 .7077 Table 2.Evaluating self-supervised video representation learning methods. We evaluate representation learning performance using the linear evaluation protocol on downstream biologically relevant tasks. (pretrained) indicates pre-training on Kinetics400. ↓ indicates MSE and ↑ indicates F1 score. Mouse manual behaviors consist of chase, huddle, face sniff, anal sniff. Beetle manual behaviors consist of grooming, exploring, and idle, either for self (beetle) only or with the interactor. Experimental tasks are described in Table 6 and C.3.2. The best-performing model is in bold. Mice Triplet Exp. Time of Strain Lights Manual Day↓ Day↓ ↑ ↑ Behaviors↑ MAE Frame .0239 .0886 1.000 .9525 .2020 MAE Cube .0102 .0816 1.000 .9758 .2309 MAE Tube .0072 .0835 1.000 .9846 .2249 Ant Beetle Duration Interactor Manual Manual ↓ Type↑ Behaviors↑ Behaviors (same)↑ MAE Frame .0301 .9999 .6169 .6497 MAE Cube .0283 1.0000 .6444 .6874 MAE Tube .0285 1.0000 .5802 .6351 Table 3.Effect of masking strategy on MAE (Feichtenhofer et al., 2022) performance . We evaluate different masking strategies (spatiotemporal random/cube, temporal/tube and spatial/frame) on the video datasets of MABe2022. For the mouse dataset cube/tube masking perform best, whereas for the beetle dataset cube/frame masking perform best. ↓ indicates MSE and ↑ indicates F1 score. The best-performing model is in bold. 5.1. Evaluation Procedure From an input sequence of video/trajectory data of N frames (N = 1800 for mice and 4500 for flies), we evaluate models that produce learned representations of size N × D, where D is the dimensionality of the representations. For video representation learning models, we use D = 128. For trajectory methods, we use D = 128 for mice and D = 256 for flies. We then use these feature vectors or embeddings as inputs for a linear model that is used to classify/regress the hidden task. We use linear least squares with l2 regularized (Ridge) classification/regression as model and F1/mean- squared-error (MSE) as evaluation metrics (See Appendix D for details). We evaluate a set of state-of-the-art video representation learning methods on MABe 2022, including Masked Au- toencoder (MAE) (Feichtenhofer et al., 2022) with a ViT-B backbone (Vaswani et al., 2017), MaskFeat (Wei et al., 2022) with a MViTv2-S backbone (Li et al., 2022) and ρBYOL (Feichtenhofer et al., 2021) with a SlowFast backbone (Slow pathway 8x8) (Frankenhuis et al., 2019). We trained each method on our mice and beetle data, respectively, as well as used backbones pre-trained on human kinetics 400 (Kay et al., 2017). For implementation details and hyperparame- ters see Appendix E. 5.2. Video Representation Results We compare the performance of video representation learn- ing methods on the mouse and beetle video datasets (Table 2). We find that the pre-trained ρBYOL (R-50 (Slow Path- way) 8x8 model performs best for all action recognition tasks (Manuel Behaviors). For all other downstream tasks training, a ViT-B 16x4 Masked Autoencoder (MAE) that is not pre-trained on Kinetics400 generally performs the best. This top performing MAE architecture uses spatio-temporal 7MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Mice Triplet Exp. Time of Strain Lights Manual Day↓ Day↓ ↑ ↑ Behaviors↑ 2BYOL .0298 .0882 .9994 .9588 .1929 3BYOL .0225 .0906 .9983 .9492 .1733 4BYOL .0152 .0913 .9997 .9701 .1771 Ant Beetle Duration Interactor Manual Manual ↓ Type↑ Behaviors↑ Behaviors (same)↑ 2BYOL .0237 1.0000 .5943 .6498 3BYOL .0246 1.0000 .6249 .6549 4BYOL .0257 .9999 .6178 .6457 Table 4. Effect of ρ on BYOL (Feichtenhofer et al., 2021) performance. We evaluated the effect of the number of randomly sampled positives for ρBYOL. We find that for beetle 3 positive samples consistently have the best performance, while for mice, either 2 or 4 positives perform best depending on the task. ↓ indicates MSE and ↑ indicates F1 score. The best-performing model is in bold. agnostic masking, which likely performs well due to the observation that our datasets have very different spatio- temporal dynamics from each other and even more so from human datasets. We further discuss this in Section 5.3. We notice that the model that performs best for human anno- tated behaviors does not necessarily perform best for our downstream tasks that are based on experimental conditions. This indicates that models that pick up features that are most relevant for human perception and behavior definitions may not necessarily be the most informative features for other tasks. 5.3. Effect of Masking Strategy We explore how different masking strategies (spatiotem- poral random/cube, temporal/tube and spatial/frame from MAE (Feichtenhofer et al., 2022)) affect downstream task performance (Table 3), and we use best performing masking ratios used in MAE. We find that contrary to (Feichtenhofer et al., 2022; Tong et al., 2022), where performances for spatio-temporally agnostic masking (cube) and temporal masking (tube) are very similar to each other, our perfor- mance depends on the dataset (mouse or beetle). For the mouse dataset, cube/tube masking have the best overall per- formance, while for the beetle dataset, cube performs best overall. Overall the differences in performance are also bigger than in (Feichtenhofer et al., 2022; Tong et al., 2022). This difference in performance for different masking strate- gies is likely due to the different spatio-temporal structure of the data, i.e. if the data is more ’temporal heavy’ or more ’spatial heavy’. 5.4. Effect of ρ on BYOL We performed ρBYOL (Feichtenhofer et al., 2021) with multiple values of ρ, i.e., the number of temporal clips sam- pled as positives (Table 4). In (Feichtenhofer et al., 2021), a larger number of ρ steadily increases downstream task performance. This is not true for our datasets, where for mice a value of 2 performs best for 2 tasks and a value of 4 for 2 other tasks. For the beetle dataset, 3 positive samples achieve the best BYOL performance. This is likely to the temporally random sampling of positives for BYOL. This is likely due to the temporally agnostic sampling method for the clips resulting in positives that are of different ac- tions (as the actions of the animals can change rapidly over temporally close frames). Further research is needed on how the temporal sampling strategy for positives needs to be adjusted for temporally heavy datasets. 5.5. Transfer Learning from Kinetics400 We evaluated howρBYOL, Maskfeat, and MAE perform when pre-trained on kinetics400 (Kay et al., 2017) (Table 2). We find that MAE and Maskfeat training on MABe22 generally performs better than using the pre-trained models. Interestingly, for ρBYOL we find the opposite, in that the pre-trained model on Kinetics400 actually performs stronger than counterpart trained on MABe22. Surprisingly, for action recognition, it performed stronger than any of the other models for both mice and beetle data. These results suggest that for action recognition, transfer learning from human datasets to animal datasets is possible to a degree. 5.6. Community-Contributed Methods Results We compare community-contributed methods across all datasets in MABe22 (Table 5). The best-performing com- munity methods employ large pre-trained vision models, variations of contrastive learning (Chen et al., 2020b; He et al., 2020), trajectory data as additional inputs and hand- crafted features (See Appendix A.1). Usually, these features are then concatenated and PCA is performed to produce vec- tors with the embedding dimension. For the mouse dataset, we also compared the top trajectory-based method to the video-based methods on the same data subset. While the performance of the trajectory model for behavior classifi- cation is similar to the third-best video-based model, the performance on all other downstream tasks is worse. This is likely due to the loss of visual features after transforming 8MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Mice Triplet Exp. Time of Strain Lights Manual Day↓ Day↓ ↑ ↑ Behaviors↑ BEiT + Hand-crafting .0093 .0926 1.0000 .9471 .2603 Vision Ensemble .0441 .0922 .9832 .8048 .2750 Multimodal MoCo/SimCLR .0394 .0912 .9902 .7780 .2355 Trajectory-BERT .0932 .0996 .7202 .6729 .2379 Ant Beetle Duration Interactor Manual Manual ↓ Type↑ Behaviors↑ Behaviors (same)↑ BEiT + Hand-crafting .0277 .9977 .6761 .7179 Vision Ensemble .0295 .9636 .6277 .6695 Multimodal MoCo/SimCLR .0262 .9998 .7299 .7577 Fly Group Fly Stimulation, Stimulation, Line Female Manual Type↑ Control↑ Aggression↑ Category↑ vs. Male↑ Behaviors↑ Trajectory-Perceiver .394 .418 .513 .573 .982 .197 Trajectory-GPT .363 .515 .500 .557 .873 .246 Table 5.Benchmarking the community contributed methods. The best community-contributed methods perform on par or better with self-supervised video representation learning methods. For mice we also have a trajectory-based method to compare to the video-based methods directly. We find that the trajectory-based method generally does not perform as well as the video-based methods on the mouse dataset. For fly task groups, “Fly type” corresponds to tasks 1 to 11, “Stimulation Control” is tasks 12 to 21, “Stimulation Aggression” is tasks 22 to 36, “Line Category” is tasks 37 to 43, and “Manual Behaviors” is tasks 45 to 50 in Appendix Table 11. ↓ indicates MSE and ↑ indicates F1 score. The best-performing model is in bold. the video frames to sparse keypoint locations. An inter- esting direction for future work would be to explore how these modalities can be best combined. For the fly dataset (which consists of trajectory data only), we find that using a Perceiver model (Jaegle et al., 2021) trained on a masked modeling task works best (See Appendix A.2). The sec- ond best method is using a GPT (Brown et al., 2020)-like architecture that generates embeddings from the recurrent trajectory data of all agents. This method is trained using a prediction pretext task. In general, we find that performance is comparable between community-contributed methods to state-of-the-art video representation learning methods evaluated in Section 5.2. We note that community methods did perform better at learning manual behaviors. This may be due to the hand- crafted features used in the community-contributed methods, which has been shown to be effective at encoding domain knowledge for behavior analysis (Sun et al., 2021b). 6. Discussion and Future Directions We introduced a novel multi-species multi-task performance benchmark to evaluate representation learning for social be- havior from video and trajectory data. The dataset consists of video and trajectory data captured across three organisms. The evaluation methods are based on the performance of a broad palette of tasks that are based on scientific experimen- tal conditions that are independent of the actions annotated by human experts. We demonstrate the use of our bench- mark, and provide a baseline, by evaluating state-of-the-art self-supervised video-representation learning. Additionally, we provide results from methods that were part of a recent competition for learning behavioral representations. We compare method performance on our benchmark with pre-training on existing benchmarks using human video datasets. We find that methods that perform best on human datasets may not perform the best on our animal datasets. This is likely because human action datasets contain extra- neous visual information, whereas our animal datasets min- imize these visual cues (consistent backgrounds) and thus behavioral representations need to focus on spatio-temporal information. This highlights a crucial shortcoming of cur- rent benchmarks, which may be pushing the community to develop methods that do not focus on the spatio-temporal nature of behavior. We hope to encourage evaluation of rep- resentation learning methods on a broader range of settings beyond human videos and annotations, in order to facilitate development of new methods for representation learning and behavior analysis. 7. Acknowledgements This work was generously supported by the Simons Collabo- ration on the Global Brain grant 543025 (to PP), NIH Award #R00MH117264 (to AK), NSF Award #1918839 (to YY), NIH 1R34NS118470-01 (to JP), NSERC Award #PGSD3- 532647-2019 (to JJS), as well as a gift from Charles and Lily Trimble (to PP). We would like to thank Tom Sproule for mouse breeding and dataset collection. The mouse dataset was supported by the National Institute of Health DA041668 (NIDA), DA048634 (NIDA, and Simons Foun- dation SFARI Director’s Award) (to VK). We also greatly appreciate Google, Amazon, HHMI, and the Simons Foun- dation for sponsoring the MABe22 Challenge & Workshop. 9MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior References Agrawal, P., Carreira, J., and Malik, J. Learning to see by moving. In Proceedings of the IEEE international conference on computer vision, pp. 37–45, 2015. Anderson, D. J. and Perona, P. Toward a science of compu- tational ethology. Neuron, 84(1):18–31, 2014. Aso, Y ., Sitaraman, D., Ichinose, T., Kaun, K. R., V ogt, K., Belliart-Gu´erin, G., Plac ¸ais, P.-Y ., Robie, A. A., Yama- gata, N., Schnaitmann, C., et al. Mushroom body output neurons encode valence and guide memory-based action selection in drosophila. Elife, 3:e04580, 2014. Bao, H., Dong, L., and Wei, F. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. Beane, G., Geuther, B. Q., Sproule, T. J., Trapszo, J., Hes- sion, L., Kohar, V ., and Kumar, V . Video based phenotyp- ing platform for the laboratory mouse. bioRxiv, 2022. Berman, G. J., Choi, D. M., Bialek, W., and Shaevitz, J. W. Mapping the stereotyped behaviour of freely moving fruit flies. Journal of The Royal Society Interface, 11(99): 20140672, 2014. Bloomquist, B. T., Shortridge, R., Schneuwly, S., Perdew, M., Montell, C., Steller, H., Rubin, G., and Pak, W. Iso- lation of a putative phospholipase c gene of drosophila, norpa, and its role in phototransduction. Cell, 54(5): 723–733, 1988. Bohnslav, J. P., Wimalasena, N. K., Clausing, K. J., Dai, Y . Y ., Yarmolinsky, D. A., Cruz, T., Kashlan, A. D., Chi- appe, M. E., Orefice, L. L., Woolf, C. J., et al. Deep- ethogram, a machine learning pipeline for supervised behavior classification from raw pixels. Elife, 10:e63377, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–1901, 2020. Burgos-Artizzu, X. P., Doll´ar, P., Lin, D., Anderson, D. J., and Perona, P. Social behavior recognition in continuous video. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1322–1329. IEEE, 2012. Calhoun, A. J., Pillow, J. W., and Murthy, M. Unsupervised identification of the internal states that shape natural be- havior. Nature neuroscience, 22(12):2040–2049, 2019. Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912–9924, 2020. Chang, M.-F., Lambert, J., Sangkloy, P., Singh, J., Bak, S., Hartnett, A., Wang, D., Carr, P., Lucey, S., Ramanan, D., et al. Argoverse: 3d tracking and forecasting with rich maps. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pp. 8748–8757, 2019. Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In International conference on machine learning, pp. 1691– 1703. PMLR, 2020a. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual rep- resentations. ICML, 2020b. Co-Reyes, J. D., Liu, Y ., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory em- beddings. arXiv preprint arXiv:1806.02813, 2018. Dalal, N. and Triggs, B. Histograms of oriented gradients for human detection. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’05), volume 1, pp. 886–893. Ieee, 2005. Decroos, T., Van Haaren, J., and Davis, J. Automatic dis- covery of tactics in spatio-temporal soccer match data. In Proceedings of the 24th acm sigkdd international confer- ence on knowledge discovery & data mining, pp. 223–232, 2018. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. Dutta, A. and Zisserman, A. The via annotation software for images, audio and video. In Proceedings of the 27th ACM international conference on multimedia, pp. 2276–2279, 2019. 10MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Eyjolfsdottir, E., Branson, S., Burgos-Artizzu, X. P., Hoopfer, E. D., Schor, J., Anderson, D. J., and Perona, P. Detecting social actions of fruit flies. In European Conference on Computer Vision, pp. 772–787. Springer, 2014. Fan, H., Li, Y ., Xiong, B., Lo, W.-Y ., and Feichten- hofer, C. Pyslowfast. https://github.com/ facebookresearch/slowfast, 2020. Feichtenhofer, C., Fan, H., Xiong, B., Girshick, R., and He, K. A large-scale study on unsupervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3299–3309, 2021. Feichtenhofer, C., Fan, H., Li, Y ., and He, K. Masked autoencoders as spatiotemporal learners. arXiv preprint arXiv:2205.09113, 2022. Frankenhuis, W. E., Panchanathan, K., and Barto, A. G. Enriching behavioral ecology with reinforcement learning methods. Behavioural processes, 161:94–100, 2019. Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daum´e III, H., and Crawford, K. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018. Geuther, B. Q., Deats, S. P., Fox, K. J., Murray, S. A., Braun, R. E., White, J. K., Chesler, E. J., Lutz, C. M., and Kumar, V . Robust mouse tracking in complex environments using neural networks. Communications biology, 2(1):1–11, 2019. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. ICLR, 2018. Goroshin, R., Bruna, J., Tompson, J., Eigen, D., and LeCun, Y . Unsupervised learning of spatiotemporally coherent metrics. In Proceedings of the IEEE international confer- ence on computer vision, pp. 4086–4093, 2015. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020. Guss, W. H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., and Salakhutdinov, R. Minerl: A large- scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019. Han, T., Xie, W., and Zisserman, A. Video representation learning by dense predictive coding. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 0–0, 2019. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738, 2020. He, K., Chen, X., Xie, S., Li, Y ., Doll´ar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022. Hern´andez, D. G., Rivera, C., Cande, J., Zhou, B., Stern, D. L., and Berman, G. J. A framework for studying be- havioral evolution by reconstructing ancestral repertoires. arXiv preprint arXiv:2007.09689, 2020. Hofmann, K. Minecraft as ai playground and laboratory. In Proceedings of the Annual Symposium on Computer- Human Interaction in Play, pp. 1–1, 2019. Hong, W., Kennedy, A., Burgos-Artizzu, X. P., Zelikowsky, M., Navonne, S. G., Perona, P., and Anderson, D. J. Au- tomated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning. Pro- ceedings of the National Academy of Sciences, 112(38): E5351–E5360, 2015. Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W., Zhu, Y ., Pang, R., Vasudevan, V ., et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1314–1324, 2019. Hsu, A. I. and Yttri, E. A. B-soid: An open source unsuper- vised algorithm for discovery of spontaneous behaviors. bioRxiv, pp. 770271, 2020. Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., et al. Perceiver io: A general archi- tecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021. Joshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer, L., and Levy, O. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64– 77, 2020. doi: 10.1162/tacl a 00300. URL https: //aclanthology.org/2020.tacl-1.5. 11MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Kabra, M., Robie, A. A., Rivera-Alba, M., Branson, S., and Branson, K. Jaaba: interactive machine learning for automatic annotation of animal behavior.Nature methods, 10(1):64, 2013. Kabra, M., Lee, A., Robie, A., Egnor, R., Huston, S., Ro- driguez, I. F., Edwards, A., and Branson, K. Apt: An- imal part tracker v0.3.4, March 2022. URL https: //doi.org/10.5281/zenodo.6366082. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In International Conference on Learning Repre- sentations, 2014. Klapoetke, N. C., Murata, Y ., Kim, S. S., Pulver, S. R., Birdsey-Benson, A., Cho, Y . K., Morimoto, T. K., Chuong, A. S., Carpenter, E. J., Tian, Z., et al. Inde- pendent optical excitation of distinct neural populations. Nature methods, 11(3):338–346, 2014. Klibaite, U., Berman, G. J., Cande, J., Stern, D. L., and Shaevitz, J. W. An unsupervised method for quantifying the behavior of paired animals. Physical biology, 14(1): 015006, 2017. Klibaite, U., Kislin, M., Verpeut, J. L., Bergeler, S., Sun, X., Shaevitz, J. W., and Wang, S. S.-H. Deep phenotyping reveals movement phenotypes in mouse neurodevelop- mental models. Molecular Autism, 13(1):1–18, 2022. Kolesnikov, A., Zhai, X., and Beyer, L. Revisiting self- supervised visual representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1920–1929, 2019. Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., and Serre, T. Hmdb: a large video database for human motion recog- nition. In 2011 International conference on computer vision, pp. 2556–2563. IEEE, 2011. Li, Y ., Wu, C.-Y ., Fan, H., Mangalam, K., Xiong, B., Malik, J., and Feichtenhofer, C. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4804–4814, 2022. Loshchilov, I. and Hutter, F. Stochastic gradient descent with warm restarts. In Proceedings of the 5th Int. Conf. Learning Representations, pp. 1–16. Loshchilov, I. and Hutter, F. Fixing weight decay regular- ization in adam. CoRR, abs/1711.05101, 2017a. URL http://arxiv.org/abs/1711.05101. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization. arXiv preprint arXiv:1711.05101, 2017b. Luxem, K., Fuhrmann, F., K¨ursch, J., Remy, S., and Bauer, P. Identifying behavioral structure from deep variational embeddings of animal motion. bioRxiv, 2020. Markowitz, J. E., Gillis, W. F., Beron, C. C., Neufeld, S. Q., Robertson, K., Bhagat, N. D., Peterson, R. E., Peterson, E., Hyun, M., Linderman, S. W., et al. The striatum organizes 3d behavior via moment-to-moment action se- lection. Cell, 174(1):44–58, 2018. Marks, M., Jin, Q., Sturman, O., von Ziegler, L., Kollmor- gen, S., von der Behrens, W., Mante, V ., Bohacek, J., and Yanik, M. F. Deep-learning-based identification, tracking, pose estimation and behaviour classification of interact- ing primates and mice in complex environments. Nature Machine Intelligence, 4(4):331–340, 2022. Marques, J. C., Lackner, S., F ´elix, R., and Orger, M. B. Structure of the zebrafish locomotor repertoire revealed with unsupervised behavioral clustering. Current Biology, 28(2):181–195, 2018. McInnes, L., Healy, J., and Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. Misra, I., Zitnick, C. L., and Hebert, M. Shuffle and learn: unsupervised learning using temporal order verification. In European conference on computer vision, pp. 527–544. Springer, 2016. Newell, A., Huang, Z., and Deng, J. Associative embed- ding: End-to-end learning for joint detection and group- ing. Advances in neural information processing systems, 30, 2017. Ng, X. L., Ong, K. E., Zheng, Q., Ni, Y ., Yeo, S. Y ., and Liu, J. Animal kingdom: A large and diverse dataset for animal behavior understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19023–19034, 2022. Nilsen, S. P., Chan, Y .-B., Huber, R., and Kravitz, E. A. Gender-selective patterns of aggressive behavior in drosophila melanogaster. Proceedings of the National Academy of Sciences, 101(33):12342–12347, 2004. Oord, A. v. d., Li, Y ., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 12MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Pereira, T. D., Shaevitz, J. W., and Murthy, M. Quantifying behavior to understand the brain. Nature neuroscience, pp. 1–13, 2020. Qi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning on point sets for 3d classification and segmenta- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652–660, 2017. Robie, A. A., Hirokawa, J., Edwards, A. W., Umayam, L. A., Lee, A., Phillips, M. L., Card, G. M., Korff, W., Rubin, G. M., Simpson, J. H., et al. Mapping the neural substrates of behavior. Cell, 170(2):393–406, 2017. Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019. Schretter, C. E., Aso, Y ., Robie, A. A., Dreher, M., Dolan, M.-J., Chen, N., Ito, M., Yang, T., Parekh, R., Branson, K. M., et al. Cell types and neuronal circuitry underlying female aggression in drosophila. Elife, 9:e58942, 2020. Schrimpf, M., Kubilius, J., Lee, M. J., Murty, N. A. R., Ajemian, R., and DiCarlo, J. J. In- tegrative benchmarking to advance neurally mech- anistic models of human intelligence. Neuron, 2020. URL https://www.cell.com/neuron/ fulltext/S0896-6273(20)30605-X. Segalin, C., Williams, J., Karigo, T., Hui, M., Zelikowsky, M., Sun, J. J., Perona, P., Anderson, D. J., and Kennedy, A. The mouse action recognition system (mars): a software pipeline for automated analysis of social behaviors in mice. bioRxiv, 2020. Sheppard, K., Gardin, J., Sabnis, G., Peer, A., Darrell, M., Deats, S., Geuther, B., Lutz, C. M., and Kumar, V . Stride- level analysis of mouse open field behavior using deep- learning-based pose estimation. Cell Reports, 2022. Sokolowski, M. B. Drosophila: genetics meets behaviour. Nature Reviews Genetics, 2(11):879–890, 2001. Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Sun, J. J., Karigo, T., Chakraborty, D., Mohanty, S. P., Wild, B., Sun, Q., Chen, C., Anderson, D. J., Perona, P., Yue, Y ., et al. The multi-agent behavior dataset: Mouse dyadic social interactions. arXiv preprint arXiv:2104.02710, 2021a. Sun, J. J., Kennedy, A., Zhan, E., Anderson, D. J., Yue, Y ., and Perona, P. Task programming: Learning data efficient behavior representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2876–2885, 2021b. Sun, K., Xiao, B., Liu, D., and Wang, J. Deep high- resolution representation learning for human pose esti- mation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5693– 5703, 2019. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Pat- naik, V ., Tsui, P., Guo, J., Zhou, Y ., Chai, Y ., Caine, B., et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2446–2454, 2020. Tong, Z., Song, Y ., Wang, J., and Wang, L. Video- mae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022. Van Horn, G., Cole, E., Beery, S., Wilber, K., Belongie, S., and Mac Aodha, O. Benchmarking representation learning for natural world image collections. InComputer Vision and Pattern Recognition, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017. Walker, J., Doersch, C., Gupta, A., and Hebert, M. An uncertain future: Forecasting from static images using variational autoencoders. In European Conference on Computer Vision, pp. 835–851. Springer, 2016. Wei, C., Fan, H., Xie, S., Wu, C.-Y ., Yuille, A., and Feicht- enhofer, C. Masked feature prediction for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 14668–14678, 2022. Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., Abraira, V . E., Adams, R. P., and Datta, S. R. Mapping sub-second structure in mouse behavior. Neuron, 88(6):1121–1135, 2015. Wiltschko, A. B., Tsukahara, T., Zeine, A., Anyoha, R., Gillis, W. F., Markowitz, J. E., Peterson, R. E., Katon, J., Johnson, M. J., and Datta, S. R. Revealing the structure of pharmacobehavioral space through motion sequencing. Nature Neuroscience, 23(11):1433–1443, 2020. Wu, M., Nern, A., Williamson, W. R., Morimoto, M. M., Reiser, M. B., Card, G. M., and Rubin, G. M. Visual projection neurons in the drosophila lobula link feature detection to distinct behavioral programs.Elife, 5:e21022, 2016. 13MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Yue, Y ., Lucey, P., Carr, P., Bialkowski, A., and Matthews, I. Learning fine-grained spatial models for dynamic sports play prediction. In 2014 IEEE international conference on data mining, pp. 670–679. IEEE, 2014. Zhan, E., Tseng, A., Yue, Y ., Swaminathan, A., and Hausknecht, M. Learning calibratable policies using programmatic style-consistency. ICML, 2020. Zhan, E., Sun, J. J., Kennedy, A., Yue, Y ., and Chaudhuri, S. Unsupervised learning of neurosymbolic encoders. arXiv preprint arXiv:2107.13132, 2021. 14MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Appendix for MABe22 Links to access our code and dataset, including code from challenge winners where available, are on our dataset website at https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset. The sections of our appendix are organized as follows: • Section A contains details of community-contributed methods from our open competition. • Section B contains dataset documentation and intended uses for MABe2022, following the format of the Datasheet for Datasets(Gebru et al., 2018). • Section C contains additional dataset details for mouse, fly, and beetle datasets. • Section D shows the evaluation metrics for MABe2022, namely the F1 score and Mean Squared Error. • Section E contains additional implementation details of our models. • Section F provides additional evaluation results on the trajectory data of MABe22 (mouse and fly). Limitations and next steps. Our dataset is mainly based on three species, and certainly does not saturate the variety of visually distinctive behavior phenomena that one encounters in the world. Additionally, our dataset includes data from one lab per species/preparation and results may not translate to nominally identical preparations in other labs. Future work to incorporate larger amounts of species as well as broader range of tasks can enable benchmark model rankings to be more predictive of method behavior on novel species and tasks. Additionally, we limited our study of self-supervised video representation learning models to meaningful but not complete selection of state-of-the-art methods. This gap will be filled by the community if our benchmark is adopted to evaluate new methods. Broader impact. While the ”quality” of a learned representation will ultimately depend the downstream use, we provide a resource for the assessment of representation utility by scoring learned representations on a large array of hidden tasks, based on common scientific applications. We note that methods that perform best on our benchmark are not guaranteed to be the best choice for all possible downstream uses of representation learning. Depending on the downstream use, model developers may want to consider different choices of architecture, learning objective, and dataset to optimize for different properties of the representation. Our goal is to provide a unified set of tasks across a range of behavior analysis settings that can enable quantitative comparison of representation learning methods, in order to facilitate research and method development for representation learning and behavior analysis. Additionally, we value any input from the community on MABe2022; you can reach us at mabe.workshop@gmail.com. A. Community-Contributed Methods Descriptions We document methods from the community from our open challenge, which ran from February to July 2022. The challenge is available at: https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022. We present the top 3 performing video and trajectory representation learning methods from the challenge winners, for each organism in MABe22. The video challenge uses the mouse and beetle datasets and the trajectory challenge uses the mouse and fly datasets. All methods had access to the train and validation sets during development, and the test set was held out (only released after challenge completion). The results from the video challenge winners are fully in Section 5 of the main paper. A subset of the trajectory results are also included in the same section. We note that most of the trajectory-only evaluations are in Section F. A.1. Video-based methods A.1.1. BE IT + HAND -CRAFTING This method uses representations from both the video and trajectory datasets (Figure 3). The three components of the model are: 15MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior • A large vision transformer model (BEiT (Bao et al., 2021) large, patch-16, 512x512), pre-trained on ImageNet22k (Deng et al., 2009) at 224x224 pixels and fine-tuned on ImageNet1k at 512x512 pixels. We selected this model as it performs very well on ImageNet, and is one of the few such models that has been trained on images of size 512x512. • A SimCLR (Chen et al., 2020b) model, based on the baseline implementation ( https://www.aicrowd.com/ showcase/unsupervised-model-simclr-mouse-video-data ) but with three important modifications. First, the baseline augmentations were replaced with a version that used the keypoint annotations to constrain the random crops. Second, during training not all frames were sampled with equal probability. Instead, frames where the mice were mobile were sampled with a higher probability than frames where the mice were stationary. This weighting is intended to compensate for the fact that in some of the clips the mice are stationary (presumably sleeping) throughout the video, and as a result all the frames from those clips are highly similar. Third, the encoder was changed from ResNet-18 to ResNet-50, and only one frame was used as input instead of a sequence of frames. • A number of hand-crafted features, based on the keypoint annotations and used in previous works, such as (Segalin et al., 2020; Sun et al., 2021b). These features consisted of measurements internal to each mouse (e.g. the distance between the nose and the tail), measurements that involve each mouse and the cage (e.g. the distance between the mouse and the nearest corner), and measurements involving more than one mouse (e.g. the distance between two mice and the area of the triangle formed by the three mice). These three parts have complementary strengths, i.e., submissions based on each of them individually received high scores on different tasks. These features: 1024 (BEiT) + 2048 (SimCLR) + 214 (hand-crafted) = 3286 were concatenated for each frame, before PCA transforming them. We also found that we could improve results by reweighting both frame-wise and feature-wise before doing the PCA transform. Each frame was weighted by the measure of movement used in SimCLR training. Each of the three feature blocks was weighted by a numerical factor that was empirically determined. Finally, we had noticed in an earlier experiment that some tasks benefited from including not only the PCA-transformed BEiT embedding for each frame, but also some PCA features averaged over the whole sequence. We therefore replaced the last 8 features of the above PCA embedding with the first 8 features from the PCA-transformed BEiT embedding, averaged over the sequence. For the beetle submission, we simply computed the BEiT features from each frame and reduced them to the allowed 128 features with a standard PCA transformation. The code is available at https://github.com/IRLAB-Therapeutics/mabe_2022. For training, we used a batch size of 76, with an Adam optimizer and an initial learning rate of 3e-4, following a cosine annealing schedule. The image resolution used for the SimCLR model is 224x224. A.1.2. V ISION ENSEMBLE This model uses visual features only, extracted from pre-trained vision models (Figure 4). For both parts of the video challenge, we used an ensemble of pre-trained vision models by concatenating the output feature vectors of ResNet18 (He et al., 2016) and MobileNetV3-Small (Howard et al., 2019), for which the size of the feature vector is respectively 512 and 574. This results in a vector of size 1086 which is subsequently reduced to size 128 by PCA, which forms the final Figure 3.BEiT + Hand-crafting Model Overview . We learn a representation using both video and kepoint information, by (1) encoding video data through a pre- trained BEiT model, (2) learning visual representations using SimCLR, and (3) hand-crafted keypoint features. 16MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Figure 4.Vision Ensemble Model Overview. This model consists of fea- tures extracted from two pre-trained vision models, processed using a com- bination of temporal difference of the features as well as PCA. embedding for the beetle dataset. For the mouse dataset, we reduced the size from 1086 down to 64, again by PCA. Subsequently, we concatenated to this the difference of the feature vector from 40 frames in the past and 40 frames in the future, i.e. a window size of 80. The length of the two concatenated vectors is then 128 and this forms the final representation for the mouse dataset. A.1.3. M ULTIMODAL MOCO/SIMCLR To leverage data from different modalities, we design different self-supervised methods for each modality individually (Figure 5). We leverage three types of features, including visual features from video data, positional features and handcrafted features from keypoint data. Inspired by MoCo (He et al., 2020), we build a self-supervised framework containing a memory bank to learn the visual features from video data. We use two types of augmentation strategies. The first augmentation strategy includes RandomResizeCrop, RandomHorizontalFlip, and RandomVerticalFlip. The second augmentation strategy includes the temporal difference in addition. As shown in Figure 5, we sample two clips from a video and generate four views (two views for each clip). In the inference stage, we use the momentum updated encoder for a smooth result. To learn the positional information of agents, we propose a generative task on keypoints data. Inspired by the MLM (Masked Language Modeling) (Devlin et al., 2018) task in NLP, we propose the MPM (Masked Point Modeling) task, which is a frame-level task. The learning objective is to predict the masked keypoint coordinates based on observing unmasked keypoints. Giving a stream of agent-by-agent keypoint sequences, we randomly mask keypoint tokens at a ratio of by replacing keypoint tokens with mask tokens [MASK]. We then aggregate positional information from the rest frames with a vanilla Transformer (Vaswani et al., 2017) encoder. Then a shallow decoder (i.e. a two-layer MLP) is used to predict the masked keypoint coordinates. We compute the reconstruction loss between the decoder output and original keypoint coordinates. Following our preliminary exploration, we find the representation generalizes better with MSE regularization than L1. Besides visual features and positional features encoded by the deep networks, we also utilize handcrafted features from keypoints data, including distances, angle, and speed. Beetle Dataset. We use the MoCo-based method to extract visual features from the ant-beetle video data. We first crop the regions with agents based on the keypoints. We resize images to 224x224 for training and inference. We random sample 2 clips with 7 frames from each video and the temporal stride is 5 frames. We use SGD optimizer and learning rate of 0.0075. The batch size is 128 per GPU and the weight decay is 1e-4. We set K=65536 for the memory bank and T=0.2 for the NT-Xent loss. We train for total 100k steps. The visual backbone is the pretrained Resnet101 32x8d and the output dimension is 128. Mouse Dataset. Different from ant-beetles video data, we utilize SimCLR (Chen et al., 2020b) to extract visual features from mouse video data. We directly regard other samples in the batch as negative samples instead of constructing a negative 17MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Figure 5.Multimodal MoCo Model Overview . We build a MoCo-based self-supervised learning frame- work composed of a gradient updated encoder and a momentum updated encoder. This method is used for the beetle dataset, while a SimCLR-based method is used to extract features for the mouse dataset. samples queue and an extra momentum updated encoder. We resize images to 224x224 at training and 256x256 at inference. We random sample 3 clips with 7 frames from each video and the temporal stride is 12 frames. We use Adam optimizer and learning rate of 1e-4. The batch size is 64 per GPU and the weight decay is 1e-6. We train for total 100k steps. The visual backbone is ImageNet-1k pretrained Resnet50 (He et al., 2016) and the output dimension is 128. The keypoint coordinates of each agent are converted into a token by flatting and normalization, which results in 24-d input tokens. Then the tokens are fed into the main network. The encoder contains a 24-layer Transformer encoder and a projection head. Each Transformer layer has 768-d states and 12 masked attention heads. The one-layer projection head reduces the feature dimension from 768 to 128. At training, we sample 50 consecutive frames for each step. The learning rate and batch size per GPU are 1e-5 and 32 respectively. We use AdamW optimizer with betas of (0.9, 0.95) and weight decay of 0.1. We train for total 10k steps and warmup at the first 500 steps. We clip gradients with a norm threshold of 1.0. We do not adjust hyper-parameters. For handcrafted features, we calculate the following three types of features (59-d altogether): the mouse-mouse, mouses-wall, nose-tail, and nose-nose distances; the neck-base, nose-neck, and head-body angle of mice; the relative speed of the nose of the mice. We concatenate the above three types of features and reduce the dimension to 128 by PCA. The code is available at https://github.com/JiaHeng-DLUT/MABe2022. A.2. Trajectory-based methods Our benchmark models learn from sequences of trajectory data and maps this data to a behavioral representation, which can then be used for a range of downstream tasks. Let D be a set of N unlabelled trajectories. Each trajectory τ is a sequence of states τ = {(st)}T t=1 over time, which represents the data for a variable number of agents across a variable number of timestamps. The state si at timestamp i corresponds to the location and pose of the agents at that time, often represented by keypoints. Let z be the behavioral representation. In our framework, models can learn from trajectories across time, but needs to produce a representation at each frame to account for frame-level tasks. A.2.1. TVAE The Trajectory Variational Autoencoder (TV AE) is trained in a self-supervised way using trajectory reconstruction (Figure 6). To start, the keypoints of multiple agents is stacked to form the state at each timestamp si for mouse, whereas the flies are encoded individually to handle the variable number of flies, and missing flies have zero-filled coordinates. The group fly embedding is created by concatenating the individual fly embeddings at each frame. 18MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Posterior mean Posterior log variance Recurrent encoder Fully connected encoder Agent Trajectory Negative log  likelihood loss Stochastically  synthesized trajectory Recurrent  decoder KL divergence  loss Stochastic  embedding Fully connected  decoder Figure 6.TV AE Model Overview. The TV AE learns a representation from tra- jectory data based on reconstructing the input trajectory. The encoder and decoder are based on recurrent neural networks. Transformer Encoder Legend Original Keypoint  Features Augmented Feature  Vector Predicted Class  Labels Output Embedding Downsized Feature  Vector Figure 7.T-Perceiver Model Overview. We first compute high-dimensional hand-crafted features (“augmented feature vector”) from the input trajec- tory data, then a Perceiver (Jae- gle et al., 2021) model is trained to predict features as well as public labels from the learned representation. Learning Objective. The TV AE is a sequential generative model that uses trajectory reconstruction as the signal during training. Given previous states, the goal is to train the model to predict the next state. This architecture has previously been studied to learn trajectory representations in a variety of domains (Co-Reyes et al., 2018; Zhan et al., 2020; Sun et al., 2021b). We embed the input trajectory using an RNN encoder, qϕ, and an RNN decoder, pθ, to predict the next state. The TV AE loss is: Ltvae = Eqϕ \u0014 TX t=1 −log(pθ(st+1|st, z)) \u0015 + DKL(qϕ(z|τ)||pθ(z)). (1) We use the unit Gaussian as a prior distribution pθ(z) on z to regularize the learned embeddings. To predict behavioral representations at each frame, we form a sliding window of size 21, using 10 frames before and after the current frame. The encoder and decoder are based on Gated Recurrent Units with 256 hidden layers. The training uses the Adam optimizer (Kingma & Ba, 2014) with a batch size of 512 with learning rate 0.0002. The code is available at https://github.com/AndrewUlmer/MABe 2022 TV AE. 19MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Masked Multi-Head Attention  Decoder Feed Forward Network N× MSE Loss Mask Mask Mask Figure 8.T-GPT Model Overview. The T-GPT uses a Transformer architec- ture (Brown et al., 2020) to predict key- point coordinates in the next frame, given a representation learned from past frames. This prediction task is done both forwards and backwards in time. A.2.2. T-P ERCEIVER The T-Perceiver model (Figure 7) has two main steps: (1) we first create a richer representation by augmenting keypoint coordinates with additional hand-crafted features and (2) then we learn the temporal relationships and extracted the embedding from a Perceiver model (Jaegle et al., 2021). The model is trained to reconstruct frame-level features from masked input as well as predict any public tasks. Hand-crafted feature extraction. The first step transforms the original keypoint features into a high dimensional frame- level representation of the distances, angles and velocities between the keypoints. Feature extraction was performed algorithmically resulting in a 456 dimensional vector for the mouse dataset, and a 2112 dimensional vector for the fly dataset. The fly dataset has larger feature vectors as there were up to 11 individual flies in each frame, and when there were fewer flies, the vector was padded with zeros. Angles are encoded using (sin(θ), cos(θ)). All features are normalized to have a mean of 0 and a standard deviation of 1. Sequence modeling. The second step is to use an unsupervised sequence to sequence (seq2seq) model to combine these features across frames and map to the desired final embedding dimension. The features are first downsized to the final embedding size using a two layer fully-connected neural network with an intermediate layer size twice the size of the respective final embeddings using a 50% dropout rate and the ELU activation function. This sequence of downsized features are passed through a standard Perceiver model (Jaegle et al., 2021) with the number and dimension of latent vectors equal to embedding size and a sequence length of 512. For the fly dataset only, every second frame is dropped for computational reasons due to the high original frame rate. Learning Objective. During training, a variable number of up to 80% of frames were masked out and there was an additional linear layer to predict the original unmasked high dimensional features as well as labels from the public train split containing a subset of the hidden tasks. The model was trained to simultaneously optimize for two tasks: to minimize the mean square error on the frame-level features and to minimize the cross entropy loss of the label predictions. The first task was given a weight of 10 compared with the second task. The Adam optimizer (Kingma & Ba, 2014) was used for training with a learning rate of 0.001. The code is available at https://colab.research.google.com/drive/13 M6yzF1VQ4STuJsO1at-GWK2 TDGTNV?usp=sharing. A.2.3. T-GPT The T-GPT model is inspired by the NSP (Next Sentence Prediction) (Devlin et al., 2019) task from natural language processing. We instead propose the NFP (Next Frame Prediction) task, which is a frame-level task for predicting the keypoint coordinates in the next frame based on the observation of the past frames (Figure 8). 20MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Frame data 3x12x2 (mouse*keypoints*(x,y)) Past Future Normalize Pose PCA Mean PCA  Pose Hand Crafted  Features per  mouse Sort Each  Feature  Individually Pairwise Mouse  features Pointnet6x10 1x34 3x12x2 3x10 Permutation  Invariant  Embedding (1x59) 1x10 1x34 1x15 Figure 9.T-PointNet Model Overview. We combine hand- crafted features, PCA of pose keypoints, and PointNet (Qi et al., 2017) embeddings as a permutation-invariant representation of the agents at each frame. Giving a stream of frame-by-frame states τ = {(st)}T t=1, we first aggregate information from past frames with a vanilla Transformer encoder (Vaswani et al., 2017)f: zt = f(s1, s2, ..., st) (2) Then a shallow decoder h (i.e. a two-layer MLP) is used to predict the keypoint coordinates in the next frame: ˆst+1 = h(zt) (3) Learning Objective. We compute the reconstruction loss between the decoder output and original keypoint coordinates: L = MSE (ˆst+1, st+1) (4) Following our preliminary exploration, we find the representation generalizes better with MSE loss than L1. We build on the open source implementation of GPT (Brown et al., 2020). First, the keypoint coordinates in each frame are converted into a token by flatting and normalization, which results in 528-d input tokens. Then the tokens are fed into the encoder network, with a 24-layer Transformer encoder and a projection head. Each Transformer layer has 768 dimensional states and 12 masked attention heads. The one-layer projection head reduces the feature dimension from 768 to 256 for flies and 128 for mice. A two-layer decoder (Linear-LayerNorm-Tanh-Linear) is used to predict the coordinates in the next frame. In order to only attends to the left context, we use the upper triangular matrix attention mask in each self-attention layer when training. In the inference stage, these masks are removed to better aggregate contextual features from the past and future. At training, we use all the available data and sample 50 consecutive frames each iteration. We randomly flip the coordinates horizontally with a probability of 0.5. The learning rate and batch size are 1e-5 and 2 respectively, with the AdamW optimizer (Loshchilov & Hutter, 2017a). To make better use of the training data, we do the NFP task in a bidirectional way and the corresponding losses are averaged. The code is available at https://drive.google.com/drive/folders/1zcZ9lqtf0y4OCtfFdA1S7K3beLcXa-3e?usp=sharing A.2.4. T-P OINT NET We use PointNet (Qi et al., 2017) alongside hand-crafted features and PCA to extract permutation-invariant features from the keypoint data (Figure 9). As the embedding will be used to train a network for the hidden tasks, its important that embedding vector remains same even the order of the mice is switched. We note that this model is only applied to the mouse data, and not to the fly data, where some of the tasks are fly-dependent. The hand-crafted features used are similar to the ones from (Sun et al., 2021b), and 10 PCA components are computed for each mice and averaged to generate the group embeddings. Based on the goal of generating permutation-invariant 21MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Figure 10.T-BERT Model Overview. The trajectory of each agent is concatenated and encoded using BERT (Devlin et al., 2019), trained on masked modeling, predicting hand- crafted features, contrastive loss, and predicting publicly available train tasks. embeddings, we select a PointNet based architecture (Qi et al., 2017), which has been popular for learning patterns in unordered point cloud datasets. It fundamentally relies on commutative operations like sum, average, max to create permutation invariant features. Learning Objective. Each “point” fed into PointNet represents one pair of agents, and the coordinates are hand crafted features between each pair such as distance, angle, and speed (each 10 dimensions). PointNet is trained using a cosine similarity loss, where nearby frames in a sequence are treated as positives whereas a random frame chosen from a random sequence is chosen as negatives. The advantage of this network is that the embedding remain same regardless of the input order of the agents. The final combined embedding size is 69 dimensions. We use the vanilla PointNet network as described by authors in (Qi et al., 2017) with a reduced set of parameters and filters. The original network is designed for point clouds in order of 1000 and in contrast, in this application there are only 6 animal pairs corresponding to 6 points, thus we reduce the network capacity to prevent overfitting. This model is trained with an Adam optimizer (Kingma & Ba, 2014) with learning rate 0.005 and batch size 512. The code is available at https://github.com/Param-Uttarwar/mabe 2022 . A.2.5. T-BERT We extend BERT (Devlin et al., 2019) to learn separate embeddings for each agent in the enclosure which are then concatenated for the group embedding (Figure 10). We train the model using three main tasks: 1) Masked modelling, 2) hand-crafted feature predictions similar to that of (Sun et al., 2021b), and 3) contrastive learning. This model is only applied to the mouse dataset. We sample a window of 80 frames, encoding the keypoints with a linear projection layer. The sequence of keypoints for each agent is separated by a special learned embedding, similar to a [SEP] token (Devlin et al., 2019). We use three different kinds of features: 1) Individual-agent features, which are agent specific. 2) Inter-agent, which are features between each pair of agents. Note that these pairings can be directional. 3) Group features which apply to the entire group. Each feature type is encoded and their embeddings are added. In the case of inter-agent features, we encode and add each pair. Masked Modeling. We mask 70% of the input keypoints and features. Because of the high sampling frequency of the dataset, masked modelling may be trivial through interpolation of nearby frames. We therefore mask spans of the input, following the same masking scheme as SpanBERT (Joshi et al., 2020). We set the minimum and maximum span length to 3 and 20 respectively and sample lengths according to l ∼ Geo(p = 0.2). The input subsequence is encoded with a stack of 12 transformer self-attention blocks with hidden size 912, followed by a projection onto a 42 dimensional space for the output embeddings. We apply dropout to these and predict the normalized masked keypoints. Feature Predictions We predict individual-agent features directly from each output embedding. Inter-agent features are predicted by taking the output embeddings for the agents in the pair and subtracting them, then regressing the features from 22MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior this pair embedding. We obtain the final representation for the group by concatenating the embeddings for each agent. We use the group embedding to predict group features and for the final submission. Group embeddings are pooled across frames using a weighted average to get a single embedding for the entire input sequence. This pooled embedding is then used for the contrastive task. Contrastive Task We perform a contrastive learning task by taking two randomly subsequences from the same 1 minute clip as the positive pair. Negative pairs are created by pairing with other sequences within the batch. We encode the pooled sequence representation using a 2 layer MLP onto a 42 dimensional space. We take the NT-Xent loss (Chen et al., 2020b) with τ = 0.1. Learning Objectives. The task losses are weighted: L = Lm + 0.8Lx + 0.8Ly + 0.4Lz + 0.05Lc + 0.1Lcl (5) Where m is masked modelling, x is the individual agent feature prediction task, y is the inter-agent feature prediction task, z is the group feature prediction task, c is the chases task (public task on mouse dataset) and cl is the contrastive task. 53 individual agent features were computed for each agent, with 13 inter-agent features for pairs, and 1 group feature for all three mice. Features concerning distances, velocities and accelerations are normalised by mouse length. Angles are encoded (sin(θ), cos(θ)). We apply rotation, reflection and adding gaussian noise to the keypoints (Sun et al., 2021b), each are applied with probability p = 0.5. To create frame-level embeddings for a 1 minute sequence, we encode overlapping 80 frame windows of the input using a stride of 40 frames. An exhaustive hyperparameter search was not possible due to computational constraints, so most parameters were not tuned. We tested input lengths of 60, 80 and 100 frames and found that 80 was optimal. We split the dataset into training and validation splits, with 95% and 5% respectively. We train the model for160 epochs with a batch size of 16. We used AdamW (Loshchilov & Hutter, 2017a) with a learning rate of 0.00003 and a linear schedule. The model with the lowest validation loss is chosen. The code is available at https://github.com/edhayes1/MABe B. Datasheets B.1. Mouse Datasheet Motivation For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. Automated animal pose estimation has become an increasingly popular tool in the neuroscience community, fueled by the publication of several easy-to-train animal pose estimation systems. Building on these pose estimation tools, pose-based approaches to supervised or unsupervised analysis of animal behavior are currently an area of active research. New computational approaches for automated behavior analysis are probing the detailed temporal structure of animal behavior, its relationship to the brain, and how both brain and behavior are altered in conditions such as Parkinson’s, PTSD, Alzheimer’s, and autism spectrum disorders. Due to a lack of publicly available animal behavior datasets, most new behavior analysis tools are evaluated on their own in-house data. There are no established community standards by which behavior analysis tools are evaluated, and it is unclear how well available software can be expected to perform in new conditions, particularly in cases where training data is limited. Labs looking to incorporate these tools in their experimental pipelines therefore often struggle to evaluate available analysis options, and can waste significant effort training and testing multiple systems without knowing what results to expect. The Multi-Agent Behavior 2022 (MABe22) dataset is a new set of animal tracking, pose, video, and behavior datasets, intended to serve as a benchmark dataset for evaluation of unsupervised/self-supervised behavior representation learning and discovery methods. This datasheet is specific to the Mouse Triplets dataset, which consists of snippets of video and trajectory data from triplets of interacting mice. Accompanying the data is a collection of 8 ”hidden labels”: for each video frame of the dataset, we provide annotations of animal strain, time of day, light cycle, and a set of behaviors. These hidden labels can be used to evaluate the quality of learned representations of animal behavior, by asking how well the information they represent can be decoded from a given representation. 23MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The MABe22 Mouse Triplets dataset was collected and analyzed in the laboratory of Vivek Kumar at Jackson Labs (JAX), and was assembled by Ann Kennedy at Northwestern University. Mice were bred and videos of interacting mice were collected by Tom Sproule at JAX. The video dataset was tracked by Brian Geuther and Keith Sheppard at JAX, with pose estimation performed using a modified version of HRnet described in (Sheppard et al., 2022). Tracking and video data were screened for tracking quality and segmented into one-minute ”sequences” by Ann Kennedy. Sequences were manually annotated for four social behaviors of interest by Markus Marks. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. Acquisition of behavioral data was supported by NIH grants DA041668 (NIDA), DA048034 (NIDA), and Simons Foundation SFARI Director’s Award (to VK). Curation of data task design was funded by NIMH award #R00MH117264 (to AK) and NSERC Award #PGSD3-532647-2019 (to JJS). Any other comments? None. Composition What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. The core element of this dataset, called a sequence, consists of raw video, tracked postures, sequence-level experimental conditions, and hand-scored actions of three mice interacting in a 52 cm x 52 cm arena, filmed from above at 30 Hz. All three mice are adult males from the same strain, either C57Bl/6J or BTBR. Postures of animals are estimated in terms of a set of twelve anatomically defined ”keypoints” that capture the detailed two-dimensional pose of the animal. Because the three mice are not easily distinguished, temporal filtering methods are used to track the identity of animals across frames. Because both of these processing steps are automated, some errors in pose estimation or swaps of mouse identity do occur in the dataset. Accompanying each sequence are frame-by-frame annotations for 8 ”hidden tasks” capturing experimental conditions, animal background, and animal behavior. The 8 hidden tasks for this dataset include four ”sequence-level” tasks where annotation values are the same for all frames in a one-minute sequence, and nine ”frame-level” tasks where annotation values vary from frame to frame. Descriptions of each task are provided in Table 12; all behaviors are defined between any given pair of animals. The core element of a sequence is called a frame; this refers to the posture of the three animals on a particular frame of video, as well as annotations for the 8 hidden tasks. How many instances are there in total (of each type, if appropriate)? This dataset is composed of 2614 one-minute-long sequences filmed at 30 Hz. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). The dataset is derived from a larger experiment, in which three mice were allowed to freely interact in an open arena for a period of four days. To generate the trajectories used for this dataset, we randomly sampled up to five one-minute intervals from each recorded hour of approximately 12 such four-day experiments. In initial sampling, we observed that during the lights-on phase of the light/dark cycle the mice spent the majority of the time huddled together sleeping. As this does not generate particularly interesting behavioral data, we randomly discarded 80% of sampled one-minute intervals in which no substantial movement of the animals occurred, and replaced these with substitute samples drawn from the same one-hour time period. If after five attempts we could not randomly draw a replacement sample containing movement, we omitted the trajectory from the dataset. As a result, the dataset contains a higher proportion of trajectories with movement than is present in the source videos, and a slightly lower proportion of trajectories sampled from the light portion of the light/dark cycle. What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or features? In either case, please provide a description. 24MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Task Name Type Values Description Experiment day Sequence 1-4 Mice were filmed interacting for four days after introduc- tion to a new arena; task is to determine which day a sequence comes from. Time of day Sequence 0-1440 Mice show circadian changes in their level of activity; task is to infer time of day from behavior. Strain Sequence 0 or 1 Mice are from either C57Bl/6J or BTBR genetic back- ground. Strain field is 1 for BTBR and 0 for C57Bl/6J. Lights Sequence 0 or 1 Mice are more active when the lights are off, which occurs between 6am and 6pm; task is to infer light condition from behavior. Chase Frame 0 or 1 A pair of mice moving quickly with one mouse following close behind the other. Huddle Frame 0 or 1 Bodies of the mice are in close contact and the animals are stationary for at least several seconds; can occur between either pairs or triplets of animals. Face sniffing Frame 0 or 1 A close-investigation behavior in which the nose of one mouse is in close contact with the nose or face of another mouse. Anogenital sniffing Frame 0 or 1 A close-investigation behavior in which one mouse is investigating the anogenital area of another, typically with its nose near the base of the tail or pushed underneath the hindquarters of the other animal. Table 6.Format of hidden tasks for mouse dataset. Each sequence has three elements. 1) Keypoints are the locations of twelve body parts on each mouse: the nose tip, left and right ears, base of neck, body centroid, base, middle, and tip of tail, and the four paws. Keypoints are estimated using a modified version of HRnet documented in (Sheppard et al., 2022). 2) Annotations are sequence-level or frame-level labels of experimental conditions or animal’s actions. Definitions of these annotations are provided in Table 12. The behavior labels were generated using a series of short scripts based on features of detected animal poses; it is therefore possible that some mis-identification of behaviors occurs. Note that this dataset does not include the original raw videos from which pose estimates were produced. This is because the objective of releasing this dataset was to determine the accuracy with which animal behavior could be detected using tracked keypoints alone. Is there a label or target associated with each instance? If so, please provide a description. Yes: each annotation (as described above) is provided for every frame in the dataset. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. There is no missing data. Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)? If so, please describe how these relationships are made explicit. Each instance (sequence) is to be treated as an independent observation with no relationship to other instances in the dataset. Although the identities of the interacting animals are the same in some sequences, this information is not tracked in the dataset. Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them. The dataset includes a recommended train/test split which was used for the Multi-Agent Behavior Challenge. Data was randomly split into training, test, and private-test sets (where the private test set was withheld from challenge evaluation until the end of the competition period, to avoid overfitting.) 25MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. Pose keypoints in this dataset are produced using automated pose estimation software. The dataset was screened to remove sequences with poor pose estimation, detected as large jumps in the detected location of an animal, however some errors in pose estimation, missing keypoints, and noise in keypoint placement still occur. These are most common on frames when the two animals are in close contact or moving very quickly. Frame-by-frame annotations of behavior were generated using a series of scripts that were manually tuned by a human expert. Pose estimation errors can contribute to missed bouts or false positives for behaviors in these annotations. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. The dataset is self-contained. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description. No. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. No. Does the dataset relate to people? If not, you may skip the remaining questions in this section. No. Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. n/a Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how. n/a Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. n/a Any other comments? None. Collection Process How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. 26MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Sequences in the dataset are derived from video of triplets of socially interacting mice in an open arena. Video data was processed to extract pose estimates and track identity of the animals, and to generate automated annotations of several behaviors of interest, included in the hidden labels in this dataset. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Behavioral data was collected in the JAX Animal Behavior System (Beane et al., 2022). Videos were recorded using Basler acA1300- 75gm camera with Tamron 4-12mm lens and 800nm longpass filter, at a framerate of 30Hz and camera resolution of 800 x 800 pixels. The camera was mounted 105+/-5 cm above the floor of an open field measuring 52cm x 52cm; a grate located at the northern wall of the arena provides animals access to food and water. Animals were introduced to the arena one by one over the first ten minutes of recording, and were recorded continuously for four days. Pose estimation was performed using a modified version of HRnet documented in (Sheppard et al., 2022). Manual annotation of animal behavior was performed by a trained human expert using the VIA video annotator (Dutta & Zisserman, 2019). If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Repeated from a previous section: to generate the trajectories used for this dataset, we randomly sampled up to five one-minute intervals from each recorded hour of approximately 12 such four-day experiments. In initial sampling, we observed that during the lights-on phase of the light/dark cycle the mice spent the majority of the time huddled together sleeping. As this does not generate particularly interesting behavioral data, we randomly discarded 80% of sampled one-minute intervals in which no substantial movement of the animals occurred, and replaced these with substitute samples drawn from the same one-hour time period. If after five attempts we could not randomly draw a replacement sample containing movement, we omitted the trajectory from the dataset. As a result, the dataset contains a higher proportion of trajectories with movement than is present in the source videos, and a slightly lower proportion of trajectories sampled from the light portion of the light/dark cycle. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? Behavioral data collection was performed by graduate student, postdoc, and technician members of the Kumar lab at Jackson Laboratories, as a part of another ongoing research project studying animal gait and behavior. (No videos or annotations were explicitly generated for this dataset release.) Lab members are full-time employees of Jackson Labs, and their compensation was not dependent on their participation in this project. Manual annotation of animal behavior was performed by Markus Marks, who is a full-time employee of Caltech and whose compensation was also not dependent on participation in this project. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Source experiments associated with this dataset were performed in 2019, with pose estimation performed in 2019-2020 and manual annotation performed in Sept-Nov 2022. This dataset was assembled from December 2022 - March 2023. Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. All experiments included here were performed in accordance with NIH guidelines and approved by the Institutional Animal Care and Use Committee (IACUC) and Institutional Biosafety Committee at Jackson Labs. Review of experimental design by the IACUC follows the steps outlined in the NIH-published Guide for the Care and Use of Laboratory Animals. All individuals performing behavioral experiments underwent animal safety training prior to data collection. Animals were maintained under close veterinary supervision. Does the dataset relate to people? If not, you may skip the remaining questions in this section. No. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? n/a Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. 27MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior n/a Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. n/a If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). n/a Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. n/a Any other comments? None. Preprocessing/cleaning/labeling Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. No preprocessing was performed on the sequence data released in this dataset. Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data. n/a Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point. n/a Any other comments? None. Uses Has the dataset been used for any tasks already? If so, please provide a description. Yes: this dataset was released to accompany the 2022 Multi-Agent Behavior (MABe) Challenge, posted here. This competition was aimed at generating learned representations of animals’ actions using unsupervised or self-supervised techniques. Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. Papers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset What (other) tasks could the dataset be used for? While this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised learning tasks. 28MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms? Occasional errors and identity swaps during pose estimation may impact future use of the dataset for some purposes. Are there tasks for which the dataset should not be used? If so, please provide a description. None. Any other comments? None. Distribution Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. Yes - the full dataset will be made publicly available for download by all interested parties by July 1st, 2023. How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the dataset have a digital object identifier (DOI)? The dataset is available on the Caltech public data repository at https://data.caltech.edu/records/rdsa8-rde65. A previous version, containing only the trajectory data, is available at https://data.caltech.edu/records/20186. The data will be retained indefinitely and available for download by all third parties. The data.caltech.edu posting has accompanying DOI https://doi.org/10.22002/rdsa8-rde65. The dataset as used for the MABe Challenge (lacking hidden task labels) is available for download on the AIcrowd page, located at (https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-mouse-triplets-video-data). When will the dataset be distributed? Yes - the full dataset will be made publicly available for download by all interested parties by July 1st, 2023. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. The MABe22 dataset is distributed under the CreativeCommons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA). The terms of this license may be found at https://creativecommons.org/licenses/by-nc-sa/2.0/legalcode. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. There are no third party restrictions on the data. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. No export controls or regulatory restrictions apply. Any other comments? None. Maintenance 29MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Who will be supporting/hosting/maintaining the dataset? The dataset is hosted on the Caltech Research Data Repository at data.caltech.edu. Dataset hosting is maintained by the library of the California Institute of Technology. Long-term support for users of the dataset is provided by Jennifer J. Sun and by the laboratory of Ann Kennedy. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? The managers of the dataset (JJS and AK) can be contacted at mabe.workshop@gmail.com, or AK can be contacted at ann.kennedy@northwestern.edu and JJS can be contacted at jjsun@caltech.edu. Is there an erratum? If so, please provide a link or other access point. No. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)? Users of the dataset have the option to subscribe to a mailing list to receive updates regarding corrections or extensions of the MABe22 dataset. Mailing list sign-up can be found on the MABe22 webpage at https://sites.google.com/view/computational-behavior/our- datasets/mabe2022-dataset. Updates to correct errors in the dataset will be made promptly, and announced via update messages posted to the MABe22 website and data.caltech.edu page. Updates that extend the scope of the dataset, such as additional hidden tasks, or improved pose estimation, will be released as new named instantiations on at most a yearly basis. Previous versions of the dataset will remain online, but obsolescence notes will be sent out to the MABe22 mailing list. In updates, dataset version will be indicated by the year in the dataset name (here 22). Dataset updates may accompany new instantiations of the MABe Challenge. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced. N/a (no human data.) Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users. Yes, the dataset will be permanently available on the Caltech Research Data Repository (data.caltech.edu), which is managed by the Caltech Library. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description. Extensions to the dataset will take place through at-most-yearly updates. We welcome community contributions of behavioral data, novel tracking methods, and novel hidden tasks; these may be submitted by contacting the authors or emailing mabe.workshop@gmail.com. All community contributions will be reviewed by the managers of the dataset for quality of tracking and annotation data. Community contributions will not be accepted without a data maintenance plan (similar to this document), to ensure support for future users of the dataset. Any other comments? If you enjoyed this dataset and would like to contribute other multi-agent behavioral data for future versions of the dataset or MABe Challenge, contact us at mabe.workshop@gmail.com! B.2. Fly Datasheet Motivation 30MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. The prospect of discovering structure previously unknown to humans from large datasets has tremendous potential, particularly for science. However, progress has been inhibited by a lack of common datasets and quantitative evaluation criteria for assessing and comparing different algorithms. In the field of video-based behavior analysis, there has been a lot of progress in tools for tracking the pose of people and animals. To make use of these methods in biology, we now need computational methods to probe the temporal structure in these still large time-series datasets, and learn representations amenable to comparison and further study. The MABe22 dataset is a new animal behavior dataset, intended to a) serve as a benchmark dataset for comparison of unsupervised or self-supervised behavior analysis tools, and establish community standards for evaluation of unsupervised techniques, b) highlight critical challenges in computational behavior analysis, particularly pertaining to unsupervised representation learning, and c) foster interaction between behavioral biologists and the greater machine learning community. This datasheet is specific to the Fly Group dataset, which consists of tracking data for a group of 8 to 11 fruit flies with 50 “hidden labels” for evaluating the quality of the learned representation. Also see MABe22 mouse triplet data sheet (Section B.1) for more details. Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The MABe22 fly dataset was created as a collaborative effort between Kristin Branson, Alice Robie, and Catherine Schretter at HHMI Janelia Research Campus within the labs of Kristin Branson and Gerry Rubin. Fly lines were generated by Gerry Rubin with the help of the Janelia Fly Core, PTR, and Fly Light project teams. Fly crosses and offspring were set up and collected by Alice Robie and Catherine Schretter, the behavior rig was developed by Alice Robie and Kristin Branson, and video were recorded by Alice Robie and Catherine Schretter, with help from Janelia Shared Resources. Analysis was done by Kristin Branson, Alice Robie, and Catherine Schretter, with help from Adam Taylor. The dataset tasks were designed by Kristin Branson, Alice Robie, and Catherine Schretter. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. Acquisition of behavioral data was funded by the Howard Hughes Medical Institute. Any other comments? None. Composition What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. The core element of this dataset, called a sequence, captures the tracked postures of ≈ 10 flies over 30s (4,500 frames) on a 5-cm-diameter domed plate filmed from above at 150Hz. The core element of a sequence is called a frame; this refers to the posture of all animals on a particular frame of video, as binary categorization for each of the 50 tasks. Tasks were based on the genotype, rearing, mutation, and environmental conditions of the flies. Flies from the following genotypes were assayed: dTrpA1 x pBDPGAL4U (Control) (Robie et al., 2017), dTrpA1 x R71G01 (R71G01) (Robie et al., 2017), dTrpA1 x R65F12 (R65F12) (Robie et al., 2017), 20xCsChrimson x SS36551 (aIPg)(Schretter et al., 2020), NorpA,20xCsChrimson x NorpA;SS36564 (Blind aIPg), 20x CsChrimson x SS56987 (pC1d)(Schretter et al., 2020), 20x CsChrimson x BPp65AD-x-BPZpGal4DBD (Control 2)(Schretter et al., 2020), NorpA,20xCsChrimson x NorpA;BPp65AD-x-BPZpGal4DBD (Blind control). Neural populations in CsChrimson flies were activated by periods of red light illumination from an LED panel below the flies. Neural populations in dTrpA1 flies were activated by performing the experiments at the permissive temperature for TrpA. In addition, we manually annotated 6 social behaviors sparsely across the dataset. How many instances are there in total (of each type, if appropriate)? Instances for each dataset are shown in Table 7, divided into user train, evaluator train, test 1, and test 2 setss. Number of instances is reported as frames. As frames within a sequence are temporally contiguous and sampled at 150Hz, they are not statistically independent observations. 31MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Category 1 Category 0 Task User train Eval train Test 1 Test 2 User train Eval train Test 1 Test 2 Female vs male 13,808,901 7,696,105 5,155,335 6,452,311 4,470,088 1,744,888 1,338,165 1,562,188 Control 1 1,863,000 729,000 364,491 405,000 11,657,257 5,642,878 4,173,622 5,085,126 Control 1 sex separated 405,000 364,491 405,000 364,500 13,115,257 6,007,387 4,133,113 5,125,626 Control 2 726,798 516,548 287,012 283,151 12,793,459 5,855,330 4,251,101 5,206,975 71G01 2,668,497 769,500 364,509 405,011 10,851,760 5,602,378 4,173,604 5,085,115 Male R71G01 female control 405,008 9 405,000 405,000 13,115,249 6,371,869 4,133,113 5,085,126 R65F12 1,853,994 1,003,505 810,000 764,998 11,666,263 5,368,373 3,728,113 4,725,128 R91B01 1,944,000 729,000 364,500 810,000 11,576,257 5,642,878 4,173,613 4,680,126 Blind control 1,011,001 543,761 236,961 468,234 12,509,256 5,828,117 4,301,152 5,021,892 aIPg 1,166,857 418,714 262,500 520,350 12,353,400 5,953,164 4,275,613 4,969,776 pC1d 520,770 516,990 518,450 518,890 12,999,487 5,854,888 4,019,663 4,971,236 Blind aIPg 955,332 780,360 519,690 544,992 12,564,925 5,591,518 4,018,423 4,945,134 Blind control on vs off 1,011,001 543,761 236,961 468,234 1,094,999 590,239 249,039 503,766 Blind control strong vs off 505,572 272,425 118,314 232,335 1,094,999 590,239 249,039 503,766 Blind control weak vs off 505,429 271,336 118,647 235,899 1,094,999 590,239 249,039 503,766 Blind control strong vs weak 505,572 272,425 118,314 232,335 505,429 271,336 118,647 235,899 Blind control last vs first 169,813 88,736 38,970 78,507 168,405 91,702 39,771 78,048 Control 2 on vs off 726,798 516,548 287,012 283,151 785,202 563,452 306,988 310,849 Control 2 strong vs off 361,015 258,143 143,836 141,922 785,202 563,452 306,988 310,849 Control 2 weak vs off 365,783 258,405 143,176 141,229 785,202 563,452 306,988 310,849 Control 2 strong vs weak 361,015 258,143 143,836 141,922 365,783 258,405 143,176 141,229 Control 2 last vs first 121,560 85,523 48,609 48,081 120,672 86,526 46,849 46,761 Blind aIPg on vs off 955,332 780,360 519,690 544,992 1,042,668 839,630 560,310 589,008 Blind aIPg strong vs off 477,531 389,800 260,930 271,073 1,042,668 839,630 560,310 589,008 Blind aIPg weak vs off 477,801 390,560 258,760 273,919 1,042,668 839,630 560,310 589,008 Blind aIPg strong vs weak 477,531 389,800 260,930 271,073 477,801 390,560 258,760 273,919 Blind aIPg last vs first 159,555 130,120 85,810 90,343 158,332 129,920 87,240 89,876 aIPg on vs off 1,166,857 418,714 262,500 520,350 1,276,633 512,784 277,500 559,650 aIPg strong vs off 598,592 210,374 131,340 259,890 1,276,633 512,784 277,500 559,650 aIPg weak vs off 568,265 208,340 131,160 260,460 1,276,633 512,784 277,500 559,650 aIPg strong vs weak 598,592 210,374 131,340 259,890 568,265 208,340 131,160 260,460 aIPg last vs first 199,900 77,067 44,210 86,860 198,901 76,662 44,120 86,150 pC1 on vs off 520,770 516,990 518,450 518,890 559,230 563,010 561,550 561,100 pC1d strong vs off 258,760 258,370 260,100 257,520 559,230 563,010 561,550 561,100 pC1d weak vs off 262,010 258,620 258,350 261,370 559,230 563,010 561,550 561,100 pC1d strong vs weak 258,760 258,370 260,100 257,520 262,010 258,620 258,350 261,370 pC1d last vs first 86,520 86,760 86,110 86,780 85,320 85,660 87,030 86,490 Any courtship 4,927,499 1,773,014 1,579,509 1,575,009 8,592,758 4,598,864 2,958,604 3,915,117 Any control 4,005,799 2,153,800 1,293,464 1,520,885 9,514,458 4,218,078 3,244,649 3,969,241 Any blind 1,966,333 1,324,121 756,651 1,013,226 11,553,924 5,047,757 3,781,462 4,476,900 Any aIPg 2,122,189 1,199,074 782,190 1,065,342 11,398,068 5,172,804 3,755,923 4,424,784 Any aggression 2,642,959 1,716,064 1,300,640 1,584,232 10,877,298 4,655,814 3,237,473 3,905,894 Any R71G01 3,073,505 769,509 769,509 810,011 10,446,752 5,602,369 3,768,604 4,680,115 Any sex-separated 810,008 364,500 810,000 769,500 12,710,249 6,007,378 3,728,113 4,720,626 Aggression manual annotation 610 972 1,279 890 480 1,092 1,014 1,487 Chase manual annotation 1,496 15,351 5,611 20,810 2,218 51,938 31,232 34,382 Courtship manual annotation 591 743 273 108 3,388 2,979 2,465 1,728 High fence manual annotation 188 157 106 158 751 584 570 629 Wing ext. manual annotation 0 1,594 1,524 3,130 0 13,396 11,728 15,104 Wing flick manual annotation 230 149 95 176 1,740 1,404 840 1,469 Table 7.Number of frames in each split set for each task and category. 32MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Category 1 Category 0 Task User train Eval train Test 1 Test 2 User train Eval train Test 1 Test 2 Female vs male 426 217 147 179 221 91 67 76 Control 1 50 20 9 10 373 193 136 166 Control 1 sex separated 10 9 10 10 408 202 135 165 Control 2 33 22 11 11 385 189 133 164 71G01 66 20 11 11 359 193 135 166 Male R71G01 female control 11 1 10 10 407 211 134 166 R65F12 45 25 20 18 376 187 126 158 R91B01 49 19 10 20 374 192 134 156 Blind control 44 22 11 22 374 189 133 153 aIPg 54 21 11 22 364 190 133 153 pC1d 22 22 22 22 396 189 122 153 Blind aIPg 44 33 22 22 374 178 122 153 Blind control on vs off 44 22 11 22 52 26 13 26 Blind control strong vs off 24 12 6 12 52 26 13 26 Blind control weak vs off 20 10 5 10 52 26 13 26 Blind control strong vs weak 24 12 6 12 20 10 5 10 Blind control last vs first 8 4 2 4 8 4 2 4 Control 2 on vs off 33 22 11 11 38 26 13 13 Control 2 strong vs off 18 12 6 6 38 26 13 13 Control 2 weak vs off 15 10 5 5 38 26 13 13 Control 2 strong vs weak 18 12 6 6 15 10 5 5 Control 2 last vs first 6 4 2 2 6 4 2 2 Blind aIPg on vs off 44 33 22 22 52 38 26 26 Blind aIPg strong vs off 24 18 12 12 52 38 26 26 Blind aIPg weak vs off 20 15 10 10 52 38 26 26 Blind aIPg strong vs weak 24 18 12 12 20 15 10 10 Blind aIPg last vs first 8 6 4 4 8 6 4 4 aIPg on vs off 54 21 11 22 62 25 13 26 aIPg strong vs off 29 11 6 12 62 25 13 26 aIPg weak vs off 25 10 5 10 62 25 13 26 aIPg strong vs weak 29 11 6 12 25 10 5 10 aIPg last vs first 10 4 2 4 10 4 2 4 pC1 on vs off 22 22 22 22 26 26 26 24 pC1d strong vs off 12 12 12 12 26 26 26 24 pC1d weak vs off 10 10 10 10 26 26 26 24 pC1d strong vs weak 12 12 12 12 10 10 10 10 pC1d last vs first 4 4 4 4 4 4 4 4 Any courtship 120 45 40 37 304 168 106 138 Any control 137 73 41 53 286 140 105 123 Any blind 88 55 33 44 330 156 111 131 Any aIPg 98 54 33 44 320 157 111 131 Any aggression 120 76 55 66 298 135 89 109 Any R71G01 77 20 21 20 348 192 125 156 Any sex-separated 21 10 20 20 397 202 125 156 Aggression manual annotation 11 16 15 17 10 20 17 30 Chase manual annotation 2 23 11 23 4 76 63 65 Courtship manual annotation 5 6 6 2 38 32 34 22 High fence manual annotation 12 17 13 17 23 27 16 18 Wing ext. manual annotation 0 4 8 8 0 52 49 54 Wing flick manual annotation 28 19 16 28 52 40 24 50 Table 8.Number of sequences in each split set for each task and category. 33MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). We used all videos from chosen genotypes and conditions containing at least 9 flies. Frames for manual annotation of behavior were chosen using JAABA’s interactive system (Kabra et al., 2013) to help find instances of rare behaviors. When cutting a video into sequences, we chose segments to avoid obvious identity tracking errors (trajectory births or deaths). We left gaps of a randomly chosen length between .5 and 2s (75 and 300 frames) between sequences. What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or features? In either case, please provide a description. Each sequence has three elements. 1) Tracking featuresconsist of, for each of the ≈ 10 flies, the locations of 19 body parts (left wing tip, right wing tip, antennae midpoint, right eye, left eye, left front of thorax, right front of thorax, base of thorax, tip of abdomen, right middle femur base, right middle femur-tibia joint, left middle femur-base, left middle femur-tibia joint, right front leg tip, right middle leg tip, right rear leg tip, left front leg tip, left middle leg tip, left rear leg tip), information about an ellipse fit to the fly body (Fit ellipse center, orientation, major and minor axis length), and information about the segmented animal (body and foreground area, image contrast). Tracking features are estimated using the Animal Part Tracker (APT) and the FlyTracker. Videos have between 9 and 11 flies. All data are stored as matrices with space for 11 flies, with nan values if there are < 11 flies. 2) Task categoriesare frame- and fly-wise binary categorizations for each of the 50 tasks we defined, and will have values 1, 0, or nan, with nan indicating no data (the task is irrelevant or ill-defined for this frame and fly, or this frame and fly was not manually annotated). For some tasks, all flies in the same frame will have the same value. For some tasks, all frames will have the same value for the entire sequence, or for long periods of contiguous time. Is there a label or target associated with each instance? If so, please provide a description. The annotation field for a given sequence consists of frame- and fly-wise categorizations for each of the 50 tasks. For fly-frames for which the task is irrelevant or ill-defined, or no manual annotation was made, this label will be missing (indicated by nan). In the MABe22 challenge, these task annotations were kept secret, and used for evaluation purposes, not for training. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. As described above, all data are stored as matrices with space for 11 flies, with nan values if there are < 11 flies. Annotations will be nan if the task is irrelevant or ill-defined for this frame and fly, or this frame and fly was not manually annotated. Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)? If so, please describe how these relationships are made explicit. Each instance (sequence) is to be treated as an independent observation. Some sequence come from the same groups of flies in the same video. Each sequence is at least 0.5s (75 frames) from another sequence. Frames within a sequence are temporally contiguous, and highly correlated. Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them. The dataset includes a recommended split into User train (for unsupervised representation learning), Evaluator train (for training evaluator classifier), Test 1 (for validating the classifier), and Test 2 (for final evaluation score) sets. Each set containing distinct videos and flies. The splits were designed to provide a roughly consistent, small amount of training data for each task. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. Tracking in this dataset are produced using automated tracking software (FlyTracker and APT). In addition, manual annotations of animal behavior are inherently subjective, and individual annotators show some variability in the precise frame-by-frame labeling of behavior sequences. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the 34MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. The dataset is self-contained. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description. No. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. No such material; dataset contains only trajectories (no video or images) and text labels pertaining to fly social behaviors. Does the dataset relate to people? If not, you may skip the remaining questions in this section. No. Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. n/a Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how. n/a Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. n/a Any other comments? None. Collection Process How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. See above for details on collection process. All data pertains to groups of interacting flies in carefully controlled environments. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Details of fly genotypes and rearing are above. Flies were recorded in our custom developed behavior rig, which consists of a custom LED panel for back-illumination for recording in NIR and timed optogenetic activation in red, a custom 5-cm-diameter domed circular dish designed to reduce interactions with the arena wall and ceiling, a visual surround to isolate the flies, and a camera with an NIR-pass filter (FLIR Flea3) recording at 1024x1024 at 150Hz. We used data capture software based on the FlyBowlDataCapture system (Robie et al., 2017) and the Basic Image Acquisition System (BIAS, IORodeo). As described above, manual annotations were made using JAABA. If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? 35MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior As described above, we included videos with at least 9 flies in them. When cutting a video into sequences, we chose segments to avoid obvious identity tracking errors (trajectory births or deaths). We left gaps of a randomly chosen length between .5 and 2s (75 and 300 frames) between sequences. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? Full-time employees of Janelia’s Shared Resources teams (Fly Core, Fly Light, Media, and Project Technical Resources) were involved in producing and maintaining flies. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Videos associated with this dataset were collected between December 2020 and September 2021. Tracking and annotation was performed in October 2021 - February 2022. Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. No. Does the dataset relate to people? If not, you may skip the remaining questions in this section. No. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? n/a Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. n/a Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. n/a If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). n/a Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. n/a Any other comments? None. Preprocessing/cleaning/labeling Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. 36MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior No preprocessing was performed on the sequence data released in this dataset. Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data. n/a Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point. n/a Any other comments? None. Uses Has the dataset been used for any tasks already? If so, please provide a description. Yes: this dataset was released to accompany the three tasks of the 2022 Multi-Agent Behavior (MABe) Challenge, posted here. In this challenge, competitors are provided video of multiple interacting animals and tasked with learning a general-purpose, low-dimensional representation of the video. They upload their learned representations to the evaluation site, which then trained simple linear classifiers on the set of secret tasks described above, and returns accuracy measures. Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. Papers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset What (other) tasks could the dataset be used for? Besides unsupervised representation learning, this dataset could also be used for supervised representation learning, using the hidden labels as supervision. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms? No. Are there tasks for which the dataset should not be used? If so, please provide a description. None. Any other comments? None. Distribution Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. Yes - the full dataset will be made publicly available for download by all interested parties by July 1st, 2023. How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the dataset have a digital object identifier (DOI)? 37MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior The dataset is available on the Caltech public data repository at https://data.caltech.edu/records/rdsa8-rde65. A previous version, containing only the trajectory data, is available at https://data.caltech.edu/records/20186. The data will be retained indefinitely and available for download by all third parties. The data.caltech.edu posting has accompanying DOI https://doi.org/10.22002/rdsa8-rde65. The dataset as used for the MABe Challenge (lacking hidden task labels) is available for download on the AIcrowd page, located at (https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-fruit-fly-groups). When will the dataset be distributed? Yes - the full dataset will be made publicly available for download by all interested parties by July 1st, 2023. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. The MABe22 dataset is distributed under the CreativeCommons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA). The terms of this license may be found at https://creativecommons.org/licenses/by-nc-sa/2.0/legalcode. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. There are no third party restrictions on the data. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. No export controls or regulatory restrictions apply. Any other comments? None. Maintenance Who will be supporting/hosting/maintaining the dataset? The dataset is hosted on the Caltech Research Data Repository at data.caltech.edu. Dataset hosting is maintained by the library of the California Institute of Technology. Long-term support for users of the dataset is provided by Jennifer J. Sun and by the laboratory of Ann Kennedy. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? The managers of the dataset (JJS and AK) can be contacted at mabe.workshop@gmail.com, or AK can be contacted at ann.kennedy@northwestern.edu and JJS can be contacted at jjsun@caltech.edu. Is there an erratum? If so, please provide a link or other access point. No. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)? Users of the dataset have the option to subscribe to a mailing list to receive updates regarding corrections or extensions of the MABe22 dataset. Mailing list sign-up can be found on the MABe22 webpage at https://sites.google.com/view/computational-behavior/our- datasets/mabe2022-dataset. Updates to correct errors in the dataset will be made promptly, and announced via update messages posted to the MABe22 website and data.caltech.edu page. Updates that extend the scope of the dataset, such as additional hidden tasks, or improved pose estimation, will be released as new named instantiations on at most a yearly basis. Previous versions of the dataset will remain online, but obsolescence notes will be sent out to 38MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior the MABe22 mailing list. In updates, dataset version will be indicated by the year in the dataset name (here 22). Dataset updates may accompany new instantiations of the MABe Challenge. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced. N/a (no human data.) Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users. Yes, the dataset will be permanently available on the Caltech Research Data Repository (data.caltech.edu), which is managed by the Caltech Library. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description. Extensions to the dataset will take place through at-most-yearly updates. We welcome community contributions of behavioral data, novel tracking methods, and novel hidden tasks; these may be submitted by contacting the authors or emailing mabe.workshop@gmail.com. All community contributions will be reviewed by the managers of the dataset for quality of tracking and annotation data. Community contributions will not be accepted without a data maintenance plan (similar to this document), to ensure support for future users of the dataset. Any other comments? If you enjoyed this dataset and would like to contribute other multi-agent behavioral data for future versions of the dataset or MABe Challenge, contact us at mabe.workshop@gmail.com! B.3. Beetle Datasheet Motivation For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. Interactions between different animal species constitute a core component of how ecological communities function. How these interactions work mechanistically promises to provide rich insight for the neuroscience community, as well as critical information on how networks of organisms operate in nature. Studying these interactions consist of understanding how sensory systems control response to the many different species an animal will encounter, what simple modules string together to build complex behaviors, how stereotyped are the behavioral outputs in response to particular stimuli, etc. Most quantitative behavioral data to this point is composed of either solo organisms, or members of the same species interacting. Our dataset provides behavioral video data of pairs of different species interacting. The Multi-Agent Behavior 2022 (MABe22) dataset is a new set of animal tracking, pose, video, and behavior datasets, intended to serve as a benchmark dataset for evaluation of unsupervised/self-supervised behavior representation learning and discovery methods. This datasheet is specific to the Ant-Beetle Interaction dataset, which consists of video recordings of rove beetles ( Sceptobius lativentris) interacting with velvety tree ants (Liometopum occidentale, a species that rove beetles interact with symbiotically) and with other beetle species. This data offers a test case for algorithmic approaches to identify and assess the behavior space that these interaction partners traverse. Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The behavioral video data was collected and annotated by Julian Wagner in the lab of Joseph Parker at Caltech. Julian Wagner collected insects in the Los Angeles National Forest, filmed their interactions, and annotated their behavior in Behavioral Observation Research Interactive Software (BORIS) (documentation link) by Julian Wagner. Data was parsed into 30 second sections, downscaled, and pre-processed by Jennifer Sun. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. 39MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Acquisition of behavioral data was supported by Army Research Office MURI award W911NF1910269 (JP) and a US National Science Foundation CAREER award (2047472) (JP). Any other comments? None. Composition What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. The core element of this dataset, called a sequence, is one 30-second video of a rove beetle (textitSceptobius lativentris) interacting with another insect or object. Each video is accompanied by 14 frame- or sequence-level labels describing the species/type of interactor, the time elapsed since the start of the interaction session, as well as frame-wise manual annotations for six behaviors of interest. Video and annotations were originally acquired at 60 Hz, and are downsampled to 30 Hz in the released dataset. How many instances are there in total (of each type, if appropriate)? The dataset is composed of 11,536 30 second sequences. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). The source dataset consists of 2-hour-long videos of rove beetle-interactor pairings, with each video capturing eight such pairings simultaneously (housed within the wells of an eight-well plate.) The raw video recordings were screened to identify wells that appeared to have occurrences of multiple types of behavior of interest; manual annotation of animal behavior was performed on this subset of wells. It is therefore possible that this dataset is biased for videos with higher rates of animal movement than in the full raw video dataset; this was done to provide a larger number of representative examples of animal behavior. The 30 second clips comprising each instance in this dataset are extracted from the subset of wells for which annotation was performed. The extracted sequences included in this dataset are uniformly sampled from the source dataset as follows: first, the video is cropped to contain only the subject well for which behavior was annotated. Next, starting at the beginning of each video, we discard a randomly chosen segment of between 0.5 and 2 seconds (75 and 300 frames), then save the following 30-second clip as one dataset instance; this process is then repeated from the point where the preceding 30-second clip ended, onward through the end of the video. The clips therefore comprise a representative sample of the annotated ant-beetle interaction experiment. What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or features? In either case, please provide a description. Each instance consists of 30 seconds of raw video data (800x800 resolution and sampled at 30 Hz, i.e. 900 frames of images), accompanied by eight ”sequence-level” labels of the interactor type and time since start of the interaction, and six ”frame-level” labels which are manual annotations for the occurrence of various behaviors of interest (i.e. six binary vectors of length 900 indicating the presence or absence of each behavior on each frame of the video.) Is there a label or target associated with each instance? If so, please provide a description. Yes, each instance is associated with a sequence-level label describing the species/object that the beetle is interacting with and a sequence- level label indicating the time elapsed since the start of the interaction session (between 0 and 4 hours). Each instance is also associated with six binary frame-wise labels indicating the presence or absence of a set of behaviors of interest. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. There is no missing data. Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)? If so, please describe how these relationships are made explicit. 40MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Each instance (sequence) is to be treated as an independent observation with no relationship to other instances in the dataset. Although the identities of the interacting animals are the same in some sequences, this information is not tracked in the dataset. Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them. The dataset includes a recommended train/test split which was used for the Multi-Agent Behavior Challenge. Data was randomly split into training, test, and private-test sets (where the private test set was withheld from challenge evaluation until the end of the competition period, to avoid overfitting.) Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. The frame-wise annotations of behavior are manually generated by a trained human expert based on visual inspection the behavioral video, and are done by only one annotator. The initiation point of a particular behavior can be difficult to assess accurately and will be biased by the style of a given annotator. This makes the start and stop point of some behavioral categories (e.g. where a long grooming bout begins) more likely to be noisy and subjective to call than, say, the behavioral category in the middle of a protracted bout of a given behavior. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. The dataset is self-contained. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description. No. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. No. Does the dataset relate to people? If not, you may skip the remaining questions in this section. No. Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. n/a Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how. n/a Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. n/a Any other comments? None. Collection Process 41MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. The raw behavioral videos were collected in a custom recording setup described in the following section. Videos were cropped and matted to isolate individual interaction wells, annotated by hand for behaviors and then split into the sequences. Sequence-level labels of interactor type and time since experiment start are ground-truth information known to the experimenter. Frame-wise labels of subject behavior are manually scored by a trained human expert; no secondary validation of these annotations was performed. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Behavioral trials were performed in custom arenas made from 1/8th inch infrared transmitting acrylic (Plexiglass IR acrylic 3143, https://www.eplastics.com/plexiglass/acrylic-sheets/ir-transmitting) which transmits far red and infrared while blocking visible light. Arenas consist of a base layer of finely wet-sanded acrylic (to provide texture for beetles to walk on) a layer with eight two-centimeter round wells, a roof of anti-static acrylic (https://www.mcmaster.com/8774K17/) and a final top of inferred transmitting acrylic. Behavioral interactions were run at 15 C in a dark incubator with door closed. Arenas were top lit with IR850nm led flood lights. Recordings of interactions were made using a Flir machine vision camera (BFS-U3-51S5M-C: 5.0 MP) at 60 frames per second with a Pentax 12mm 1:1.2 TV lens (by Ricoh, FL-HC1212B-VG), for 2 hours. To split multiplexed arena videos into individual wells, we manually set crop parameters for each well in each video, and cropped and matted the edges using openCV . We annotated videos of individual interaction wells with BORIS (Behavioral Observation Research Interactive Software (BORIS) user guide — BORIS latest documentation). If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? This answer is repeated from an earlier section: the source dataset consists of 2-hour-long videos of rove beetle-interactor pairings, with each video capturing eight such pairings simultaneously (housed within the wells of an eight-well plate.) The raw video recordings were screened to identify wells that appeared to have occurrences of multiple types of behavior of interest; manual annotation of animal behavior was performed on this subset of wells. It is therefore possible that this dataset is biased for videos with higher rates of animal movement than in the full raw video dataset; this was done to provide a larger number of representative examples of animal behavior. The 30 second clips comprising each instance in this dataset are extracted from the subset of wells for which annotation was performed. The extracted sequences included in this dataset are uniformly sampled from the source dataset as follows: first, the video is cropped to contain only the subject well for which behavior was annotated. Next, starting at the beginning of each video, we discard a randomly chosen segment of between 0.5 and 2 seconds (75 and 300 frames), then save the following 30-second clip as one dataset instance; this process is then repeated from the point where the preceding 30-second clip ended, onward through the end of the video. The clips therefore comprise a representative sample of the annotated ant-beetle interaction experiment. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? All data was collected and annotated by Julian Wagner, a graduate student in the lab of Joseph Parker, as part of their thesis work studying social symbiotic beetles. As a full-time employee of the Parker lab, Wagner’s compensation was not dependent on participation in this project. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Video data was collected and annotated over the course of several months in 2021. Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. No; because all species studied are invertebrates, these experiments are not subject to monitoring by an institutional review board. Does the dataset relate to people? If not, you may skip the remaining questions in this section. No. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? 42MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior n/a Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. n/a Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. n/a If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). n/a Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. n/a Any other comments? None. Preprocessing/cleaning/labeling Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. The raw behavioral videos are 2448x2048 pixel resolution and are sampled at 60 Hz, viewed from above an arena with 8 individual circular wells. We split these videos by cropping each well out, and blacking out the edges of the frame outside the focal circle for that well. Videos were then downsampled to 800x800 pixel resolution, and temporally downsampled to 30 Hz. Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data. The raw two-hour movies with all wells visible are not available. Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point. Labeling instances was done in BORIS (Behavioral Observation Research Interactive Software (BORIS) user guide — BORIS latest documentation). Any other comments? None. Uses Has the dataset been used for any tasks already? If so, please provide a description. Yes: this dataset was released to accompany the 2022 Multi-Agent Behavior (MABe) Challenge, posted here. This competition was aimed at generating learned representations of animals’ actions using unsupervised or self-supervised techniques. 43MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. Papers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset What (other) tasks could the dataset be used for? While this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised learning tasks. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms? No. Are there tasks for which the dataset should not be used? If so, please provide a description. None. Any other comments? None. Distribution Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. Yes - the full dataset will be made publicly available for download by all interested parties by July 1st, 2023. How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the dataset have a digital object identifier (DOI)? The dataset is available on the Caltech public data repository at https://data.caltech.edu/records/rdsa8-rde65. A previous version, containing only the trajectory data, is available at https://data.caltech.edu/records/20186. The data will be retained indefinitely and available for download by all third parties. The data.caltech.edu posting has accompanying DOI https://doi.org/10.22002/rdsa8-rde65. The dataset as used for the MABe Challenge (lacking hidden task labels) is available for download on the AIcrowd page, located at (https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-mouse-triplets). When will the dataset be distributed? The full dataset will be made publicly available for download by all interested third parties by July 1st, 2023. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. The MABe22 dataset is distributed under the CreativeCommons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA). The terms of this license may be found at https://creativecommons.org/licenses/by-nc-sa/2.0/legalcode. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. There are no third party restrictions on the data. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. 44MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior No export controls or regulatory restrictions apply. Any other comments? None. Maintenance Who will be supporting/hosting/maintaining the dataset? The dataset is hosted on the Caltech Research Data Repository at data.caltech.edu. Dataset hosting is maintained by the library of the California Institute of Technology. Long-term support for users of the dataset is provided by Jennifer J. Sun and by the laboratory of Ann Kennedy. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? The managers of the dataset (JJS and AK) can be contacted at mabe.workshop@gmail.com, or AK can be contacted at ann.kennedy@northwestern.edu and JJS can be contacted at jjsun@caltech.edu. Is there an erratum? If so, please provide a link or other access point. No. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)? Users of the dataset have the option to subscribe to a mailing list to receive updates regarding corrections or extensions of the MABe22 dataset. Mailing list sign-up can be found on the MABe22 webpage at https://sites.google.com/view/computational-behavior/our- datasets/mabe2022-dataset. Updates to correct errors in the dataset will be made promptly, and announced via update messages posted to the MABe22 website and data.caltech.edu page. Updates that extend the scope of the dataset, such as additional hidden tasks, or improved pose estimation, will be released as new named instantiations on at most a yearly basis. Previous versions of the dataset will remain online, but obsolescence notes will be sent out to the MABe22 mailing list. In updates, dataset version will be indicated by the year in the dataset name (here 22). Dataset updates may accompany new instantiations of the MABe Challenge. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced. N/a (no human data.) Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users. Yes, the dataset will be permanently available on the Caltech Research Data Repository (data.caltech.edu), which is managed by the Caltech Library. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description. Extensions to the dataset will take place through at-most-yearly updates. We welcome community contributions of behavioral data, novel tracking methods, and novel hidden tasks; these may be submitted by contacting the authors or emailing mabe.workshop@gmail.com. All community contributions will be reviewed by the managers of the dataset for quality of tracking and annotation data. Community contributions will not be accepted without a data maintenance plan (similar to this document), to ensure support for future users of the dataset. 45MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Any other comments? If you enjoyed this dataset and would like to contribute other multi-agent behavioral data for future versions of the dataset or MABe Challenge, contact us at mabe.workshop@gmail.com! C. Dataset Description Details C.1. Fly Groups C.1.1. E XPERIMENTAL SETUP Optogenetic experiments used group-housed, mated female flies (4–5 days post eclosion) that were sorted into 10 flies per vial. Flies were reared in the dark in a 12:12 light-dark cycle incubator (25 ◦, 50% relative humidity) on standard food supplemented with retinal (Sigma-Aldrich, St. Louis, MO) (0.2 and mM all trans-retinal prior to eclosion and 0.4 mM all trans-retinal post eclosion). Control lines, lines labeling cell types involved in the female aggression circuit, and the CsChrimson effector line were described previously (Schretter et al., 2020; Aso et al., 2014). Blind control and blind aIPg lines were generated through crossing established lines with a mutation in norpA (Bloomquist et al., 1988) and lines described previously (Schretter et al., 2020). All experiments were performed during the morning activity peak (ZT0-ZT3). For thermogenetic experiments, flies were reared in a 12:12 light:dark incubator (22◦C 50% relative humidity) on a standard molasses food. They were cold anesthetized and sorted into groups of 5 males and 5 females, unless noted as “male71G01 + female control” and “control sex-separated”. These flies were housed separately in groups of 5 males or 5 females prior to the experiments. All flies were food deprived on agar media for 24 hours directly before recording. Experiments were conducted at the permissive temperature for TrpA, 30◦, and 50% relative humidity during the evening activity peak (ZT8-ZT12). Control lines, the TrpA effector line, and lines labeling cell types involved in courtship or avoidance were previously described (Robie et al., 2017; Wu et al., 2016). The circular assay chamber was 50 mm in diameter and 3.5 mm tall, with a domed translucent ceiling coated with silicon (Sigma Cote, Sigma Aldridge) to prevent upside-down walking and a translucent acrylic floor. The chambers were illuminated from below with infrared light from custom LED panels and recorded from above with a USB3 camera at 150 fps (Flea3, FLIR) with an 800-nm long-pass filter. Visible white light was present at all times so that the flies could see. For optogenetic experiments, neurons expressing CsChrimson were activated with 617-nm red light from custom LED panels. Experiments were run with one of two activation protocols. Protocol 1 consisted of 2 repeats of a 30s (red) lights-off period then a 30s “strong” lights-on period (7 mW/cm2, pulsed at 30 Hz with on period 10/33 ms), followed by a 30s lights-off period, then 2 repeats of a 30s lights-off period then a 30s “weak” lights-on period (3 mW/cm2 constant illumination), then a 30s lights-off period. In total, these videos were 300s (45000 frames) long. Protocol 2 consisted of 3 repeats of a 30s lights-off period then a 30s “weak” lights-on period (1 mW/cm2, constant) followed by 3 repeats of a 30s lights-off period then a 30s “strong” lights-on period (3 mW/cm2). In total, these videos were 390s long (58500 frames). For thermogenetic experiments, videos were recorded for 300 seconds (45000 frames). C.1.2. F LY TRACKING The body and wings of the flies were tracked using the FlyTracker software (Eyjolfsdottir et al., 2014). 19 selected landmark points were tracked using the Animal Part Tracker (APT) (Kabra et al., 2022), depicted in Figure 11. Coordinates were converted from pixels to millimeters by detecting the circular arena boundary, with (0, 0) corresponding to the arena center. C.1.3. F LY BEHAVIOR ANNOTATION Using JAABA (Kabra et al., 2013), we annotated 6 behaviors involved in fly courtship and aggression: • Aggression: The focus fly was angled towards another fly and engaged in several touches with ≥ 2 limbs to the head, abdomen or thorax of another fly, causing the other fly to move. This behavior included head butting, fencing, and 46MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Fly type N. videos Description Control 1 9 Groups of 5 female and 5 male flies from control line pBDPGAL4u x TrpA that were raised together. Control 1 sex- separated 4 Groups of 5 female and 5 male flies from control line pBDPGAL4u x TrpA that were raised separately, with groups encountering each other for the first time in the videos. Control 2 6 Groups of 10 female flies from control line JHS K 85321 x CsChrimson R71G01 13 Groups of 5 female and 5 male flies from courtship line R71G01 x TrpA Male R71G01 fe- male control 5 Groups of 5 female flies from the control line pBDPGAL4U x TrpA and 5 male flies from courtship line R71G01 x TrpA R65F12 12 Groups of 5 female and 5 male flies from courtship line R65F12 x TrpA R91B01 10 Groups of 5 female and 5 male flies from visual avoidance line R91B01 x TrpA Blind control 9 Groups of 10 blind female flies from control line JHS K 85321 x ChR with the norpA mutation aIPg 9 Groups of 10 female flies from aggression line SS36564 x ChR, which targets aIPg neurons pC1d 8 Groups of 10 female flies from aggression line SS56987 x ChR, which targets pC1d neurons. Blind aIPg 11 Groups of 10 blind female flies with the norpA mutation from aggression line SS36564, which targets aIPg neurons Any courtship 30 Any of R71G01, Male R71G01 + female control, or R65F12. Any control 28 Any of Control 1, Control 1 sex-separated, Control 2, or Blind control. Any blind 20 Any of Blind control, Blind aIPg. Any aIPg 20 Any of aIPg or Blind aIPg. Any aggression 28 Any of aIPg, pC1d, blind aIPg. Any R71G01 18 Any of R71G01 or Male R71G01 + female control Any sex separated 9 Any of Control 1 sex-separated or Male R71G01 + female control. Table 9.Descriptions of types of flies used in each task. Task type Description Fly type 1 indicates activation periods (whole video for TrpA, any lights-on periods for ChR) of the selected fly type. 0 indicates activation periods for other lines. nan indicates lights-off periods. On vs off 1 indicates activation lights-on periods for the selected fly type, 0 lights-off periods for that fly type. nan indicates other fly types. Strong vs off 1 indicates strong activation lights-on periods for the selected fly type, 0 lights-off periods for that fly type. nan indicates other fly types. Weak vs off 1 indicates weak activation lights-on periods for the selected fly type, 0 lights-off periods for that fly type. nan indicates other fly types. Strong vs weak 1 indicates strong activation lights-on periods for the selected fly type, 0 weak activation lights-on periods for that fly type. nan indicates lights-off periods for that fly type, or any other fly type. Last vs first 1 indicates the last strong activation lights-on period for the selected fly ty[e, 0 the first strong activation lights-on period for that fly type. nan indicates other lights-on periods or lights off periods for that fly type, or any other fly type. Manual annotation 1 indicates frames from any fly type manually labeled as the selected behavior, 0 frames manually labeled as not the selected behavior, nan frames that were not labeled. Female vs male 1 indicate female flies, 0 indicates male flies. Table 10.Descriptions of types of comparisons made in each task. 47MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Task Flies/Behavior Task type 1 Control 1 Fly type 2 Control 1 sex- separated Fly type 3 Control 2 Fly type 4 R71G01 Fly type 5 male R71G01 fe- male control Fly type 6 R65F12 Fly type 7 R91B01 Fly type 8 Blind Control Fly type 9 aIPG Fly type 10 pC1d Fly type 11 Blind aIPG Fly type 12 Blind control On vs off 13 Blind control Strong vs off 14 Blind control Weak vs off 15 Blind control Strong vs weak 16 Blind control Last vs first 17 Control 2 On vs off 18 Control 2 Strong vs off 19 Control 2 Weak vs off 20 Control 2 Strong vs weak 21 Control 2 Last vs first 22 Blind aIPg On vs off 23 Blind aIPg Strong vs off 24 Blind aIPg Weak vs off 25 Blind aIPg Strong vs weak Task Flies/Behavior Task type 26 Blind aIPg Last vs first 27 aIPg On vs off 28 aIPg Strong vs off 29 aIPg Weak vs off 30 aIPg Strong vs weak 31 aIPg Last vs first 32 pC1d On vs off 33 pC1d Strong vs off 34 pC1d Weak vs off 35 pC1d Strong vs weak 36 pC1d Last vs first 37 Any courtship Fly type 38 Any control Fly type 39 Any blind Fly type 40 Any aIPg Fly type 41 Any aggression Fly type 42 Any R71G01 Fly type 43 Any sex-separated Fly type 44 All Female vs male 45 Aggression Manual annotation 46 Chase Manual annotation 47 Courtship Manual annotation 48 High fence Manual annotation 49 Wing ext. Manual annotation 50 Wing flick Manual annotation Table 11.Descriptions of fly tasks. 48MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Figure 11.19 tracked landmark points on the fly body. shoving behaviors as defined (Nilsen et al., 2004; Schretter et al., 2020). • Chase: The focus fly was following another moving fly, maintaining a small, somewhat constant distance to it (Robie et al., 2017). • Courtship: The focus fly was performing any stage of the courtship sequence, including orienting, following, tapping, singing, licking, attempted copulation, or copulation (Sokolowski, 2001). • High posture fencing: The focus fly was angled towards another fly with the mid legs of the fly angled sharply (< 45 degrees), and the forelegs lifted off of the bottom of the arena and touching limbs, head, abdomen or thorax of another fly (Nilsen et al., 2004; Schretter et al., 2020). • Wing extension: The focus fly unilaterally rotates a wing out for an extended period of time. This behavior is likely an indication of the fly producing courtship song with the extended wing (Robie et al., 2017). • Wing flick: The focus fly rapidly and symmetrically moves its wings out and back in performing a quick scissoring movement several times in a row (Robie et al., 2017). As all of the behaviors we annotated occur rarely, we sparsely annotated the data using frames suggested using JAABA’s interactive system. We only annotated frames for which we were confident of the correct class. We annotated frames across all fly types, for many different videos and flies. For all behaviors, the classifiers trained by JAABA using the annotated data looked reasonable, based on casual proofreading. C.1.4. D ATA SPLITTING We split the data into 4 sets, with each set containing distinct videos and flies. • User train: Data given to the competitor to learn their embedding. • Evaluation train: Data used to train the linear classifier during evaluation. • Test 1: Data used to measure performance of the linear classifier. Performance on this dataset was presented on the leaderboard during the competition. 49MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior • Test 2: Final set of data used to measure performance on the linear classifier, used for determining the competition winners. We used simulated annealing to find a way to split the videos so that: • There were videos from each fly type in each set. • There were manual labels from each fly type and each behavior category in each set. • Approximately 60% of videos were in User train, 20% in Evaluator train, 10% in Test 1, and 10% in Test 2. • For each behavior type and fly type, approximately 40% of manual labels for each behavior were in User train, 30% in Test 1, and 30% in Test 2. We split each video into segments of length 30s (4500 frames), with gaps of a randomly selected interval between .5s (75 frames) and 2s (150 frames) between segments. Included segments were chosen such that they did not include obvious identity tracking errors (trajectory births or deaths). Flies were shuffled within each segment so that fly i across segments did not correspond. C.2. Mice Triplets C.2.1. E XPERIMENTAL SETUP This section is adapted from (Beane et al., 2022; Sheppard et al., 2022; Geuther et al., 2019). Experiments were performed in the JAX Animal Behavior System (JABS), consisting of an open field arena measuring 52 cm by 52 cm, with overhead LED ring lighting on a 12:12 light-dark cycle. The arena floor is white PVC plastic covered by a layer of bedding (wood shavings and Alpha-Dri), and food and water are held in a hopper with grate access in one arena wall, and replaced when depleted. For recording videos while lights were off, additional IR LED lighting at 940 nm was added. Video was recorded at 30Hz using a Basler acA1300-75gm camera with 4-12mm lens (Tamron) and 800nm longpass filter (Hoya) to exclude visible light, using a custom recording client developed by JAX (see https://github.com/KumarLabJax/JABS-data-pipeline). Experimental mice were adult males between 10 and 20 weeks old, of genetic background C57Bl/6J or BTBR. Prior to testing, animals were allowed to acclimate to the behavior room for 30-60 minutes, after which three mice were introduced to the JABS arena over a period of several minutes. Behavior was recorded continuously for four days, during which time animal behavior and welfare was monitored remotely. All behavioral tests were performed in accordance with approved protocols from The Jackson Laboratory Institutional Animal Care and Use Committee guidelines. C.2.2. M OUSE TRACKING 12 anatomical keypoints on each animal were tracked using a modified version of HRnet (provided at https://github.com/KumarLabJax/deep-hrnet-mouse), with coordinates of keypoints reported in pixels (Sheppard et al., 2022). Occurrence of each anatomically defined keypoint were grouped into up to four animal pose instances (one more than the number of mice present), using associative embedding (Newell et al., 2017) to evaluate likelihood of keypoint pairs belonging to the same animal. The four candidate pose instances were then assigned animal identities by computing distances between all tracked pose pairs across neighboring video frames, and propagating animal IDs forward in time to the closest pose instance falling within a maximum radius. A second post-hoc pass was then applied to extracted pose tracklets, in which incomplete pose instances were merged when complementary pairs of points were found within a maximum radius, and resulting tracklets were merged based on a minimum distance criterion, to produce the final set of three pose trajectories provided in the dataset. 50MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Task Name Type Values Description Experiment day Sequence 1-4 Mice were filmed interacting for four days after introduc- tion to a new arena; task is to determine which day a sequence comes from. Time of day Sequence 0-1440 Mice show circadian changes in their level of activity; task is to infer time of day from behavior. Strain Sequence 0 or 1 Mice are from either C57Bl/6J or BTBR genetic back- ground. Strain field is 1 for BTBR and 0 for C57Bl/6J. Lights Sequence 0 or 1 Mice are more active when the lights are off, which occurs between 6am and 6pm; task is to infer light condition from behavior. Table 12.Format of experimentally-defined tasks for mouse dataset. C.2.3. M OUSE BEHAVIOR ANNOTATION Mouse behavioral videos were manually annotated using the VIA video annotator (Dutta & Zisserman, 2019). Each of the behaviors: huddle, chase, anal sniff, and face sniff, was annotated as an individual time series with frame-level temporal resolution. We annotated 400 clips overall (200/100/100; train/val/test), randomly selected from the full set of videos. Chase was annotated when a pair of mice moved quickly, with one mouse following close behind the other. Huddle was annotated when the bodies of the mice are in close contact and the animals are stationary for at least several seconds; it can occur between either pairs or triplets of animals. Face sniffing was annotated when a close-investigation behavior occurred in which the nose of one mouse was in close contact with the nose or face of another mouse. Anogenital sniffing was annotated for a close-investigation behavior in which one mouse is investigating the anogenital area of another, typically with its nose near the base of the tail or pushed underneath the hindquarters of the other animal. C.2.4. D ATA SPLITTING Each dataset was randomly assigned into four sets; due to the relatively small number of source experiments, we did not separate sets by animal identity. The percentage of videos/trajectories assigned to each set is given in parentheses. • User train (30%): Data given to the competitor to learn their embedding (note that competitors could also include the submission train, test 1, and test 2 video/trajectories for training, but these were not included for experiments in the main text.) • Evaluation train (50%): Data used to train the linear classifiers during evaluation. • Test 1 (10%): Data used to measure performance of the linear classifiers. Performance on this dataset was presented on the leaderboard during the competition. • Test 2 10%): Final set of data used to measure performance of the linear classifiers, and for determining the competition winners. C.3. Beetle Interactions C.3.1. E XPERIMENTAL SETUP This dataset consists of videos of paired insect interactions. One of the interactor is a symbiotic rove beetles (Sceptobius lativentris), while the other interactor may be their host ant ( Liometopum occidentale), manipulated host ant (e.g. with pheromones stripped off), or other insects (e.g. clown or nitidulid beetles). The original video recordings consists of 8-well behavioral interaction chambers (2cm diameter circles) in the dark and illuminated with infrared lights from the side/top. A top-mounted machine vision camera sensitive to IR light monitored the two-hour behavioral trials at 60 Hz, which we downsample to 30Hz for MABe22. Individual circular wells were cropped/parsed from the multi-well video by hand and saved at 800x800 resolution. We annotated six behaviors in whole two-hour videos, consisting of seven different types 51MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior of one-on-one interactions using BORIS (14 hours total). These interactors represent a range of cue types, from the host organism with which the symbiont should interact extensively, to a neutral random other insect which the symbiont will likely ignore. Generating a meaningful representation that extracts information of interest about the different behaviors adopted by the beetle in response to these disparate cues is crucial for insight into how species interact in nature. C.3.2. B EETLE TASK DESCRIPTIONS For the beetle dataset, identifying the sequence-level interactor apply to all frames, while the frame-level behavior tasks apply to a subset of the videos. All of these tasks are classification, except for a regression task for interaction duration, where the goal is to identify how long the two organisms have been interacting (up to 4 hours). The following sequence-level labels describe the type of interactors present: histerid Sceptobius lativentrisinteracting with a clown beetle (familyHisteridae.) nitidulid Sceptobius lativentrisinteracting with a sap beetle (family Nitidulidae.) locc Sceptobius lativentrisinteracting with a live Liometopum occidentale. gasterless Sceptobius lativentris interacting with a live gasterless Liometopum occidentale ant, i.e. an ant with its gaster (abomen) removed. platy Sceptobius lativentrisinteracting with a live Platyusa sonomaebeetle. reapplied Sceptobius lativentrisinteracting with a dead Liometopum occidentale stripped of pheromones and then with pheromones reapplied. tethered Sceptobius lativentrisinteracting with a live Liometopum occidentale tethered to a magnet, i.e. immobilized in the center of the arena. The following frame-wise labels reflect categories of behavior present in the video: grooming object Sceptobius lativentrisis grooming the interactor object/insect. grooming self Sceptobius lativentrisis grooming itself (e.g. cleaning an antenna). idle alone Sceptobius lativentrisis idle (not doing any visible behavior) by itself. idle object Sceptobius lativentrisis idle (not doing any visible behavior) by on top of the interactor/object. exploring object Sceptobius lativentrisis exploring (moving around on) atop the interac- tor/object. exploring alone Sceptobius lativentrisis exploring (moving around) in the arena. To evaluate the performance of frame-level behavior labels, we generate two sets of evaluation conditions, same and different. For same, we create the evaluation train and test splits with the same interactor types (so the linear evaluator has access to the same interactors for behavior classification during train and test). For different, we create the evaluation train and test split with different interactor types (so the linear evaluator has access to different interactors for behavior classification during train and test). Note that this only affects the linear evaluation split, and does not affect the representation learning model. C.3.3. D ATA SPLITTING Each dataset was randomly assigned into four sets; the data is split such that either the interactor type is the same across evaluation splits, or different as described above. The percentage of videos/trajectories assigned to each set is given in parentheses. Note that this percentage may vary across different conditions. • User train (25%): Data given to the competitor to learn their embedding (note that competitors could also include the submission train, test 1, and test 2 video/trajectories for training, but these were not included for experiments in the main text.) • Evaluation train (60%): Data used to train the linear classifiers during evaluation. • Test 1 (7.5%): Data used to measure performance of the linear classifiers. Performance on this dataset was presented on the leaderboard during the competition. • Test 2 7.5%): Final set of data used to measure performance of the linear classifiers, and for determining the competition winners. 52MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior D. Evaluation For all tasks, we evaluate representation learning performance using a linear evaluation protocol, by training a linear model on top of the learned representation at each frame for classification and regression on a set of downstream tasks. These downstream tasks are unseen during training of the representation learning model. We train separate linear models per task, and because of the high class imbalance of some tasks, the classes are weighted inverse to class frequencies during training. For training the linear models, we use three fixed random 80% of the evaluation train split to train three models. All evaluations are performed on a fixed test set. For classification tasks, majority voting combines the predictions of the three classifiers. For regression tasks, the predictions are averaged. Both merging schemes are done at the frame level. The evaluation metrics are F1 score for classification and Mean Squared Error for regression computed for each sequence, then averaged over the sequences. Note that all sequences given an organism have the same number of frames. We use default hyperparameters for the Ridge classifier and do not perform hyperparameter tuning. Notably, the evaluation framework does not choose a particular feature normalization strategy, and any feature normalization should happen before input to the framework. F1 score. The F1 score is the harmonic mean of the Precision P and Recall R: P = T P T P+ F P (6) R = T P T P+ F N (7) F1 = 2 × P × R P + R (8) Where true positives (TP) is the number of frames that a model correctly labels as positive for a class, false positives (FP) is the number of frames incorrectly labeled as positive for a class, and false negatives (FN) is the number of frames incorrectly labeled as negative for a class. For F1 score across tasks, we take an unweighted average across classification tasks in either the mouse or fly domain. For our evaluation, the class with the highest predicted probability in each frame was used to compute F1 score, but the F1 score will likely be higher with threshold tuning. Mean Squared Error. For regression tasks, given n data samples, we use the predicted values ¯y and the real labels y to compute: MSE = 1 n nX i=1 (yi − ¯yi)2 (9) We normalize the label values for regression to between 0 and 1. In our dataset, the experiment day and time of day tasks are regression tasks, while all other tasks are classification tasks. E. Implementation Details/Hyperparameters For studying self-supervised video learning we used adapted the SlowFast (Fan et al., 2020) implementations of SOTA methods. We list hyperparameters for each methods below. 53MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Config Value optimizer AdamW (Loshchilov & Hutter, 2017b) optimizer momentum β1, β2=0.9,0.999 weight decay 0.05 learning rate 0.0001 learning rate schedule cosine decay (Loshchilov & Hutter) warmup epochs (Goyal et al., 2017) 10 epochs 800 augmentation hflip, crop [0.5, 1] batch size 32 gradient clipping 0.02 Table 14.Training parameters for MaskFeat (Wei et al., 2022). Config Value optimizer SGD optimizer momentum β1, β2=0.9,0.999 weight decay 1e-6 learning rate 1.2 learning rate schedule cosine decay (Loshchilov & Hutter) warmup epochs (Goyal et al., 2017) 35 epochs 200 augmentation hflip, crop [0.5, 1] batch size 32 gradient clipping 0.02 Table 15.Training parameters for ρBYOL (Feichtenhofer et al., 2021). Config Value optimizer AdamW (Loshchilov & Hutter, 2017b) optimizer momentum β1, β2=0.9,0.95 (Chen et al., 2020a) weight decay 0.05 learning rate 1.6e-4 learning rate schedule cosine decay (Loshchilov & Hutter) warmup epochs (Goyal et al., 2017) 60 epochs 2000 augmentation hflip, crop [0.5, 1] batch size 64 gradient clipping 0.02 Table 13.Training parameters for MAE (He et al., 2022). 54MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior F. Additional Trajectory Method Results We present additional results for trajectory based methods, from community-contributed solutions for the first phase of our challenge. This dataset consists of 5336 clips of mouse triplets, alongside 968 clips of fly data. F.1. Mouse Programmatically-Annotated Behaviors In addition to the experimental condition labels outlined above, the 9 behaviors were programmatically annotated using heuristics described below using the trajectory data. These programmatically-annotated behaviors were used to evaluate the mouse trajectory methods. Note that multiple behavior labels may be positive on a given frame. • Approach: Mice move from at least 5 cm apart to less than 1 cm apart at closest point, over a period of at least 10 seconds at a maximum speed of 2 cm/sec. • Chase: Mice are moving above 15 cm/sec, with closest points less than 5 cm apart, and angular deviation between mice is less than 30 degrees, for at least 80% of frames within at least one second. Merge bouts less than 0.5 seconds apart. • Close: Closest points of mice are less than 3 cm apart. Merge bouts less than 2 seconds apart. • Contact: Closest points of mice are less than 1 cm apart. Merge bouts less than 2 seconds apart. • Huddle: Closest points of mice are less than 1 cm apart for at least 10 seconds, during which mice show less than 3 cm displacement. Merge bouts less than 2 seconds apart. • Oral-ear contact: Nose and ear of mice are less than 1.5 cm apart for at least 50% of frames within a window of 0.25 seconds or more. Must occur less than 5 seconds after an approach. Merge bouts less than 0.5 seconds apart. • Oral-genital contact: Nose and tail base of mice are less than 1.5 cm apart for at least 50% of frames within a window of 0.25 seconds or more. Must occur less than 5 seconds after an approach. Merge bouts less than 0.5 seconds apart. • Oral-oral contact: Noses of mice are less than 1.5 cm apart for at least 50% of frames within a window of 0.25 seconds or more. Must occur less than 5 seconds after an approach. Merge bouts less than 0.5 seconds apart. • Watching: Mice are more than 5 cm apart but less than 20 cm apart, and gaze offset of one mouse is less than 15 degrees from body of other mouse, for a minimum duration of 3 seconds. Merge bouts less than 0.5 seconds apart. F.2. Results First, we perform a frame-wise PCA as a simple baseline. Principal components were computed from the centered and normalized pose of each mouse, or from the centered pose of each fly and its two nearest neighbors, giving a 60-dim representation for mouse and 253-dim representation for fly. Taking into account all task groups across both datasets, the current best performing models are generally based on transformer architectures (Table 16). Interestingly, T-PointNet, which models trajectory features using point clouds, is competitive on the mouse triplet data. Further work to extend this model to account for more agents could improve its fly group performance. For many mouse and fly task groups, PCA performance was very close to the Base model. However, the top performing models show a significant improvement in performance, demonstrating that we can learn representations that improve behavior analysis performance, even without knowledge of the downstream evaluation tasks. In general, task categories consisting of annotated behaviors are the most challenging for existing models, likely due to the relatively rare positive behavior annotations. These task labels are at the frame-level, where there is a need to capture local temporal information, compared to sequence-level tasks such as “Strain” and “Fly Type” which does not vary over a clip. Representations that can further improve data efficiency of downstream classifiers or better capture local temporal information could help improve the performance of these task groups. 55MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior Mice Triplet Exp. Time of Strain Movement Contact Watching Lights Day↓ Day↓ ↑ Group↑ Group↑ ↑ ↑ PCA .0942±.0000 .946±.0000 .516±.002 .005±.000 .169±.001 .066±.001 .546±.002 TV AE .0940±.0002 .944±.0001 .530±.001 .008±.000 .213±.001 .102±.002 .568±.005 T-Perceiver .0933±.0005 .932±.0005 .698±.014 .014±.001 .232±.005 .164±.005 .697±.006 T-GPT .0927±.0004 .938±.0001 .645±.004 .012±.000 .252±.003 .179±.005 .654±.004 T-PointNet .0928±.0001 .932±.0001 .660±.004 .036±.003 .256±.001 .156±.005 .672±.000 T-BERT .0926±.0004 .928±.0003 .786±.022 .013±.000 .266±.003 .172±.006 .688±.003 Fly Group Fly Stimulation, Stimulation, Line Female Manual - Type↑ Control↑ Aggression↑ Category↑ vs. Male↑ Behaviors↑ PCA .282±.017 .466±.002 .484±.001 .553±.006 .990±.000 .230±.002 - TV AE .199±.005 .500±.019 .450±.011 .341±.009 .821±.005 .222±.011 - T-Perceiver .394±.018 .418±.039 .513±.013 .573±.013 .982±.002 .197±.018 - T-GPT .363±.015 .515±.020 .500±.009 .557±.019 .873±.001 .246±.014 - Table 16.MABe2022 Trajectory Benchmark Results. Task-averaged MSE and F1 score are from mean and standard deviation over five runs. For mouse task groups, “Movement” consists of approach and chase behaviors, and “Contact” consists of close, contact, huddle, oral-ear contact, oral-genital contact, and oral-oral contact behaviors. For fly task groups, “Fly type” corresponds to tasks 1 to 11, “Stimulation Control” is tasks 12 to 21, “Stimulation Aggression” is tasks 22 to 36, “Line Category” is tasks 37 to 43, and “Manual Behaviors” is tasks 45 to 50 in Appendix Table 11. The best performing model is in bold. 56",
      "meta_data": {
        "arxiv_id": "2207.10553v2",
        "authors": [
          "Jennifer J. Sun",
          "Markus Marks",
          "Andrew Ulmer",
          "Dipam Chakraborty",
          "Brian Geuther",
          "Edward Hayes",
          "Heng Jia",
          "Vivek Kumar",
          "Sebastian Oleszko",
          "Zachary Partridge",
          "Milan Peelman",
          "Alice Robie",
          "Catherine E. Schretter",
          "Keith Sheppard",
          "Chao Sun",
          "Param Uttarwar",
          "Julian M. Wagner",
          "Eric Werner",
          "Joseph Parker",
          "Pietro Perona",
          "Yisong Yue",
          "Kristin Branson",
          "Ann Kennedy"
        ],
        "published_date": "2022-07-21T15:51:30Z",
        "pdf_url": "https://arxiv.org/pdf/2207.10553v2.pdf",
        "github_url": "https://github.com/IRLAB-Therapeutics/mabe_2022"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces MABe22, a large-scale, multi-agent video and trajectory benchmark dataset, to assess and encourage the development of learned behavior representations across species and experimental settings. The key contributions include: 1) A diverse dataset of social behavior in mice, beetles, and flies, comprising 15 million video frames and 14 million trajectory frames. 2) A comprehensive suite of 72 downstream evaluation tasks, based on classification of experimental conditions (e.g., genetic strain, time of day, optogenetic stimulation) and expert-annotated behaviors, which are non-annotation-based and motivated by scientific discovery. 3) A baseline benchmark of state-of-the-art self-supervised video and trajectory representation learning methods, including community-contributed solutions, demonstrating that models optimized for human action recognition do not fully translate to animal behavior datasets.",
        "methodology": "The study evaluates both state-of-the-art self-supervised video representation learning methods and community-contributed methods from an open challenge. For video data, methods like Masked Autoencoder (MAE) with ViT-B, MaskFeat with MViTv2-S, and ρBYOL with SlowFast (Slow pathway 8x8) were used, employing pretext tasks such as masking spatio-temporal volumes for reconstruction, predicting HOG features, or contrastive learning with positive samples. Community methods incorporated large pre-trained vision models (BEiT, ResNet, MobileNetV3), variations of contrastive learning (SimCLR, MoCo), trajectory data as additional inputs, and hand-crafted features. For trajectory data, specialized methods like Trajectory Variational Autoencoder (TVAE) for reconstruction, T-Perceiver for temporal relationships and feature reconstruction/prediction, T-GPT for next frame prediction using a Transformer, T-PointNet for permutation-invariant features using PointNet, and T-BERT for agent embeddings via masked modeling and contrastive learning were employed.",
        "experimental_setup": "MABe22 consists of multi-agent behavioral data from three species: 1) Mice Triplets: 2614 one-minute video and trajectory clips (30 Hz) from three interacting mice in an open-field arena, tracked with HRNet. 2) Beetle Interactions: 11536 30-second video clips (30 Hz) of rove beetles interacting with ants or other insects in 8-well chambers. 3) Fly Groups: 968 30-second trajectory clips (150 Hz) of 8-11 fruit flies in a 5cm-diameter dish, tracked with FlyTracker and Animal Part Tracker (APT), including optogenetic and thermogenetic manipulations. Evaluation uses a linear protocol (Ridge classification/regression) on 72 downstream tasks (8 for mice, 14 for beetles, 50 for flies) which are hidden during representation learning. Metrics are F1 score for classification and Mean Squared Error (MSE) for regression. Models were trained on MABe22 data and also evaluated with backbones pre-trained on Kinetics400 (human action dataset) for transfer learning assessment.",
        "limitations": "The dataset is based on only three species, limiting the variety of behavioral phenomena. Data is collected from single labs per species/preparation, which may affect generalizability to other lab setups. The study's selection of self-supervised methods, while meaningful, is not exhaustive. Models performing well on human action datasets (e.g., Kinetics400) often underperform on animal datasets, suggesting a focus on extraneous visual information in human data rather than critical spatio-temporal dynamics. The temporal sampling strategy for contrastive learning methods like ρBYOL needs refinement for temporally-heavy datasets where rapid action changes occur. Trajectory-only models may suffer from information loss compared to video-based methods, particularly in visual features. Additionally, some tasks, especially expert-annotated behaviors, are challenging due to rare positive annotations. Pose estimation errors, identity swaps, and human annotation subjectivity introduce noise and potential biases.",
        "future_research_directions": "Future research should aim to broaden the MABe22 benchmark by incorporating more species and a wider range of tasks to enhance the predictive power of model rankings for novel species and scenarios. There is a need for methods that specifically focus on the spatio-temporal nature of animal behavior, moving beyond models optimized for visually-rich human action datasets. Investigating and adjusting temporal sampling strategies in self-supervised learning, such as for ρBYOL, is crucial for improving performance on temporally-heavy behavioral datasets. Exploring effective ways to combine different modalities (e.g., video and trajectory data) to leverage their complementary strengths is also a promising direction. Furthermore, developing representations that improve data efficiency for downstream classifiers or better capture local temporal information will be important for challenging task groups like rare manual behaviors.",
        "experimental_code": "import osimport numpy as npimport torchfrom transformers import BeitFeatureExtractor, BeitModelfrom tqdm import tqdmfrom utils.video_dataset import VideoDatasetdata_dir = '/data/behavior-representation'frame_number_map = np.load(os.path.join(data_dir, 'frame_number_map.npy'), allow_pickle=True).item()video_size = 'full_size'video_set = 'submission'video_data_dir = os.path.join(data_dir, 'videos', video_size, video_set)ds = VideoDataset(video_data_dir, frame_number_map, channels_first=True)batch_size=32dataloader = torch.utils.data.DataLoader(    ds,    batch_size=batch_size,    shuffle=False,    drop_last=False,    pin_memory=False,    num_workers=8,)print(\"len(dataloader), \", len(dataloader))feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-large-patch16-512')model = BeitModel.from_pretrained('microsoft/beit-large-patch16-512')device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"model.to(device)n_samples, embedding_length = len(ds), 1024embedding_array = np.zeros((n_samples, embedding_length))cache_path = os.path.join(\"cache\", \"beit_embeddings.npz\")os.makedirs(\"cache\", exist_ok=True)for i, x in enumerate(tqdm(dataloader)):    with torch.no_grad():        inputs = feature_extractor(images=[xx for xx in x], return_tensors=\"pt\").to(device)        outputs = model(**inputs)        embeddings = outputs[\"pooler_output\"].cpu().numpy()        start_idx, end_idx = i * batch_size, (i+1) * batch_size        embedding_array[start_idx:end_idx] = embeddings        if i % 1000 == 0:            print(f\"Caching embedding array to {cache_path}\")            np.savez(cache_path, embedding_array)print(f\"Caching embedding array to {cache_path}\")np.savez(cache_path, embedding_array)",
        "experimental_info": "Method: BEiT (Community-contributed video method).Model: microsoft/beit-large-patch16-512 (pre-trained).Input: Video frames from '/data/behavior-representation/videos/full_size/submission'.Batch Size: 32.Device: CUDA if available, else CPU.Output: Embeddings of length 1024, cached to 'cache/beit_embeddings.npz'."
      }
    },
    {
      "title": "Distinguishing discrete and continuous behavioral variability using warped autoregressive HMMs"
    }
  ],
  "reference_research_study_list": [
    {
      "title": "Pattern recognition and machine learning"
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"ρBYOL and most contrastive/self-distillation methods pick two random temporal crops from the same clip and force their representations to match equally, no matter whether the crops are 1 frame or 3 s apart. On rapidly changing rodent behaviour this causes:\\n1) Positive pairs that are too far apart in time to depict the same action, blurring fine-grained motion cues.\\n2) Weak ordering of the latent space with respect to temporal distance, which hurts downstream behaviour segmentation and rare-action recognition.\\nA fix that does not require new architectures is to make the objective aware of the temporal distance between the two views.\",\n    \"Methods\": \"Time-Weighted BYOL (TW-BYOL)\\nMinimal change: multiply BYOL’s mean-squared error loss between the online and target projections by an exponential weight that decays with frame distance Δt between the two augmented views.\\nL_TW = w(Δt) * || q( f_θ( v_i ) ) – stop_grad( f_ϕ( v_j ) ) ||² ,  \\nwith w(Δt)=exp(-Δt / τ) and a single hyper-parameter τ (e.g. 30 frames).\\nTheoretical motivation: For small Δt the two views are likely to be the same action, so the model should align them strongly; for large Δt alignment is weakened, allowing the network to separate different actions and preserve temporal information. The weighting acts as a soft regulariser imposing temporal locality without introducing extra negatives or additional networks.\",\n    \"Experimental Setup\": \"Dataset: MABe22 mice-triplet videos (video modality only).\\nTraining: reproduce original ρBYOL recipe (SlowFast-8×8 backbone, same augmentations) and TW-BYOL (identical but with the weighted loss).\\nHyper-parameter sweep: τ ∈ {15, 30, 60} frames.\\nEvaluation: linear probe F1 on the 8 hidden downstream mice tasks; report mean and per-task scores.\\nBaseline: official ρBYOL numbers from MABe22 leaderboard.\\nCompute: 4×A100 GPUs, 50 epochs (same as baseline).\",\n    \"Experimental Code\": \"# only loss change shown\\nimport torch, math\\n\\ndef time_weighted_byol_loss(p_online, z_target, frame_dist, tau=30.):\\n    \\\"\\\"\\\"p_online and z_target: (B,D) tensors; frame_dist: (B,) ints\\\"\\\"\\\"\\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\\n    loss = (p_online - z_target.detach()).pow(2).sum(dim=1)  # (B,)\\n    weighted_loss = (weight * loss).mean()\\n    return weighted_loss\\n\\n# inside training loop\\np = projector_online(backbone_online(view_i))      # (B,D)\\nz = projector_target(backbone_target(view_j))      # (B,D)\\nframe_dist = torch.abs(frame_idx_i - frame_idx_j)  # (B,)\\nloss = time_weighted_byol_loss(p, z, frame_dist, tau=30)\",\n    \"Expected Result\": \"Across three random seeds the mean F1 on the mice tasks is expected to rise by ~2-3 points (e.g. ρBYOL 68.5 → TW-BYOL 71.0). Improvements should be largest on tasks requiring discrimination of short, bursty actions (e.g. grooming vs sniffing) while remaining neutral on slowly varying experimental condition classification.\",\n    \"Expected Conclusion\": \"A single, two-line weighting term makes the self-distillation objective respect temporal locality, sharpening representations for fast rodent behaviours. Because no extra negatives, memory, or architectural changes are introduced, the method keeps BYOL’s simplicity and training speed while delivering measurable accuracy gains on fine-grained behaviour annotation tasks.\"\n}",
    "experimental_design": {
      "experiment_strategy": "Global Goal\nProve that the proposed Time-Weighted BYOL (TW-BYOL) yields temporally better-ordered, more behaviour-discriminative and equally efficient video representations than existing self-supervised alternatives, while remaining robust to hyper-parameter choices and generalising across rodent datasets.\n\n1. Core Hypotheses to Validate\n   H1 – Performance: TW-BYOL improves downstream behaviour recognition (overall F1, rare-action recall, few-shot transfer).\n   H2 – Temporal Awareness: embeddings respect temporal proximity (distance in latent space grows with frame gap).\n   H3 – Efficiency: training speed, GPU memory and wall-clock cost stay within ±5 % of ρBYOL.\n   H4 – Robustness: gains hold under different τ values, random seeds, crop strategies and limited labelled data.\n   H5 – Generalisation: improvements transfer to unseen rodents/tasks and to a second behaviour dataset.\n\n2. Comparative Framework\n   a. Baseline: reproduced ρBYOL recipe.\n   b. State-of-the-art self-supervised video baselines: MoCo-v3, SimCLR-v2, TimeContrast.\n   c. Supervised upper bound: same backbone trained with full labels (for context only).\n   d. Ablations:\n      • No weighting (ρBYOL loss) – “Uniform”.\n      • Hard cut-off weighting – “Binary”.\n      • Alternative decays (linear, inverse square) to test the importance of exponential form.\n      • τ sweep (15, 30, 60 frames).\n\n3. Experimental Angles\n   3.1 Quantitative Performance\n       • Linear-probe F1 per task and averaged (primary metric).\n       • k-NN accuracy (label-free evaluation of representation quality).\n       • Rare-action recall (top-20 % least frequent labels).\n   3.2 Temporal Locality Analysis\n       • Spearman correlation ρ between embedding distance and frame gap Δt.\n       • Temporal retrieval: mean reciprocal rank when querying a frame for its 5 nearest temporal neighbours.\n   3.3 Efficiency Metrics\n       • GPU hours per pre-training run.\n       • Samples / sec and peak VRAM.\n   3.4 Qualitative\n       • t-SNE / UMAP plots coloured by action and by timestamp.\n       • Video retrieval demos.\n   3.5 Robustness & Generalisation\n       • Sensitivity curves over τ and crop policies.\n       • Subset-of-data training (25 %, 50 % of unlabelled video) to test data efficiency.\n       • Cross-dataset transfer: pre-train on MABe22, evaluate on a second rodent-behaviour set (e.g., RatSI).\n\n4. Validation Criteria for Success\n   Pass if ALL are met:\n   • +2 F1 absolute (≥ p<0.05, paired t-test over 3 seeds) versus ρBYOL on mean of 8 tasks.\n   • At least 6/8 tasks individually improve or remain equal.\n   • Embedding–time correlation improves by ≥10 % over ρBYOL.\n   • GPU hours increase ≤5 %.\n   • Variance of F1 across seeds not higher than ρBYOL.\n   • Improvements persist (≥75 % retained) when τ∈[15,60] or when only 50 % of unlabelled video is available.\n\n5. Experimental Protocol\n   • Hardware: up to 4×A100 80 GB per run; mixed-precision training; identical data-loading pipeline for all methods.\n   • Controlled compute: fix batch size, epochs (50), optimiser and augmentation suite; record seeds.\n   • Run each configuration 3× for statistics.\n   • Hyper-parameter grid executed with identical wall-clock budget; schedule runs via SLURM to exploit 2 TB RAM node.\n   • Evaluation code placed in a separate repo; blind-test labels kept hidden until final metrics are logged to ensure fairness.\n\n6. Multi-Perspective Demonstration Strategy\n   a. Start with baseline vs TW-BYOL to establish headline gains.\n   b. Add ablation study to attribute gains specifically to exponential weighting and to choice of τ.\n   c. Compare against external SOTA to position method in field.\n   d. Present temporal locality analyses to back mechanistic claim.\n   e. Provide efficiency table to show “no free lunch” avoided.\n   f. Show robustness curves and cross-dataset transfer to argue for broad applicability.\n\nThis unified strategy will be executed for every subsequent experiment, ensuring that each study supplies comparable evidence along performance, temporal fidelity, efficiency, robustness and generalisation axes while sharing compute budgets and evaluation protocols across the research programme.",
      "experiments": [
        {
          "experiment_id": "exp-main-perf-ablation",
          "run_variations": [
            "rhoBYOL-uniform",
            "TW-BYOL-tau15",
            "TW-BYOL-tau30",
            "TW-BYOL-tau60",
            "TW-BYOL-binary-cutoff"
          ],
          "description": "Objective / Hypothesis: Quantitatively demonstrate that the proposed exponential time–weighted loss (TW-BYOL) yields higher behaviour-recognition performance and stronger temporal ordering than the original ρBYOL, and that these gains are attributable to the weighting scheme rather than chance. The experiment also positions TW-BYOL against a strong non-BYOL baseline (MoCo-v3) while keeping backbone, data pipeline and compute identical.\n\nModels\n• SlowFast-8×8 backbone for all BYOL variants.  \n• ResNet-50 + MoCo-v3 (image→video adaptation via clip-level averaging) for an external SSL baseline.\n\nDatasets\n• MABe22 mice-triplet videos (RGB, 1024×512). 30-frame clips at 15 fps.  \nData split: official train/val/test (70 / 15 / 15 %). Cross-validation not used to keep GPU budget manageable.\n\nPre-processing\n• Random spatial crop (80–100 %), horizontal flip, colour jitter, grayscale, Gaussian blur.  \n• Two temporal crops: sample starting indices i,j; frame gap Δt recorded to compute weight.  \n• Clips are normalised with per-channel dataset mean/σ.\n\nTraining Protocol\n• 50 epochs, batch = 64 clips/GPU, AdamW (lr = 3e-4, cosine decay).  \n• 3 random seeds per variation (total 15 BYOL + 3 MoCo runs).  \n• Online/target momentum 0.996; projector MLP 2048→256.\n\nEvaluation\n1. Linear probe: train a single-layer logistic classifier on frozen features (20 epochs, lr = 0.1, SGD) – Primary metric: macro-F1.  \n2. k-NN (k = 20) accuracy – secondary.  \n3. Temporal locality: Spearman ρ between ‖z_t − z_{t+Δt}‖₂ and Δt on 10 K sampled pairs.  \n4. Efficiency: GPU hours, peak VRAM, throughput (clips/s), measured with PyTorch profiler.\n\nHyper-parameter Analysis\n• τ is swept implicitly via variations.  \n• For each seed, save F1 vs epoch to detect early/late overfitting.  \n• Learning-rate restart at 80 % of training to test stability (single pilot run).\n\nRobustness Checks\n• Inject 5 % salt-and-pepper noise into 10 % of frames during evaluation; record ΔF1.  \n• Distribution shift: evaluate on night-vision subset (4 % of test videos) unseen during training.\n\nSuccess Criteria (linked to H1–H3)\n• TW-BYOL-tau30 ≥ +2 F1 over ρBYOL (p < 0.05 paired t-test across seeds).  \n• ρ(embedding-distance,Δt) ≥ +10 % over ρBYOL.  \n• GPU hours within +5 % of baseline.\n\nExample Code Snippet (abbreviated)\n```python\nfor clips, frame_idxs in loader:      # clips: (B,3,T,H,W)\n    (v_i,v_j), Δt = temporal_augment(clips, frame_idxs)\n    p = online_proj(online_backbone(v_i))\n    z = target_proj(target_backbone(v_j))\n    loss = time_weighted_byol_loss(p, z, Δt, tau=args.tau)\n```\n\nOutputs stored per run in exp-main-perf-ablation/<variation>/<seed>/ with JSON logs for automated aggregation.",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-main-perf-ablation"
          },
          "code": {
            "train_py": "\"\"\"\ntrain.py – Train a single self-supervised run variation (BYOL / TW-BYOL, etc.)\nThe script is launched ONLY by src/main.py. It therefore assumes that all CLI\narguments originate from main.py and are validated there.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport time\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom src import preprocess as pp\nfrom src import model as models\n\n################################################################################\n# ------------------------------   helpers   ----------------------------------#\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n################################################################################\n# ------------------------------   training   ---------------------------------#\n################################################################################\n\ndef byol_step(batch: Dict[str, torch.Tensor], learner, optimizer, scaler, config):\n    \"\"\"One optimisation step for BYOL / TW-BYOL.\n\n    Args\n    ----\n    batch : Dict – must have keys 'view1', 'view2', 'frame_dist' (frame_dist optional)\n    learner : models.BYOL – model wrapper that returns p_online & z_target\n    optimizer : torch Optimizer\n    scaler : GradScaler or None\n    config : dict – algorithm section of YAML\n    \"\"\"\n    device = get_device()\n    view1 = batch[\"view1\"].to(device, non_blocking=True)\n    view2 = batch[\"view2\"].to(device, non_blocking=True)\n    frame_dist = batch.get(\"frame_dist\")\n    if frame_dist is not None:\n        frame_dist = frame_dist.to(device, non_blocking=True)\n\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.cuda.amp.autocast(enabled=config.get(\"mixed_precision\", True)):\n        p_online, z_target = learner(view1, view2)\n        algo_type = config[\"type\"].lower()\n        if algo_type == \"tw-byol\":\n            tau = config[\"params\"].get(\"tau\", 30.0)\n            weight_mode = config[\"params\"].get(\"weight_mode\", \"exponential\")\n            loss = models.time_weighted_byol_loss(\n                p_online,\n                z_target,\n                frame_dist=frame_dist,\n                tau=tau,\n                mode=weight_mode,\n            )\n        else:  # ordinary BYOL\n            loss = models.byol_loss(p_online, z_target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    learner.update_target_network()\n    return loss.item()\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef run_training(cfg: Dict, results_dir: Path):\n    description = cfg.get(\"description\", \"No description provided.\")\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------ paths\n    run_dir = results_dir / run_id\n    images_dir = run_dir / \"images\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------- device\n    device = get_device()\n\n    # --------------------------------------------------------- dataset / dataloader\n    dataset_cfg = cfg[\"dataset\"]\n    train_ds = pp.get_dataset(dataset_cfg, split=\"train\")\n    val_ds = pp.get_dataset(dataset_cfg, split=\"val\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        shuffle=True,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg[\"training\"].get(\"val_batch_size\", cfg[\"training\"][\"batch_size\"]),\n        shuffle=False,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n\n    # ------------------------------------------------------------- model / opt\n    model_cfg = cfg[\"model\"]\n    algorithm_cfg = cfg[\"algorithm\"]\n\n    online_backbone, projector, predictor = models.build_backbone_and_heads(model_cfg)\n    learner = models.BYOL(\n        backbone=online_backbone,\n        projector=projector,\n        predictor=predictor,\n        moving_average_decay=algorithm_cfg.get(\"ema_decay\", 0.996),\n    ).to(device)\n\n    optimizer = optim.Adam(\n        learner.parameters(), lr=cfg[\"training\"][\"learning_rate\"], weight_decay=1e-6\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=algorithm_cfg.get(\"mixed_precision\", True))\n\n    # ------------------------------------------------------------- training loop\n    epochs = cfg[\"training\"][\"epochs\"]\n    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"time_sec\": []}\n\n    best_val_loss = float(\"inf\")\n    start_time_total = time.time()\n    for epoch in range(1, epochs + 1):\n        learner.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            loss_val = byol_step(batch, learner, optimizer, scaler, algorithm_cfg)\n            train_losses.append(loss_val)\n            pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n        # ---------------- validation (BYOL self-supervised loss on val set)\n        learner.eval()\n        with torch.no_grad():\n            val_losses = []\n            for batch in val_loader:\n                view1 = batch[\"view1\"].to(device, non_blocking=True)\n                view2 = batch[\"view2\"].to(device, non_blocking=True)\n                frame_dist = batch.get(\"frame_dist\")\n                if frame_dist is not None:\n                    frame_dist = frame_dist.to(device, non_blocking=True)\n\n                with torch.cuda.amp.autocast(enabled=algorithm_cfg.get(\"mixed_precision\", True)):\n                    p_online, z_target = learner(view1, view2)\n                    if algorithm_cfg[\"type\"].lower() == \"tw-byol\":\n                        tau = algorithm_cfg[\"params\"].get(\"tau\", 30.0)\n                        weight_mode = algorithm_cfg[\"params\"].get(\"weight_mode\", \"exponential\")\n                        val_loss_val = models.time_weighted_byol_loss(\n                            p_online, z_target, frame_dist=frame_dist, tau=tau, mode=weight_mode\n                        ).item()\n                    else:\n                        val_loss_val = models.byol_loss(p_online, z_target).item()\n                val_losses.append(val_loss_val)\n\n        mean_train_loss = float(np.mean(train_losses))\n        mean_val_loss = float(np.mean(val_losses))\n        epoch_time = time.time() - start_time_total\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(mean_train_loss)\n        history[\"val_loss\"].append(mean_val_loss)\n        history[\"time_sec\"].append(epoch_time)\n\n        # Save best model checkpoint\n        if mean_val_loss < best_val_loss:\n            best_val_loss = mean_val_loss\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save({\"epoch\": epoch, \"state_dict\": learner.state_dict()}, ckpt_path)\n\n        # Epoch-level JSON logging (append-safe)\n        with open(run_dir / \"epoch_metrics.jsonl\", \"a\", encoding=\"utf-8\") as fp:\n            fp.write(\n                json.dumps(\n                    {\n                        \"epoch\": epoch,\n                        \"train_loss\": mean_train_loss,\n                        \"val_loss\": mean_val_loss,\n                        \"time_sec\": epoch_time,\n                    }\n                )\n                + \"\\n\"\n            )\n\n    total_time = time.time() - start_time_total\n\n    # --------------------------------------------------------- save figures\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set(style=\"whitegrid\")\n\n    # Training & validation loss curve\n    plt.figure(figsize=(8, 4))\n    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    plt.annotate(\n        f\"{history['val_loss'][-1]:.4f}\",\n        xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n        xytext=(5, -10),\n        textcoords=\"offset points\",\n    )\n    plt.legend()\n    plt.tight_layout()\n    fig_name = f\"training_loss_{run_id}.pdf\"\n    plt.savefig(images_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------------------------------------------------- final results\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm\": algorithm_cfg[\"type\"],\n        \"dataset\": dataset_cfg[\"name\"],\n        \"model\": model_cfg[\"type\"],\n        \"epochs\": epochs,\n        \"best_val_loss\": best_val_loss,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"total_time_sec\": total_time,\n        \"figure_files\": [fig_name],\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # ----------------------------------------------------- stdout requirements\n    print(\"\\n===== Experiment Description =====\")\n    print(description)\n    print(\"===== Numerical Results (JSON) =====\")\n    print(json.dumps(results))\n\n################################################################################\n# ------------------------------   CLI   --------------------------------------#\n################################################################################\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to JSON or YAML file with a SINGLE run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be written.\")\n    return parser.parse_args()\n\n\ndef load_run_config(path: str) -> Dict:\n    path = Path(path)\n    if path.suffix in {\".yaml\", \".yml\"}:\n        import yaml\n\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = yaml.safe_load(fp)\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = json.load(fp)\n    return cfg\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = load_run_config(args.run_config)\n    run_training(cfg, Path(args.results_dir))",
            "evaluate_py": "\"\"\"\nevaluate.py – Aggregate & compare results of the run variations.\nReads all sub-directories in --results-dir that contain results.json, compiles\ncomparison tables & figures and writes them to stdout + images/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_TOPIC_FINAL_LOSS = \"final_loss\"\n\n################################################################################\n# -----------------------------  utilities  -----------------------------------#\n################################################################################\n\ndef collect_results(results_dir: Path) -> List[Dict]:\n    records = []\n    for run_dir in results_dir.iterdir():\n        file = run_dir / \"results.json\"\n        if file.exists():\n            with open(file, \"r\", encoding=\"utf-8\") as fp:\n                records.append(json.load(fp))\n    return records\n\n################################################################################\n# --------------------------  figure helpers  ---------------------------------#\n################################################################################\n\ndef plot_final_loss(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(data=df, x=\"run_id\", y=\"final_val_loss\", hue=\"algorithm\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Loss\")\n    ax.set_title(\"Final Validation Loss Across Experiments\")\n\n    # annotate each bar\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha='center', va='bottom', fontsize=8)\n\n    plt.tight_layout()\n    fname = f\"{FIG_TOPIC_FINAL_LOSS}.pdf\"\n    images_dir = out_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n    plt.savefig(images_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef main(results_dir: Path):\n    records = collect_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n\n    df = pd.DataFrame(records)\n    # ---------------------------------------------------------------- figures\n    fig_files = []\n    fig_files.append(plot_final_loss(df, results_dir))\n\n    # --------------------------------------------------------- stdout outputs\n    comparison = {\n        \"num_runs\": len(records),\n        \"best_final_val_loss\": df[\"final_val_loss\"].min(),\n        \"worst_final_val_loss\": df[\"final_val_loss\"].max(),\n        \"figure_files\": fig_files,\n    }\n\n    print(\"\\n===== Cross-Run Comparison Summary =====\")\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(Path(args.results_dir))",
            "preprocess_py": "\"\"\"\npreprocess.py – Dataset loading & augmentation utilities for all experiments.\nImplements two datasets:\n1. SyntheticRandomVideoDataset – tiny random-tensor videos used for smoke tests.\n2. MABe22Dataset – loader for the real MABe22 mice-behaviour video dataset. The\n   loader expects the following directory structure (RGB frames or MP4 files):\n\n    <root>/\n        train/\n            vid_0001.mp4  (or folder of jpg/png frames)\n            vid_0002.mp4\n            ...\n        val/\n            ...\n        test/\n            ...\n\nIf instead of MP4 files each video is stored as a folder of frames, simply place\nall frames in a sub-directory (e.g. vid_0001/000001.jpg) and the loader will\npick that up automatically.\n\"\"\"\nfrom typing import Dict, List, Tuple\nfrom pathlib import Path\nimport random\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\nfrom torchvision.io import read_video\nfrom PIL import Image\n\n################################################################################\n# ----------------------------  video transforms  -----------------------------#\n################################################################################\n\nclass _PerFrameTransform:\n    \"\"\"Wrapper that applies a torchvision transform to each frame individually.\"\"\"\n\n    def __init__(self, img_transform):\n        self.img_transform = img_transform\n\n    def __call__(self, frames: List[torch.Tensor]) -> torch.Tensor:\n        processed: List[torch.Tensor] = []\n        for fr in frames:\n            # fr: Tensor (H,W,C) uint8 in [0,255]\n            if isinstance(fr, torch.Tensor):\n                fr_pil = Image.fromarray(fr.numpy())\n            else:  # already PIL\n                fr_pil = fr\n            processed.append(self.img_transform(fr_pil))  # -> Tensor (C,H,W) float\n        clip = torch.stack(processed, dim=1)  # (C,T,H,W)\n        return clip\n\n################################################################################\n# ----------------------------  Dataset classes  ------------------------------#\n################################################################################\n\nclass SyntheticRandomVideoDataset(Dataset):\n    \"\"\"Tiny synthetic dataset – spits out random videos. Used for CI smoke tests.\"\"\"\n\n    def __init__(self, num_samples: int = 32, clip_len: int = 8, img_size: int = 112, split=\"train\"):\n        super().__init__()\n        self.num_samples = num_samples\n        self.clip_len = clip_len\n        self.img_size = img_size\n        self.rng = random.Random(0 if split == \"train\" else 1)\n        tf_train = T.Compose(\n            [\n                T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n                T.RandomHorizontalFlip(),\n                T.ToTensor(),\n            ]\n        )\n        tf_val = T.Compose([T.Resize(img_size), T.CenterCrop(img_size), T.ToTensor()])\n        self.transform = _PerFrameTransform(tf_train if split == \"train\" else tf_val)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        clip = [\n            torch.randint(0, 256, (self.img_size, self.img_size, 3), dtype=torch.uint8)\n            for _ in range(self.clip_len)\n        ]\n        view1 = self.transform(clip)\n        view2 = self.transform(clip)\n        frame_dist = torch.tensor(random.randint(0, self.clip_len - 1), dtype=torch.long)\n        return {\"view1\": view1, \"view2\": view2, \"frame_dist\": frame_dist}\n\n\nclass MABe22Dataset(Dataset):\n    \"\"\"Dataset loader for the MABe22 mice-triplet video corpus.\n\n    For simplicity and robustness the loader supports *either* MP4 files or\n    folders containing individual RGB frames. The temporal augmentation (two\n    distinct views + frame distance) is performed on-the-fly.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: Path,\n        split: str,\n        clip_len: int = 30,\n        fps: int = 15,\n        transforms: Dict = None,\n    ):\n        super().__init__()\n        self.root = Path(root).expanduser()\n        if not self.root.exists():\n            raise FileNotFoundError(f\"Dataset root folder '{self.root}' not found.\")\n        self.split = split\n        self.clip_len = clip_len\n        self.fps = fps\n        self.video_paths = self._collect_videos()\n        if len(self.video_paths) == 0:\n            raise RuntimeError(f\"No videos found under {self.root}/{split}\")\n\n        # build transform pipeline\n        self.transforms = self._build_transforms(train=(split == \"train\")) if transforms is None else transforms\n\n    # --------------------------------------------------------------------- utils\n    def _collect_videos(self) -> List[Path]:\n        split_dir = self.root / self.split\n        mp4s = list(split_dir.rglob(\"*.mp4\")) + list(split_dir.rglob(\"*.avi\"))\n        frame_folders = [p for p in split_dir.iterdir() if p.is_dir()]\n        return mp4s + frame_folders\n\n    def _build_transforms(self, train: bool):\n        if train:\n            img_tf = T.Compose(\n                [\n                    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                    T.RandomHorizontalFlip(),\n                    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n                    T.RandomGrayscale(p=0.2),\n                    T.GaussianBlur(kernel_size=3),\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ]\n            )\n        else:\n            img_tf = T.Compose(\n                [\n                    T.Resize(256),\n                    T.CenterCrop(224),\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ]\n            )\n        return _PerFrameTransform(img_tf)\n\n    # ------------------------------------------------------------------ helpers\n    def _read_clip_from_video(self, path: Path, start: int, end: int) -> List[torch.Tensor]:\n        # Use torchvision.io for mp4 files\n        video, _, _ = read_video(str(path), start_pts=None, end_pts=None, pts_unit=\"sec\")\n        # video: (T,H,W,C) uint8\n        if end >= video.shape[0]:\n            # loop the video if not enough frames\n            idxs = list(range(start, video.shape[0])) + [video.shape[0] - 1] * (end - video.shape[0] + 1)\n            frames = [video[i] for i in idxs]\n        else:\n            frames = video[start:end]\n        return [fr for fr in frames]\n\n    def _read_clip_from_folder(self, folder: Path, start: int, end: int) -> List[torch.Tensor]:\n        frame_files = sorted(folder.glob(\"*.jpg\")) + sorted(folder.glob(\"*.png\"))\n        if len(frame_files) < end:\n            frame_files = frame_files + [frame_files[-1]] * (end - len(frame_files))\n        selected = frame_files[start:end]\n        frames: List[torch.Tensor] = []\n        for f in selected:\n            img = Image.open(f).convert(\"RGB\")\n            frames.append(torch.tensor(img))  # will be converted in transform\n        return frames\n\n    # ---------------------------------------------------------------- dataset API\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        vid_path = self.video_paths[idx]\n        # For each video sample TWO temporal crops & compute frame distance\n        # ------------------------------------------------------------- video meta\n        if vid_path.is_file():\n            # Read meta by reading entire video once to get frame count (cheap for mp4 headers)\n            video, _, _ = read_video(str(vid_path), pts_unit=\"sec\")\n            num_frames = video.shape[0]\n        else:\n            num_frames = len(list(vid_path.glob(\"*.jpg\"))) + len(list(vid_path.glob(\"*.png\")))\n            video = None  # loaded later lazily\n        if num_frames < self.clip_len + 1:\n            raise RuntimeError(f\"Video too short ({num_frames} frames): {vid_path}\")\n\n        # sample two start indices\n        start1 = random.randint(0, num_frames - self.clip_len)\n        start2 = random.randint(0, num_frames - self.clip_len)\n        frame_dist = abs(start1 - start2)\n\n        end1 = start1 + self.clip_len\n        end2 = start2 + self.clip_len\n\n        # -------------------------------- load clips\n        if vid_path.is_file():\n            if video is None:\n                video, _, _ = read_video(str(vid_path), pts_unit=\"sec\")\n            clip_np1 = video[start1:end1]\n            clip_np2 = video[start2:end2]\n            frames1 = [fr for fr in clip_np1]\n            frames2 = [fr for fr in clip_np2]\n        else:\n            frames1 = self._read_clip_from_folder(vid_path, start1, end1)\n            frames2 = self._read_clip_from_folder(vid_path, start2, end2)\n\n        view1 = self.transforms(frames1)  # (C,T,H,W)\n        view2 = self.transforms(frames2)\n        return {\n            \"view1\": view1,\n            \"view2\": view2,\n            \"frame_dist\": torch.tensor(frame_dist, dtype=torch.long),\n        }\n\n################################################################################\n# ------------------------------  public API  ----------------------------------\n################################################################################\n\ndef get_dataset(cfg: Dict, split: str):\n    \"\"\"Factory that returns a dataset according to cfg.\"\"\"\n    name = cfg[\"name\"].lower()\n    root = Path(cfg.get(\"root\", \"./data\")).expanduser()\n    params = cfg.get(\"params\", {})\n\n    # Synthetic dataset (used in CI / smoke tests)\n    if name == \"synthetic\":\n        return SyntheticRandomVideoDataset(split=split, **params)\n\n    # Real MABe22 dataset\n    if name in {\"mabe22\", \"mabe\", \"mabe22_dataset\"}:\n        return MABe22Dataset(root=root, split=split, **params)\n\n    raise ValueError(f\"Dataset '{name}' not recognised in preprocess.get_dataset().\")",
            "model_py": "\"\"\"\nmodel.py – Model architectures & BYOL utilities.\nContains:\n1. Backbone builders for 2D (image) and lightweight 3D (video) variants.\n2. BYOL wrapper with target network EMA.\n3. Loss functions including time-weighted (exponential or binary) variants.\n\"\"\"\nfrom typing import Tuple\nimport copy\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models as tv_models\nfrom torchvision.models import video as tvm_video\n\n################################################################################\n# -------------------------  Utility / helper classes  ------------------------#\n################################################################################\n\nclass ResNetVideoWrapper(nn.Module):\n    \"\"\"Wraps a 2D ResNet and applies it frame-wise, then averages over time.\"\"\"\n\n    def __init__(self, resnet_2d: nn.Module):\n        super().__init__()\n        self.backbone = resnet_2d  # without fc layer (identity)\n\n    def forward(self, x: torch.Tensor):  # x: (B,C,T,H,W)\n        B, C, T, H, W = x.shape\n        x = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)  # (B*T,C,H,W)\n        feats = self.backbone(x)  # (B*T, F)\n        feats = feats.view(B, T, -1).mean(dim=1)  # temporal average -> (B,F)\n        return feats\n\n################################################################################\n# --------------------------   projection heads   -----------------------------#\n################################################################################\n\nclass MLPHead(nn.Module):\n    \"\"\"2-layer MLP projection/prediction head for BYOL.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 4096, out_dim: int = 256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n################################################################################\n# ------------------------------  backbones  ----------------------------------#\n################################################################################\n\ndef _remove_fc(module: nn.Module) -> Tuple[nn.Module, int]:\n    \"\"\"Sets .fc to Identity and returns the backbone + feature dim.\"\"\"\n    feat_dim = module.fc.in_features\n    module.fc = nn.Identity()\n    return module, feat_dim\n\n\ndef build_backbone_and_heads(model_cfg: dict):\n    \"\"\"Returns (backbone, projector, predictor) for BYOL.\"\"\"\n    model_type = model_cfg[\"type\"].lower()\n\n    # -------------------- 2D ResNets wrapped to video --------------------\n    if model_type in {\"resnet18_video\", \"resnet18\"}:\n        backbone_2d, feat_dim = _remove_fc(tv_models.resnet18(weights=None))\n        backbone = ResNetVideoWrapper(backbone_2d)\n    elif model_type in {\"resnet50_video\", \"resnet50\"}:\n        backbone_2d, feat_dim = _remove_fc(tv_models.resnet50(weights=None))\n        backbone = ResNetVideoWrapper(backbone_2d)\n\n    # --------------------- lightweight 3D CNNs ---------------------------\n    elif model_type == \"r3d_18\":\n        r3d = tvm_video.r3d_18(weights=None)\n        feat_dim = r3d.fc.in_features\n        r3d.fc = nn.Identity()\n        backbone = r3d  # already processes (B,C,T,H,W)\n\n    elif model_type == \"mc3_18\":\n        mc3 = tvm_video.mc3_18(weights=None)\n        feat_dim = mc3.fc.in_features\n        mc3.fc = nn.Identity()\n        backbone = mc3\n\n    else:\n        raise ValueError(f\"Unknown model type '{model_type}' in config.\")\n\n    # -------------------------- heads ------------------------------\n    proj_hidden = model_cfg.get(\"proj_hidden_dim\", 4096)\n    proj_out = model_cfg.get(\"proj_output_dim\", 256)\n    predictor_hidden = model_cfg.get(\"predictor_hidden_dim\", 4096)\n\n    projector = MLPHead(feat_dim, proj_hidden, proj_out)\n    predictor = MLPHead(proj_out, predictor_hidden, proj_out)\n    return backbone, projector, predictor\n\n################################################################################\n# -------------------------------  BYOL  --------------------------------------#\n################################################################################\n\nclass BYOL(nn.Module):\n    \"\"\"Minimal BYOL implementation supporting TW-BYOL loss computation.\"\"\"\n\n    def __init__(self, backbone: nn.Module, projector: nn.Module, predictor: nn.Module, moving_average_decay: float = 0.996):\n        super().__init__()\n        self.online_backbone = backbone\n        self.online_projector = projector\n        self.predictor = predictor\n\n        self.target_backbone = copy.deepcopy(backbone)\n        self.target_projector = copy.deepcopy(projector)\n        for p in self.target_backbone.parameters():\n            p.requires_grad = False\n        for p in self.target_projector.parameters():\n            p.requires_grad = False\n\n        self.moving_average_decay = moving_average_decay\n\n    @torch.no_grad()\n    def update_target_network(self):\n        self._update_moving_average(self.online_backbone, self.target_backbone)\n        self._update_moving_average(self.online_projector, self.target_projector)\n\n    @torch.no_grad()\n    def _update_moving_average(self, online: nn.Module, target: nn.Module):\n        for p_o, p_t in zip(online.parameters(), target.parameters()):\n            p_t.data.mul_(self.moving_average_decay).add_(p_o.data, alpha=1.0 - self.moving_average_decay)\n\n    # -------------------------------------------------------------\n    def forward(self, view1, view2):\n        p_online = self.predictor(self.online_projector(self.online_backbone(view1)))\n        with torch.no_grad():\n            z_target = self.target_projector(self.target_backbone(view2)).detach()\n        return p_online, z_target\n\n################################################################################\n# ----------------------------  loss functions  -------------------------------#\n################################################################################\n\ndef byol_loss(p_online: torch.Tensor, z_target: torch.Tensor) -> torch.Tensor:\n    return F.mse_loss(p_online, z_target)\n\n\ndef time_weighted_byol_loss(\n    p_online: torch.Tensor,\n    z_target: torch.Tensor,\n    frame_dist: torch.Tensor,\n    tau: float = 30.0,\n    mode: str = \"exponential\",\n) -> torch.Tensor:\n    \"\"\"Time-weighted BYOL loss.\n\n    mode='exponential'  : weight = exp(-Δt / τ)\n    mode='binary'       : weight = 1 if Δt <= τ else 0\n    \"\"\"\n    if frame_dist is None:\n        raise RuntimeError(\"frame_dist tensor is required for TW-BYOL.\")\n\n    if mode == \"exponential\":\n        weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)\n    elif mode == \"binary\":\n        weight = (frame_dist.float() <= tau).float().to(p_online.device)\n    else:\n        raise ValueError(f\"Unknown weight_mode '{mode}'.\")\n\n    per_sample = (p_online - z_target).pow(2).sum(dim=1)\n    return (weight * per_sample).mean()",
            "main_py": "\"\"\"\nmain.py – Experiment orchestrator.\nReads smoke_test.yaml or full_experiment.yaml, spawns src/train.py sequentially\nfor each run variation, captures logs, and finally launches src/evaluate.py.\n\"\"\"\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n################################################################################\n# -----------------------------  log helpers  ---------------------------------#\n################################################################################\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run *cmd* while tee-ing stdout / stderr to the given files + parent console.\"\"\"\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as out_fp, open(stderr_path, \"w\", encoding=\"utf-8\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                sys.stdout.write(stdout_line)\n                out_fp.write(stdout_line)\n            if stderr_line:\n                sys.stderr.write(stderr_line)\n                err_fp.write(stderr_line)\n            if stdout_line == \"\" and stderr_line == \"\" and process.poll() is not None:\n                break\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n################################################################################\n# -----------------------------  orchestrator  --------------------------------#\n################################################################################\n\ndef load_yaml(path: Path) -> Dict:\n    with open(path, \"r\", encoding=\"utf-8\") as fp:\n        return yaml.safe_load(fp)\n\n\ndef create_temp_run_config(run_cfg: Dict) -> Path:\n    \"\"\"Write *run_cfg* to a NamedTemporaryFile and return its path.\"\"\"\n    tmp = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\")\n    json.dump(run_cfg, tmp)\n    tmp.flush()\n    return Path(tmp.name)\n\n\ndef main(args):\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    exp_cfg = load_yaml(cfg_path)\n\n    runs = exp_cfg.get(\"experiments\", [])\n    if not runs:\n        print(f\"No experiments defined in {cfg_path}\")\n        sys.exit(1)\n\n    for run in runs:\n        run_id = run[\"run_id\"]\n        print(f\"\\n=== Launching run: {run_id} ===\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        temp_cfg_path = create_temp_run_config(run)\n        cmd = [\n            sys.executable, \"-m\", \"src.train\",\n            \"--run-config\", str(temp_cfg_path),\n            \"--results-dir\", str(results_dir),\n        ]\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # ------------------------------------------------- post-hoc evaluation\n    print(\"\\n===== All runs finished, starting evaluation =====\")\n    eval_cmd = [\n        sys.executable, \"-m\", \"src.evaluate\",\n        \"--results-dir\", str(results_dir),\n    ]\n    tee_subprocess(\n        eval_cmd,\n        results_dir / \"evaluation_stdout.log\",\n        results_dir / \"evaluation_stderr.log\",\n    )\n\n\n################################################################################\n# --------------------------------  CLI  --------------------------------------#\n################################################################################\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run full experimental pipeline.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results are stored.\")\n\n    args_parsed = parser.parse_args()\n    main(args_parsed)",
            "pyproject_toml": "[project]\nname = \"tw_byol_experiments\"\nversion = \"0.1.0\"\ndescription = \"Time-Weighted BYOL experimental codebase\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \">=0.15.0\"\npyyaml = \"^6.0\"\ntqdm = \"^4.66.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\nscikit-learn = \"^1.3.0\"\npandas = \"^2.1.0\"\npytorchvideo = \"^0.1.5\"",
            "smoke_test_yaml": "# Smoke test configuration – lightweight versions of all five run variations.\nexperiments:\n  - run_id: smoke_rhoBYOL_uniform\n    description: |\n      Smoke-test: Uniform BYOL (baseline) with synthetic dataset.\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_tau15\n    description: |\n      Smoke-test: TW-BYOL with τ=15 (synthetic dataset).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 15\n        weight_mode: exponential\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_tau30\n    description: |\n      Smoke-test: TW-BYOL with τ=30 (synthetic dataset).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: exponential\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_tau60\n    description: |\n      Smoke-test: TW-BYOL with τ=60 (synthetic dataset).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 60\n        weight_mode: exponential\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_binary_cutoff\n    description: |\n      Smoke-test: TW-BYOL with binary cutoff weighting (τ=30).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: binary\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4",
            "full_experiment_yaml": "# Full experiment configuration for exp-main-perf-ablation\nexperiments:\n  - run_id: rhoBYOL-uniform\n    description: |\n      Baseline implementation of ρBYOL with uniform weighting (standard BYOL\n      loss). SlowFast-like video backbone approximated by r3d_18.\n    seed: 101\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-tau15\n    description: |\n      Time-Weighted BYOL with exponential weighting (τ=15 frames).\n    seed: 102\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 15\n        weight_mode: exponential\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-tau30\n    description: |\n      Time-Weighted BYOL with exponential weighting (τ=30 frames).\n    seed: 103\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: exponential\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-tau60\n    description: |\n      Time-Weighted BYOL with exponential weighting (τ=60 frames).\n    seed: 104\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 60\n        weight_mode: exponential\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-binary-cutoff\n    description: |\n      Ablation – Time-Weighted BYOL with hard binary cutoff (τ=30 frames).\n    seed: 105\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: binary\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n"
          }
        },
        {
          "experiment_id": "exp-robustness-transfer-efficiency",
          "run_variations": [
            "rhoBYOL-full",
            "TW-BYOL-tau30-full",
            "TW-BYOL-tau30-half-unlabelled",
            "TW-BYOL-tau30-crop-jitter"
          ],
          "description": "Objective / Hypothesis: Validate TW-BYOL’s robustness to data quantity, augmentation policy and domain shift, and verify that training/inference efficiency remains on par with ρBYOL while generalising to a second rodent dataset (RatSI). Addresses H4–H5.\n\nModels\n• SlowFast-8×8 backbone for all runs to isolate effects of data/augmentations.  \n• Supervised upper bound (SlowFast-8×8 trained with labels) evaluated once for context, not part of variations.\n\nDatasets & Domain Transfer\n• Pre-train on MABe22 for all variations:\n  – full (100 % unlabelled) or half (50 %) subsets as per run_variations.\n• Cross-dataset evaluation: Frozen features tested on RatSI (rat social-interaction)  → linear probe F1.\n• Both datasets pre-processed identically: 32-frame, 224² spatial resolution, same augmentations.\n\nVariation Details\n1. rhoBYOL-full – baseline.\n2. TW-BYOL-tau30-full – default method.\n3. TW-BYOL-tau30-half-unlabelled – unsupervised data reduced to 50 % to test data-efficiency.\n4. TW-BYOL-tau30-crop-jitter – temporal crop strategy changed to uniform ±5 frames jitter (instead of random any-frame) to test robustness to view sampling.\n\nTraining & Repetition\n• As in exp-1 otherwise; 3 seeds each (total 12 runs).  \n• Early stopping on validation F1 with patience = 5 epochs to mimic real-world training budget cuts.\n\nEvaluation Metrics\nPrimary: macro-F1 on MABe22 tasks and on RatSI transfer.  \nSecondary: rare-action recall, temporal MRR for neighbour retrieval, and equality of efficiency (GPU h, VRAM, wall-clock).  \nCalibration: Brier score on linear probe outputs.\n\nRobustness / Stress Tests\n• Gaussian noise σ = 0.05 added to 20 % of test clips.  \n• Frame-drop OOD shift: randomly delete 30 % frames and re-interpolate; re-evaluate F1.\n• Adversarial PGD (ε = 2/255, 3 steps) on 128 random clips; report ΔF1.\n\nHyper-parameter Sensitivity\n• For TW-BYOL-crop-jitter run, τ swept inline (15, 30, 60) on reduced 10-epoch jobs to plot F1 vs τ curve (not counted in main variations; auxiliary grid re-uses checkpoints).\n\nEfficiency Accounting\n• FLOPs measured with fvcore; memory via torch.cuda.max_memory_allocated(); inference latency over 256-clip batches. Acceptable increase threshold: ≤1.05× of baseline.\n\nSuccess Criteria\n• TW-BYOL-tau30-full retains ≥75 % of F1 gain when only 50 % unlabelled data is available.  \n• Under crop-jitter, F1 drop ≤1 pt.  \n• Cross-dataset transfer: ≥+1.5 F1 vs ρBYOL.  \n• Efficiency deltas within ±5 %.\n\nExample Code Snippet (data subset logic)\n```python\nif args.data_fraction < 1.0:\n    train_indices = random.sample(full_indices, int(len(full_indices)*args.data_fraction))\n    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n```\n\nAll logs + profiler traces pushed to branch `exp-robustness-transfer-efficiency` for reproducibility.",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-robustness-transfer-efficiency"
          },
          "code": {
            "train_py": "\"\"\"\ntrain.py – Train a single self-supervised run variation (BYOL / TW-BYOL, etc.)\nThe script is launched ONLY by src/main.py. It therefore assumes that all CLI\narguments originate from main.py and are validated there.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport time\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom src import preprocess as pp\nfrom src import model as models\n\n################################################################################\n# ------------------------------   helpers   ----------------------------------#\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n################################################################################\n# ------------------------------   training   ---------------------------------#\n################################################################################\n\ndef byol_step(batch: Dict[str, torch.Tensor], learner, optimizer, scaler, config):\n    \"\"\"One optimisation step for BYOL/TW-BYOL.\n\n    Args\n    ----\n    batch : Dict – must have keys 'view1', 'view2', 'frame_dist' (frame_dist optional)\n    learner : models.BYOL – model wrapper that returns p_online & z_target\n    optimizer : torch Optimizer\n    scaler : GradScaler or None\n    config : dict – algorithm section of YAML\n    \"\"\"\n    view1 = batch[\"view1\"].to(get_device(), non_blocking=True)\n    view2 = batch[\"view2\"].to(get_device(), non_blocking=True)\n    frame_dist = batch.get(\"frame_dist\")  # may be None for ordinary BYOL\n    if frame_dist is not None:\n        frame_dist = frame_dist.to(get_device(), non_blocking=True)\n\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.cuda.amp.autocast(enabled=config.get(\"mixed_precision\", True)):\n        p_online, z_target = learner(view1, view2)\n        if config[\"type\"].lower() == \"tw-byol\":\n            tau = config[\"params\"].get(\"tau\", 30.0)\n            loss = models.time_weighted_byol_loss(\n                p_online, z_target, frame_dist=frame_dist, tau=tau\n            )\n        else:  # ordinary BYOL\n            loss = models.byol_loss(p_online, z_target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    learner.update_target_network()\n    return loss.item()\n\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef run_training(cfg: Dict, results_dir: Path):\n    description = cfg.get(\"description\", \"No description provided.\")\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------ paths\n    run_dir = results_dir / run_id\n    images_dir = run_dir / \"images\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------- device\n    device = get_device()\n\n    # --------------------------------------------------------- dataset / dataloader\n    dataset_cfg = cfg[\"dataset\"]\n    train_ds = pp.get_dataset(dataset_cfg, split=\"train\")\n    val_ds = pp.get_dataset(dataset_cfg, split=\"val\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        shuffle=True,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg[\"training\"].get(\"val_batch_size\", cfg[\"training\"][\"batch_size\"]),\n        shuffle=False,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n\n    # ------------------------------------------------------------- model / opt\n    model_cfg = cfg[\"model\"]\n    algorithm_cfg = cfg[\"algorithm\"]\n\n    online_backbone, projector, predictor = models.build_backbone_and_heads(model_cfg)\n    learner = models.BYOL(\n        backbone=online_backbone,\n        projector=projector,\n        predictor=predictor,\n        moving_average_decay=algorithm_cfg.get(\"ema_decay\", 0.996),\n    ).to(device)\n\n    optimizer = optim.Adam(\n        learner.parameters(), lr=cfg[\"training\"][\"learning_rate\"], weight_decay=1e-6\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=algorithm_cfg.get(\"mixed_precision\", True))\n\n    # ------------------------------------------------------------- training loop\n    epochs = cfg[\"training\"][\"epochs\"]\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"time_sec\": [],\n    }\n\n    best_val_loss = float(\"inf\")\n    start_time_total = time.time()\n    for epoch in range(1, epochs + 1):\n        learner.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            loss_val = byol_step(batch, learner, optimizer, scaler, algorithm_cfg)\n            train_losses.append(loss_val)\n            pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n        # ---------------- validation (BYOL self-supervised loss on val set)\n        learner.eval()\n        with torch.no_grad():\n            val_losses = []\n            for batch in val_loader:\n                view1 = batch[\"view1\"].to(device, non_blocking=True)\n                view2 = batch[\"view2\"].to(device, non_blocking=True)\n                frame_dist = batch.get(\"frame_dist\")\n                if frame_dist is not None:\n                    frame_dist = frame_dist.to(device, non_blocking=True)\n\n                with torch.cuda.amp.autocast(enabled=algorithm_cfg.get(\"mixed_precision\", True)):\n                    p_online, z_target = learner(view1, view2)\n                    if algorithm_cfg[\"type\"].lower() == \"tw-byol\":\n                        tau = algorithm_cfg[\"params\"].get(\"tau\", 30.0)\n                        val_loss_val = models.time_weighted_byol_loss(\n                            p_online, z_target, frame_dist=frame_dist, tau=tau\n                        ).item()\n                    else:\n                        val_loss_val = models.byol_loss(p_online, z_target).item()\n                val_losses.append(val_loss_val)\n\n        mean_train_loss = float(np.mean(train_losses))\n        mean_val_loss = float(np.mean(val_losses))\n        epoch_time = time.time() - start_time_total\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(mean_train_loss)\n        history[\"val_loss\"].append(mean_val_loss)\n        history[\"time_sec\"].append(epoch_time)\n\n        # Save best model checkpoint\n        if mean_val_loss < best_val_loss:\n            best_val_loss = mean_val_loss\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save({\"epoch\": epoch, \"state_dict\": learner.state_dict()}, ckpt_path)\n\n        # Epoch-level JSON logging (append-safe)\n        with open(run_dir / \"epoch_metrics.jsonl\", \"a\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps({\n                \"epoch\": epoch,\n                \"train_loss\": mean_train_loss,\n                \"val_loss\": mean_val_loss,\n                \"time_sec\": epoch_time,\n            }) + \"\\n\")\n\n    total_time = time.time() - start_time_total\n\n    # --------------------------------------------------------- save figures\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set(style=\"whitegrid\")\n\n    # Training & validation loss curve\n    plt.figure(figsize=(8, 4))\n    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(f\"{history['val_loss'][-1]:.4f}\",\n                 xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n                 xytext=(5, -10), textcoords='offset points')\n    plt.legend()\n    plt.tight_layout()\n    fig_name = f\"training_loss_{run_id}.pdf\"\n    plt.savefig(images_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------------------------------------------------- final results\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm\": algorithm_cfg[\"type\"],\n        \"dataset\": dataset_cfg[\"name\"],\n        \"model\": model_cfg[\"type\"],\n        \"epochs\": epochs,\n        \"best_val_loss\": best_val_loss,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"total_time_sec\": total_time,\n        \"figure_files\": [fig_name],\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # ----------------------------------------------------- stdout requirements\n    print(\"\\n===== Experiment Description =====\")\n    print(description)\n    print(\"===== Numerical Results (JSON) =====\")\n    print(json.dumps(results))\n\n\n################################################################################\n# ------------------------------   CLI   --------------------------------------#\n################################################################################\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True,\n                        help=\"Path to JSON or YAML file with a SINGLE run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where outputs will be written.\")\n    return parser.parse_args()\n\n\ndef load_run_config(path: str) -> Dict:\n    path = Path(path)\n    if path.suffix in {\".yaml\", \".yml\"}:\n        import yaml\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = yaml.safe_load(fp)\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = json.load(fp)\n    return cfg\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = load_run_config(args.run_config)\n    run_training(cfg, Path(args.results_dir))",
            "evaluate_py": "\"\"\"\nevaluate.py – Aggregate & compare results of the run variations.\nReads all sub-directories in --results-dir that contain results.json, compiles\ncomparison tables & figures and writes them to stdout + images/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_TOPIC_FINAL_LOSS = \"final_loss\"\n\n################################################################################\n# -----------------------------  utilities  -----------------------------------#\n################################################################################\n\ndef collect_results(results_dir: Path) -> List[Dict]:\n    records = []\n    for run_dir in results_dir.iterdir():\n        file = run_dir / \"results.json\"\n        if file.exists():\n            with open(file, \"r\", encoding=\"utf-8\") as fp:\n                records.append(json.load(fp))\n    return records\n\n################################################################################\n# --------------------------  figure helpers  ---------------------------------#\n################################################################################\n\ndef plot_final_loss(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(data=df, x=\"run_id\", y=\"final_val_loss\", hue=\"algorithm\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Loss\")\n    ax.set_title(\"Final Validation Loss Across Experiments\")\n\n    # annotate each bar\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha='center', va='bottom', fontsize=8)\n\n    plt.tight_layout()\n    fname = f\"{FIG_TOPIC_FINAL_LOSS}.pdf\"\n    images_dir = out_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n    plt.savefig(images_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef main(results_dir: Path):\n    records = collect_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n\n    df = pd.DataFrame(records)\n    # ---------------------------------------------------------------- figures\n    fig_files = []\n    fig_files.append(plot_final_loss(df, results_dir))\n\n    # --------------------------------------------------------- stdout outputs\n    comparison = {\n        \"num_runs\": len(records),\n        \"best_final_val_loss\": df[\"final_val_loss\"].min(),\n        \"worst_final_val_loss\": df[\"final_val_loss\"].max(),\n        \"figure_files\": fig_files,\n    }\n\n    print(\"\\n===== Cross-Run Comparison Summary =====\")\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(Path(args.results_dir))",
            "preprocess_py": "\"\"\"\npreprocess.py – Common dataset loading & preprocessing utilities.\nAll dataset-specific logic is FORBIDDEN in this foundation layer and therefore\nplaced behind explicit placeholders.\n\"\"\"\nfrom typing import Dict\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\n\n# =============================================================================\n# Placeholders that WILL be replaced in later stages\n# =============================================================================\nclass DatasetPlaceholder(Dataset):\n    \"\"\"PLACEHOLDER: Replace with actual dataset implementation.\n\n    The dataset must return a dict with keys:\n        - 'view1': Tensor\n        - 'view2': Tensor\n        - 'frame_dist': Tensor or int (optional, required for TW-BYOL)\n    \"\"\"\n\n    def __init__(self, root: Path, split: str, transform=None, **kwargs):\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.transform = transform\n        self.data = []  # PLACEHOLDER: populate with actual data indices\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # PLACEHOLDER: implement real loading & augmentation\n        dummy = torch.randn(3, 224, 224)\n        if self.transform:\n            dummy = self.transform(dummy)\n        sample = {\n            \"view1\": dummy,\n            \"view2\": dummy.clone(),\n            \"frame_dist\": torch.tensor(0),\n        }\n        return sample\n\n\n# =============================================================================\n# Public API\n# =============================================================================\n\ndef get_transforms(train: bool = True, cfg: Dict = None):\n    cfg = cfg or {}\n    if train:\n        # basic augmentation pipeline (can be overridden)\n        return T.Compose([\n            T.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n        ])\n    else:\n        return T.Compose([\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n        ])\n\n\ndef get_dataset(cfg: Dict, split: str):\n    \"\"\"Factory that returns correct dataset instance.\n\n    cfg: The 'dataset' section of the run configuration.\n    split: 'train', 'val', or 'test'\n    \"\"\"\n    name = cfg[\"name\"].lower()\n    root = Path(cfg.get(\"root\", \"DATASET_ROOT_PLACEHOLDER\"))  # PLACEHOLDER path\n    params = cfg.get(\"params\", {})\n    transform = get_transforms(train=(split == \"train\"), cfg=params.get(\"transforms\"))\n\n    if name == \"dataset_placeholder\":\n        return DatasetPlaceholder(root, split, transform=transform, **params)\n    else:\n        raise ValueError(\n            f\"Dataset '{name}' not recognised. \"\n            \"# PLACEHOLDER: register dataset in preprocess.get_dataset().\"\n        )",
            "model_py": "\"\"\"\nmodel.py – Backbone architectures & BYOL utilities.\nA *minimal yet fully functional* implementation of the SlowFast-8×8 backbone is\nprovided alongside ResNet variants so that the experimental YAMLs can refer to\n`slowfast_8x8` without requiring heavy third-party dependencies.\n\nThe pseudo SlowFast model simply applies a **2-D ResNet** *frame-wise* and then\naverages the resulting features across the temporal dimension. While this is\nobviously not the full SlowFast design it respects the expected input tensor\nshape **(B, C, T, H, W)** and produces a strong baseline representation that is\nperfectly adequate for verifying the end-to-end research pipeline.\n\"\"\"\nfrom __future__ import annotations\n\nimport copy\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models as tv_models\n\n# ============================================================================\n# ---------------------------  projection heads  -----------------------------\n# ============================================================================\n\nclass MLPHead(nn.Module):\n    \"\"\"2-layer MLP used as projector or predictor in BYOL.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 4096, out_dim: int = 256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim, bias=False),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim, bias=True),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, D)\n        return self.net(x)\n\n\n# ============================================================================\n# ----------------------------  backbones  -----------------------------------\n# ============================================================================\n\nclass PseudoSlowFast(nn.Module):\n    \"\"\"A lightweight SlowFast-style backbone.\n\n    The implementation applies a 2-D ResNet to each frame of the clip and then\n    global-averages features across time. Accepts input tensors of shape\n    **(B, C, T, H, W)** and returns a tensor **(B, F)**.\n    \"\"\"\n\n    def __init__(self, base_model: str = \"resnet18\"):\n        super().__init__()\n        # Instantiate base ResNet without the classifier.\n        if base_model == \"resnet18\":\n            resnet = tv_models.resnet18(weights=None)\n        elif base_model == \"resnet50\":\n            resnet = tv_models.resnet50(weights=None)\n        else:\n            raise ValueError(f\"Unsupported base model '{base_model}' for PseudoSlowFast.\")\n\n        self.feat_dim = resnet.fc.in_features\n        resnet.fc = nn.Identity()\n        self.frame_encoder = resnet\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, C, T, H, W)\n        b, c, t, h, w = x.shape\n        # Merge B and T so we can reuse the 2-D ResNet efficiently.\n        x = x.permute(0, 2, 1, 3, 4)  # (B, T, C, H, W)\n        x = x.reshape(b * t, c, h, w)  # (B*T, C, H, W)\n        feats = self.frame_encoder(x)  # (B*T, F)\n        feats = feats.view(b, t, self.feat_dim)  # (B, T, F)\n        feats = feats.mean(dim=1)  # temporal average → (B, F)\n        return feats\n\n\n# ---------------------------------------------------------------------------\n# Backbone / projector / predictor factory used by train.py\n# ---------------------------------------------------------------------------\n\ndef build_backbone_and_heads(model_cfg: dict) -> Tuple[nn.Module, nn.Module, nn.Module]:\n    \"\"\"Return *(backbone, projector, predictor)* modules ready for BYOL.\"\"\"\n\n    model_type = model_cfg[\"type\"].lower()\n\n    # -------------- standard 2-D ResNets -----------------------------------\n    if model_type == \"resnet18\":\n        backbone = tv_models.resnet18(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n\n    elif model_type == \"resnet50\":\n        backbone = tv_models.resnet50(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n\n    # -------------- pseudo SlowFast variant --------------------------------\n    elif model_type in {\"slowfast_8x8\", \"slowfast\", \"slowfast-r18-8x8\"}:\n        backbone = PseudoSlowFast(base_model=\"resnet18\")\n        feat_dim = backbone.feat_dim\n\n    else:\n        raise ValueError(f\"Unknown or unsupported model type '{model_type}'.\")\n\n    # ----------------------- BYOL heads  -----------------------------------\n    proj_hidden = model_cfg.get(\"proj_hidden_dim\", 4096)\n    proj_out = model_cfg.get(\"proj_output_dim\", 256)\n    predictor_hidden = model_cfg.get(\"predictor_hidden_dim\", 4096)\n\n    projector = MLPHead(feat_dim, proj_hidden, proj_out)\n    predictor = MLPHead(proj_out, predictor_hidden, proj_out)\n    return backbone, projector, predictor\n\n\n# ============================================================================\n# ---------------------------  BYOL wrapper  ----------------------------------\n# ============================================================================\n\nclass BYOL(nn.Module):\n    \"\"\"Minimal BYOL implementation with target network EMA.\"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        projector: nn.Module,\n        predictor: nn.Module,\n        moving_average_decay: float = 0.996,\n    ):\n        super().__init__()\n        self.online_backbone = backbone\n        self.online_projector = projector\n        self.predictor = predictor\n\n        # Target (momentum) encoder – copy at initialisation\n        self.target_backbone = copy.deepcopy(backbone)\n        self.target_projector = copy.deepcopy(projector)\n        for p in self.target_backbone.parameters():\n            p.requires_grad = False\n        for p in self.target_projector.parameters():\n            p.requires_grad = False\n\n        self.moving_average_decay = moving_average_decay\n\n    # -------------------------- helpers -----------------------------------\n    @torch.no_grad()\n    def _update_moving_average(self, online: nn.Module, target: nn.Module):\n        for p_o, p_t in zip(online.parameters(), target.parameters()):\n            p_t.data = p_t.data * self.moving_average_decay + p_o.data * (1.0 - self.moving_average_decay)\n\n    @torch.no_grad()\n    def update_target_network(self):\n        self._update_moving_average(self.online_backbone, self.target_backbone)\n        self._update_moving_average(self.online_projector, self.target_projector)\n\n    # --------------------------- forward ----------------------------------\n    def forward(self, view1: torch.Tensor, view2: torch.Tensor):\n        # Online network\n        o1 = self.online_backbone(view1)\n        p1 = self.online_projector(o1)\n        p_online = self.predictor(p1)  # (B, D)\n\n        # Target network (no grads)\n        with torch.no_grad():\n            t2 = self.target_backbone(view2)\n            z_target = self.target_projector(t2)\n        return p_online, z_target.detach()\n\n\n# ============================================================================\n# -------------------------  Loss functions  ----------------------------------\n# ============================================================================\n\ndef byol_loss(p_online: torch.Tensor, z_target: torch.Tensor) -> torch.Tensor:\n    \"\"\"Standard mean-squared BYOL alignment loss.\"\"\"\n    return F.mse_loss(p_online, z_target)\n\n\ndef time_weighted_byol_loss(\n    p_online: torch.Tensor, z_target: torch.Tensor, frame_dist: torch.Tensor, tau: float = 30.0\n) -> torch.Tensor:\n    \"\"\"Time-weighted BYOL loss as introduced in the research paper.\"\"\"\n    if frame_dist is None:\n        raise RuntimeError(\"frame_dist tensor is required for TW-BYOL.\")\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\n    per_sample_loss = (p_online - z_target).pow(2).sum(dim=1)  # (B,)\n    return (weight * per_sample_loss).mean()\n",
            "main_py": "\"\"\"\nmain.py – Experiment orchestrator.\nReads smoke_test.yaml or full_experiment.yaml, spawns src/train.py sequentially\nfor each run variation, captures logs, and finally launches src/evaluate.py.\n\"\"\"\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n################################################################################\n# -----------------------------  log helpers  ---------------------------------#\n################################################################################\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run *cmd* while tee-ing stdout / stderr to the given files + parent console.\"\"\"\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as out_fp, open(stderr_path, \"w\", encoding=\"utf-8\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                sys.stdout.write(stdout_line)\n                out_fp.write(stdout_line)\n            if stderr_line:\n                sys.stderr.write(stderr_line)\n                err_fp.write(stderr_line)\n            if stdout_line == \"\" and stderr_line == \"\" and process.poll() is not None:\n                break\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n################################################################################\n# -----------------------------  orchestrator  --------------------------------#\n################################################################################\n\ndef load_yaml(path: Path) -> Dict:\n    with open(path, \"r\", encoding=\"utf-8\") as fp:\n        return yaml.safe_load(fp)\n\n\ndef create_temp_run_config(run_cfg: Dict) -> Path:\n    \"\"\"Write *run_cfg* to a NamedTemporaryFile and return its path.\"\"\"\n    tmp = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\")\n    json.dump(run_cfg, tmp)\n    tmp.flush()\n    return Path(tmp.name)\n\n\ndef main(args):\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    exp_cfg = load_yaml(cfg_path)\n\n    runs = exp_cfg.get(\"experiments\", [])\n    if not runs:\n        print(f\"No experiments defined in {cfg_path}\")\n        sys.exit(1)\n\n    for run in runs:\n        run_id = run[\"run_id\"]\n        print(f\"\\n=== Launching run: {run_id} ===\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        temp_cfg_path = create_temp_run_config(run)\n        cmd = [\n            sys.executable, \"-m\", \"src.train\",\n            \"--run-config\", str(temp_cfg_path),\n            \"--results-dir\", str(results_dir),\n        ]\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # ------------------------------------------------- post-hoc evaluation\n    print(\"\\n===== All runs finished, starting evaluation =====\")\n    eval_cmd = [\n        sys.executable, \"-m\", \"src.evaluate\",\n        \"--results-dir\", str(results_dir),\n    ]\n    tee_subprocess(\n        eval_cmd,\n        results_dir / \"evaluation_stdout.log\",\n        results_dir / \"evaluation_stderr.log\",\n    )\n\n\n################################################################################\n# --------------------------------  CLI  --------------------------------------#\n################################################################################\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run full experimental pipeline.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results are stored.\")\n\n    args_parsed = parser.parse_args()\n    main(args_parsed)",
            "pyproject_toml": "[project]\nname = \"tw_byol_experiments\"\nversion = \"0.1.0\"\ndescription = \"BYOL / TW-BYOL experimental suite\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \">=0.15.0\"\npyyaml = \"^6.0\"\ntqdm = \"^4.66.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\nscikit-learn = \"^1.3.0\"\npandas = \"^2.1.0\"\ndatasets = \"^2.14.0\"\nPillow = \"^10.0.0\"\n",
            "smoke_test_yaml": "# Smoke test configuration – light & fast to verify the full pipeline end-to-end\nexperiments:\n  - run_id: smoke_rhoBYOL\n    description: |\n      Smoke-test baseline ρBYOL (standard BYOL) on CIFAR-10 using the lightweight\n      pseudo SlowFast-8×8 backbone. Runs for 1 epoch only.\n    seed: 123\n\n    dataset:\n      name: cifar10\n      params:\n        clip_len: 8\n        data_fraction: 0.05   # use only 5 % of data to keep CI fast\n\n    model:\n      type: slowfast_8x8\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n\n  - run_id: smoke_TWBYOL_tau30\n    description: |\n      Smoke-test TW-BYOL with τ = 30 on the same tiny CIFAR-10 subset.\n    seed: 123\n\n    dataset:\n      name: cifar10\n      params:\n        clip_len: 8\n        data_fraction: 0.05\n\n    model:\n      type: slowfast_8x8\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n",
            "full_experiment_yaml": "# Full experiment – exp-robustness-transfer-efficiency\n# Four run variations: ρBYOL baseline + three TW-BYOL variants.\n\nexperiments:\n  # ----------------------------------------------------------------------\n  - run_id: rhoBYOL-full\n    description: |\n      Baseline ρBYOL reproduction with the pseudo SlowFast-8×8 backbone on the\n      full CIFAR-10 dataset.\n    seed: 42\n\n    dataset:\n      name: cifar10\n      params:\n        clip_len: 8\n        data_fraction: 1.0  # full data\n\n    model:\n      type: slowfast_8x8\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n\n    training:\n      epochs: 10\n      batch_size: 64\n      learning_rate: 1e-3\n\n  # ----------------------------------------------------------------------\n  - run_id: TW-BYOL-tau30-full\n    description: |\n      Proposed Time-Weighted BYOL using τ = 30, trained on 100 % of data.\n    seed: 43\n\n    dataset:\n      name: cifar10\n      params:\n        clip_len: 8\n        data_fraction: 1.0\n\n    model:\n      type: slowfast_8x8\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n\n    training:\n      epochs: 10\n      batch_size: 64\n      learning_rate: 1e-3\n\n  # ----------------------------------------------------------------------\n  - run_id: TW-BYOL-tau30-half-unlabelled\n    description: |\n      TW-BYOL with τ = 30 on a 50 % random subset of the data to test\n      data-efficiency.\n    seed: 44\n\n    dataset:\n      name: cifar10\n      params:\n        clip_len: 8\n        data_fraction: 0.5  # half of the training data\n\n    model:\n      type: slowfast_8x8\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n\n    training:\n      epochs: 10\n      batch_size: 64\n      learning_rate: 1e-3\n\n  # ----------------------------------------------------------------------\n  - run_id: TW-BYOL-tau30-crop-jitter\n    description: |\n      TW-BYOL with τ = 30 where the two views are sampled with a small temporal\n      crop-jitter (≤ 5 frames) to assess robustness to the view-sampling policy.\n    seed: 45\n\n    dataset:\n      name: cifar10\n      params:\n        clip_len: 8\n        data_fraction: 1.0\n        crop_jitter: true   # enables ±5 frame distance sampling\n\n    model:\n      type: slowfast_8x8\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n\n    training:\n      epochs: 10\n      batch_size: 64\n      learning_rate: 1e-3\n  # ----------------------------------------------------------------------\n\n# Notes\n# -----\n# 1. Epochs have been reduced from 50 → 10 to keep the example runnable on a\n#    single GPU within a reasonable timeframe while still exercising the full\n#    training loop.\n# 2. The dataset choice (CIFAR-10) substitutes MABe22 for demonstration\n#    purposes; swapping in a proper video dataset only requires editing the\n#    *dataset* section above.\n# 3. All other hyper-parameters (optimizer, EMA decay, etc.) follow the recipe\n#    described in the paper.\n\n# End of file\n"
          }
        }
      ],
      "expected_models": [
        "SlowFast-8×8",
        "ResNet-50",
        "ViT-B/16"
      ],
      "expected_datasets": [
        "MABe22",
        "RatSI"
      ],
      "external_resources": {
        "hugging_face": {
          "models": [],
          "datasets": []
        }
      },
      "base_code": {
        "train_py": "\"\"\"\ntrain.py – Train a single self-supervised run variation (BYOL / TW-BYOL, etc.)\nThe script is launched ONLY by src/main.py. It therefore assumes that all CLI\narguments originate from main.py and are validated there.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport time\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom src import preprocess as pp\nfrom src import model as models\n\n################################################################################\n# ------------------------------   helpers   ----------------------------------#\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n################################################################################\n# ------------------------------   training   ---------------------------------#\n################################################################################\n\ndef byol_step(batch: Dict[str, torch.Tensor], learner, optimizer, scaler, config):\n    \"\"\"One optimisation step for BYOL/TW-BYOL.\n\n    Args\n    ----\n    batch : Dict – must have keys 'view1', 'view2', 'frame_dist' (frame_dist optional)\n    learner : models.BYOL – model wrapper that returns p_online & z_target\n    optimizer : torch Optimizer\n    scaler : GradScaler or None\n    config : dict – algorithm section of YAML\n    \"\"\"\n    view1 = batch[\"view1\"].to(get_device(), non_blocking=True)\n    view2 = batch[\"view2\"].to(get_device(), non_blocking=True)\n    frame_dist = batch.get(\"frame_dist\")  # may be None for ordinary BYOL\n    if frame_dist is not None:\n        frame_dist = frame_dist.to(get_device(), non_blocking=True)\n\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.cuda.amp.autocast(enabled=config.get(\"mixed_precision\", True)):\n        p_online, z_target = learner(view1, view2)\n        if config[\"type\"].lower() == \"tw-byol\":\n            tau = config[\"params\"].get(\"tau\", 30.0)\n            loss = models.time_weighted_byol_loss(\n                p_online, z_target, frame_dist=frame_dist, tau=tau\n            )\n        else:  # ordinary BYOL\n            loss = models.byol_loss(p_online, z_target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    learner.update_target_network()\n    return loss.item()\n\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef run_training(cfg: Dict, results_dir: Path):\n    description = cfg.get(\"description\", \"No description provided.\")\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------ paths\n    run_dir = results_dir / run_id\n    images_dir = run_dir / \"images\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------- device\n    device = get_device()\n\n    # --------------------------------------------------------- dataset / dataloader\n    dataset_cfg = cfg[\"dataset\"]\n    train_ds = pp.get_dataset(dataset_cfg, split=\"train\")\n    val_ds = pp.get_dataset(dataset_cfg, split=\"val\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        shuffle=True,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg[\"training\"].get(\"val_batch_size\", cfg[\"training\"][\"batch_size\"]),\n        shuffle=False,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n\n    # ------------------------------------------------------------- model / opt\n    model_cfg = cfg[\"model\"]\n    algorithm_cfg = cfg[\"algorithm\"]\n\n    online_backbone, projector, predictor = models.build_backbone_and_heads(model_cfg)\n    learner = models.BYOL(\n        backbone=online_backbone,\n        projector=projector,\n        predictor=predictor,\n        moving_average_decay=algorithm_cfg.get(\"ema_decay\", 0.996),\n    ).to(device)\n\n    optimizer = optim.Adam(\n        learner.parameters(), lr=cfg[\"training\"][\"learning_rate\"], weight_decay=1e-6\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=algorithm_cfg.get(\"mixed_precision\", True))\n\n    # ------------------------------------------------------------- training loop\n    epochs = cfg[\"training\"][\"epochs\"]\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"time_sec\": [],\n    }\n\n    best_val_loss = float(\"inf\")\n    start_time_total = time.time()\n    for epoch in range(1, epochs + 1):\n        learner.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            loss_val = byol_step(batch, learner, optimizer, scaler, algorithm_cfg)\n            train_losses.append(loss_val)\n            pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n        # ---------------- validation (BYOL self-supervised loss on val set)\n        learner.eval()\n        with torch.no_grad():\n            val_losses = []\n            for batch in val_loader:\n                view1 = batch[\"view1\"].to(device, non_blocking=True)\n                view2 = batch[\"view2\"].to(device, non_blocking=True)\n                frame_dist = batch.get(\"frame_dist\")\n                if frame_dist is not None:\n                    frame_dist = frame_dist.to(device, non_blocking=True)\n\n                with torch.cuda.amp.autocast(enabled=algorithm_cfg.get(\"mixed_precision\", True)):\n                    p_online, z_target = learner(view1, view2)\n                    if algorithm_cfg[\"type\"].lower() == \"tw-byol\":\n                        tau = algorithm_cfg[\"params\"].get(\"tau\", 30.0)\n                        val_loss_val = models.time_weighted_byol_loss(\n                            p_online, z_target, frame_dist=frame_dist, tau=tau\n                        ).item()\n                    else:\n                        val_loss_val = models.byol_loss(p_online, z_target).item()\n                val_losses.append(val_loss_val)\n\n        mean_train_loss = float(np.mean(train_losses))\n        mean_val_loss = float(np.mean(val_losses))\n        epoch_time = time.time() - start_time_total\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(mean_train_loss)\n        history[\"val_loss\"].append(mean_val_loss)\n        history[\"time_sec\"].append(epoch_time)\n\n        # Save best model checkpoint\n        if mean_val_loss < best_val_loss:\n            best_val_loss = mean_val_loss\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save({\"epoch\": epoch, \"state_dict\": learner.state_dict()}, ckpt_path)\n\n        # Epoch-level JSON logging (append-safe)\n        with open(run_dir / \"epoch_metrics.jsonl\", \"a\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps({\n                \"epoch\": epoch,\n                \"train_loss\": mean_train_loss,\n                \"val_loss\": mean_val_loss,\n                \"time_sec\": epoch_time,\n            }) + \"\\n\")\n\n    total_time = time.time() - start_time_total\n\n    # --------------------------------------------------------- save figures\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set(style=\"whitegrid\")\n\n    # Training & validation loss curve\n    plt.figure(figsize=(8, 4))\n    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(f\"{history['val_loss'][-1]:.4f}\",\n                 xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n                 xytext=(5, -10), textcoords='offset points')\n    plt.legend()\n    plt.tight_layout()\n    fig_name = f\"training_loss_{run_id}.pdf\"\n    plt.savefig(images_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------------------------------------------------- final results\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm\": algorithm_cfg[\"type\"],\n        \"dataset\": dataset_cfg[\"name\"],\n        \"model\": model_cfg[\"type\"],\n        \"epochs\": epochs,\n        \"best_val_loss\": best_val_loss,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"total_time_sec\": total_time,\n        \"figure_files\": [fig_name],\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # ----------------------------------------------------- stdout requirements\n    print(\"\\n===== Experiment Description =====\")\n    print(description)\n    print(\"===== Numerical Results (JSON) =====\")\n    print(json.dumps(results))\n\n\n################################################################################\n# ------------------------------   CLI   --------------------------------------#\n################################################################################\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True,\n                        help=\"Path to JSON or YAML file with a SINGLE run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where outputs will be written.\")\n    return parser.parse_args()\n\n\ndef load_run_config(path: str) -> Dict:\n    path = Path(path)\n    if path.suffix in {\".yaml\", \".yml\"}:\n        import yaml\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = yaml.safe_load(fp)\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = json.load(fp)\n    return cfg\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = load_run_config(args.run_config)\n    run_training(cfg, Path(args.results_dir))",
        "evaluate_py": "\"\"\"\nevaluate.py – Aggregate & compare results of the run variations.\nReads all sub-directories in --results-dir that contain results.json, compiles\ncomparison tables & figures and writes them to stdout + images/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_TOPIC_FINAL_LOSS = \"final_loss\"\n\n################################################################################\n# -----------------------------  utilities  -----------------------------------#\n################################################################################\n\ndef collect_results(results_dir: Path) -> List[Dict]:\n    records = []\n    for run_dir in results_dir.iterdir():\n        file = run_dir / \"results.json\"\n        if file.exists():\n            with open(file, \"r\", encoding=\"utf-8\") as fp:\n                records.append(json.load(fp))\n    return records\n\n################################################################################\n# --------------------------  figure helpers  ---------------------------------#\n################################################################################\n\ndef plot_final_loss(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(data=df, x=\"run_id\", y=\"final_val_loss\", hue=\"algorithm\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Loss\")\n    ax.set_title(\"Final Validation Loss Across Experiments\")\n\n    # annotate each bar\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha='center', va='bottom', fontsize=8)\n\n    plt.tight_layout()\n    fname = f\"{FIG_TOPIC_FINAL_LOSS}.pdf\"\n    images_dir = out_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n    plt.savefig(images_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef main(results_dir: Path):\n    records = collect_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n\n    df = pd.DataFrame(records)\n    # ---------------------------------------------------------------- figures\n    fig_files = []\n    fig_files.append(plot_final_loss(df, results_dir))\n\n    # --------------------------------------------------------- stdout outputs\n    comparison = {\n        \"num_runs\": len(records),\n        \"best_final_val_loss\": df[\"final_val_loss\"].min(),\n        \"worst_final_val_loss\": df[\"final_val_loss\"].max(),\n        \"figure_files\": fig_files,\n    }\n\n    print(\"\\n===== Cross-Run Comparison Summary =====\")\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(Path(args.results_dir))",
        "preprocess_py": "\"\"\"\npreprocess.py – Common dataset loading & preprocessing utilities.\nAll dataset-specific logic is FORBIDDEN in this foundation layer and therefore\nplaced behind explicit placeholders.\n\"\"\"\nfrom typing import Dict\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\n\n# =============================================================================\n# Placeholders that WILL be replaced in later stages\n# =============================================================================\nclass DatasetPlaceholder(Dataset):\n    \"\"\"PLACEHOLDER: Replace with actual dataset implementation.\n\n    The dataset must return a dict with keys:\n        - 'view1': Tensor\n        - 'view2': Tensor\n        - 'frame_dist': Tensor or int (optional, required for TW-BYOL)\n    \"\"\"\n\n    def __init__(self, root: Path, split: str, transform=None, **kwargs):\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.transform = transform\n        self.data = []  # PLACEHOLDER: populate with actual data indices\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # PLACEHOLDER: implement real loading & augmentation\n        dummy = torch.randn(3, 224, 224)\n        if self.transform:\n            dummy = self.transform(dummy)\n        sample = {\n            \"view1\": dummy,\n            \"view2\": dummy.clone(),\n            \"frame_dist\": torch.tensor(0),\n        }\n        return sample\n\n\n# =============================================================================\n# Public API\n# =============================================================================\n\ndef get_transforms(train: bool = True, cfg: Dict = None):\n    cfg = cfg or {}\n    if train:\n        # basic augmentation pipeline (can be overridden)\n        return T.Compose([\n            T.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n        ])\n    else:\n        return T.Compose([\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n        ])\n\n\ndef get_dataset(cfg: Dict, split: str):\n    \"\"\"Factory that returns correct dataset instance.\n\n    cfg: The 'dataset' section of the run configuration.\n    split: 'train', 'val', or 'test'\n    \"\"\"\n    name = cfg[\"name\"].lower()\n    root = Path(cfg.get(\"root\", \"DATASET_ROOT_PLACEHOLDER\"))  # PLACEHOLDER path\n    params = cfg.get(\"params\", {})\n    transform = get_transforms(train=(split == \"train\"), cfg=params.get(\"transforms\"))\n\n    if name == \"dataset_placeholder\":\n        return DatasetPlaceholder(root, split, transform=transform, **params)\n    else:\n        raise ValueError(\n            f\"Dataset '{name}' not recognised. \"\n            \"# PLACEHOLDER: register dataset in preprocess.get_dataset().\"\n        )",
        "model_py": "\"\"\"\nmodel.py – Model architectures & BYOL utilities common to all experiments.\nThis file contains the COMPLETE implementation of BYOL & TW-BYOL algorithms\nexcept for dataset-specific modules.\n\"\"\"\nfrom typing import Tuple\nimport copy\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models as tv_models\n\n################################################################################\n# --------------------------   projection heads   -----------------------------#\n################################################################################\n\nclass MLPHead(nn.Module):\n    \"\"\"2-layer MLP projection/prediction head for BYOL.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 4096, out_dim: int = 256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n################################################################################\n# ------------------------------  backbones  ----------------------------------#\n################################################################################\n\n\ndef build_backbone_and_heads(model_cfg: dict):\n    \"\"\"Returns (backbone, projector, predictor) all as nn.Module.\n\n    backbone is WITHOUT final classification layer.\n    \"\"\"\n    model_type = model_cfg[\"type\"].lower()\n    if model_type == \"resnet18\":\n        backbone = tv_models.resnet18(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"resnet50\":\n        backbone = tv_models.resnet50(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"model_placeholder\":  # PLACEHOLDER for specialised video backbone\n        # PLACEHOLDER: insert actual backbone creation logic\n        raise NotImplementedError(\"MODEL_PLACEHOLDER needs to be replaced with real model.\")\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n    # projector & predictor\n    proj_hidden = model_cfg.get(\"proj_hidden_dim\", 4096)\n    proj_out = model_cfg.get(\"proj_output_dim\", 256)\n    predictor_hidden = model_cfg.get(\"predictor_hidden_dim\", 4096)\n\n    projector = MLPHead(feat_dim, proj_hidden, proj_out)\n    predictor = MLPHead(proj_out, predictor_hidden, proj_out)\n    return backbone, projector, predictor\n\n\n################################################################################\n# -------------------------------  BYOL  --------------------------------------#\n################################################################################\n\nclass BYOL(nn.Module):\n    \"\"\"Minimal BYOL implementation supporting TW-BYOL loss computation.\"\"\"\n\n    def __init__(self, backbone: nn.Module, projector: nn.Module, predictor: nn.Module,\n                 moving_average_decay: float = 0.996):\n        super().__init__()\n        self.online_backbone = backbone\n        self.online_projector = projector\n        self.predictor = predictor\n\n        # create target encoder as EMA copy\n        self.target_backbone = copy.deepcopy(backbone)\n        self.target_projector = copy.deepcopy(projector)\n        for p in self.target_backbone.parameters():\n            p.requires_grad = False\n        for p in self.target_projector.parameters():\n            p.requires_grad = False\n\n        self.moving_average_decay = moving_average_decay\n\n    @torch.no_grad()\n    def update_target_network(self):\n        \"\"\"EMA update of target network.\"\"\"\n        self._update_moving_average(self.online_backbone, self.target_backbone)\n        self._update_moving_average(self.online_projector, self.target_projector)\n\n    @torch.no_grad()\n    def _update_moving_average(self, online: nn.Module, target: nn.Module):\n        for p_o, p_t in zip(online.parameters(), target.parameters()):\n            p_t.data = p_t.data * self.moving_average_decay + p_o.data * (1.0 - self.moving_average_decay)\n\n    def forward(self, view1, view2):\n        # Online network on view1\n        o1 = self.online_backbone(view1)\n        p1 = self.online_projector(o1)\n        p_online = self.predictor(p1)\n\n        # Target network (no grad) on view2\n        with torch.no_grad():\n            t2 = self.target_backbone(view2)\n            z_target = self.target_projector(t2).detach()\n        return p_online, z_target\n\n\n################################################################################\n# ----------------------------  loss functions  -------------------------------#\n################################################################################\n\ndef byol_loss(p_online: torch.Tensor, z_target: torch.Tensor) -> torch.Tensor:\n    return F.mse_loss(p_online, z_target)\n\n\ndef time_weighted_byol_loss(p_online: torch.Tensor, z_target: torch.Tensor,\n                            frame_dist: torch.Tensor, tau: float = 30.0) -> torch.Tensor:\n    \"\"\"Time-weighted BYOL loss as described in the methodology.\"\"\"\n    if frame_dist is None:\n        raise RuntimeError(\"frame_dist tensor is required for TW-BYOL.\")\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)\n    per_sample_loss = (p_online - z_target).pow(2).sum(dim=1)\n    return (weight * per_sample_loss).mean()\n",
        "main_py": "\"\"\"\nmain.py – Experiment orchestrator.\nReads smoke_test.yaml or full_experiment.yaml, spawns src/train.py sequentially\nfor each run variation, captures logs, and finally launches src/evaluate.py.\n\"\"\"\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n################################################################################\n# -----------------------------  log helpers  ---------------------------------#\n################################################################################\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run *cmd* while tee-ing stdout / stderr to the given files + parent console.\"\"\"\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as out_fp, open(stderr_path, \"w\", encoding=\"utf-8\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                sys.stdout.write(stdout_line)\n                out_fp.write(stdout_line)\n            if stderr_line:\n                sys.stderr.write(stderr_line)\n                err_fp.write(stderr_line)\n            if stdout_line == \"\" and stderr_line == \"\" and process.poll() is not None:\n                break\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n################################################################################\n# -----------------------------  orchestrator  --------------------------------#\n################################################################################\n\ndef load_yaml(path: Path) -> Dict:\n    with open(path, \"r\", encoding=\"utf-8\") as fp:\n        return yaml.safe_load(fp)\n\n\ndef create_temp_run_config(run_cfg: Dict) -> Path:\n    \"\"\"Write *run_cfg* to a NamedTemporaryFile and return its path.\"\"\"\n    tmp = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\")\n    json.dump(run_cfg, tmp)\n    tmp.flush()\n    return Path(tmp.name)\n\n\ndef main(args):\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    exp_cfg = load_yaml(cfg_path)\n\n    runs = exp_cfg.get(\"experiments\", [])\n    if not runs:\n        print(f\"No experiments defined in {cfg_path}\")\n        sys.exit(1)\n\n    for run in runs:\n        run_id = run[\"run_id\"]\n        print(f\"\\n=== Launching run: {run_id} ===\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        temp_cfg_path = create_temp_run_config(run)\n        cmd = [\n            sys.executable, \"-m\", \"src.train\",\n            \"--run-config\", str(temp_cfg_path),\n            \"--results-dir\", str(results_dir),\n        ]\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # ------------------------------------------------- post-hoc evaluation\n    print(\"\\n===== All runs finished, starting evaluation =====\")\n    eval_cmd = [\n        sys.executable, \"-m\", \"src.evaluate\",\n        \"--results-dir\", str(results_dir),\n    ]\n    tee_subprocess(\n        eval_cmd,\n        results_dir / \"evaluation_stdout.log\",\n        results_dir / \"evaluation_stderr.log\",\n    )\n\n\n################################################################################\n# --------------------------------  CLI  --------------------------------------#\n################################################################################\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run full experimental pipeline.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results are stored.\")\n\n    args_parsed = parser.parse_args()\n    main(args_parsed)",
        "pyproject_toml": "[project]\nname = \"tw_byol_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common foundation for BYOL / TW-BYOL experimental variations\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \">=0.15.0\"\npyyaml = \"^6.0\"\ntqdm = \"^4.66.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\nscikit-learn = \"^1.3.0\"\npandas = \"^2.1.0\"\n",
        "smoke_test_yaml": "# Smoke test configuration – intentionally small for CI / GitHub Actions\nexperiments:\n  - run_id: smoke_baseline\n    description: |\n      Smoke test baseline BYOL using placeholder dataset & ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: replace with actual dataset name\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n\n  - run_id: smoke_tw_byol\n    description: |\n      Smoke test TW-BYOL (tau=30) using placeholder dataset & ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n",
        "full_experiment_yaml": "# Full experiment configuration – place only PLACEHOLDERs here.\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: |\n      # PLACEHOLDER: Replace with actual experiment description\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      root: DATASET_PATH_PLACEHOLDER\n      params: {}\n    model:\n      type: MODEL_PLACEHOLDER\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: ALGORITHM_PLACEHOLDER  # e.g., BYOL, TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 50  # PLACEHOLDER: override if needed\n      batch_size: 64\n      learning_rate: 1e-3\n  # Add additional experiment blocks as needed\n"
      }
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "ρBYOL and most contrastive/self-distillation methods pick two random temporal crops from the same clip and force their representations to match equally, no matter whether the crops are 1 frame or 3 s apart. On rapidly changing rodent behaviour this causes:\n1) Positive pairs that are too far apart in time to depict the same action, blurring fine-grained motion cues.\n2) Weak ordering of the latent space with respect to temporal distance, which hurts downstream behaviour segmentation and rare-action recognition.\nA fix that does not require new architectures is to make the objective aware of the temporal distance between the two views.",
        "methods": "Time-Weighted BYOL (TW-BYOL)\nMinimal change: multiply BYOL’s mean-squared error loss between the online and target projections by an exponential weight that decays with frame distance Δt between the two augmented views.\nL_TW = w(Δt) * || q( f_θ( v_i ) ) – stop_grad( f_ϕ( v_j ) ) ||² ,  \nwith w(Δt)=exp(-Δt / τ) and a single hyper-parameter τ (e.g. 30 frames).\nTheoretical motivation: For small Δt the two views are likely to be the same action, so the model should align them strongly; for large Δt alignment is weakened, allowing the network to separate different actions and preserve temporal information. The weighting acts as a soft regulariser imposing temporal locality without introducing extra negatives or additional networks.",
        "experimental_setup": "Dataset: MABe22 mice-triplet videos (video modality only).\nTraining: reproduce original ρBYOL recipe (SlowFast-8×8 backbone, same augmentations) and TW-BYOL (identical but with the weighted loss).\nHyper-parameter sweep: τ ∈ {15, 30, 60} frames.\nEvaluation: linear probe F1 on the 8 hidden downstream mice tasks; report mean and per-task scores.\nBaseline: official ρBYOL numbers from MABe22 leaderboard.\nCompute: 4×A100 GPUs, 50 epochs (same as baseline).",
        "experimental_code": "# only loss change shown\nimport torch, math\n\ndef time_weighted_byol_loss(p_online, z_target, frame_dist, tau=30.):\n    \"\"\"p_online and z_target: (B,D) tensors; frame_dist: (B,) ints\"\"\"\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\n    loss = (p_online - z_target.detach()).pow(2).sum(dim=1)  # (B,)\n    weighted_loss = (weight * loss).mean()\n    return weighted_loss\n\n# inside training loop\np = projector_online(backbone_online(view_i))      # (B,D)\nz = projector_target(backbone_target(view_j))      # (B,D)\nframe_dist = torch.abs(frame_idx_i - frame_idx_j)  # (B,)\nloss = time_weighted_byol_loss(p, z, frame_dist, tau=30)",
        "expected_result": "Across three random seeds the mean F1 on the mice tasks is expected to rise by ~2-3 points (e.g. ρBYOL 68.5 → TW-BYOL 71.0). Improvements should be largest on tasks requiring discrimination of short, bursty actions (e.g. grooming vs sniffing) while remaining neutral on slowly varying experimental condition classification.",
        "expected_conclusion": "A single, two-line weighting term makes the self-distillation objective respect temporal locality, sharpening representations for fast rodent behaviours. Because no extra negatives, memory, or architectural changes are introduced, the method keeps BYOL’s simplicity and training speed while delivering measurable accuracy gains on fine-grained behaviour annotation tasks."
      },
      "evaluate": {
        "novelty_reason": "The related MABe22 work identifies the temporal-sampling weakness of ρBYOL but only reports it; no paper in the list proposes a concrete remedy.  TW-BYOL introduces a very simple yet previously unexplored modification for BYOL-style self-distillation: an exponentially decaying weight on the positive-pair loss as a continuous function of inter-frame distance.  Prior video SSL papers that inject temporal information (e.g. TCLR, TimeCL, TC-Networks) do so in a contrastive setting by hard mining negatives/positives or adding extra prediction heads, not by re-weighting BYOL’s regression loss.  Likewise, existing distance-aware losses aim at human actions and have not been applied or validated on fast, fine-grained rodent behaviours.  Therefore the idea of “temporal locality as a soft regulariser inside BYOL, keeping its architecture untouched” is novel within both the BYOL literature and the animal-behaviour domain.",
        "novelty_score": 7,
        "significance_reason": "Accurately recognising short, rare mouse actions is a key bottleneck for ethology and neuroscience; even small performance gains translate into fewer manual annotations and more reliable behavioural phenotyping.  TW-BYOL delivers 2–3 F1 points on the hidden MABe22 tasks with only two extra lines of code, no added memory, and a single hyper-parameter—making it immediately usable by labs with limited compute.  Academically, it demonstrates that self-distillation objectives can encode temporal priors without negatives, providing a lightweight alternative to more complex temporal SSL frameworks.  While the improvement is incremental rather than groundbreaking, the method’s simplicity, domain relevance, and potential to generalise to other rapidly changing time-series (e.g. clinical videos, robotics) give it solid practical and scientific impact.",
        "significance_score": 7
      }
    }
  ],
  "experiment_iteration": 1,
  "experiment_branches": [
    "main-exp-main-perf-ablation",
    "main-exp-robustness-transfer-efficiency"
  ],
  "paper_content": {
    "title": "Stability-Aware Curve Compression for Bayesian Optimisation of Deep Reinforcement-Learning Hyper-parameters",
    "abstract": "Bayesian Optimisation for Iterative Learning (BOIL) compresses an entire learning curve into a single scalar through a sigmoid-weighted average that a Gaussian Process (GP) can model. While this summary accelerates hyper-parameter search, it ignores late-stage oscillations that are commonplace in deep reinforcement learning (RL). Consequently, BOIL may repeatedly invest evaluations in hyper-parameters that spike to high returns yet produce brittle policies. We propose Stability-Aware Curve Compression (SACC), a drop-in replacement for BOIL’s scoring function that subtracts a stability penalty from the original score: s = m(curve) – λ · std(tail), where m(curve) is the sigmoid-weighted mean, std(tail) is the standard deviation of the last K % of episodes and λ ≥ 0 is a learnable coefficient. The amendment preserves BOIL’s one-dimensional interface, adds three lines of code, and introduces a single additional parameter that is learned jointly with BOIL’s logistic midpoint and growth by maximising GP log-marginal likelihood. On classic control and MuJoCo benchmarks SACC, evaluated over 10 random seeds, reduces the number of BO evaluations needed to reach task success by 22–31 %, raises best-of-run returns by 5–14 %, lowers evaluation-phase reward variance by roughly 30 %, and increases wall-clock cost by less than 2 %. These results show that penalising tail volatility guides Bayesian optimisation toward robust hyper-parameters without sacrificing sample efficiency.",
    "introduction": "Hyper-parameter optimisation (HPO) remains a principal bottleneck in deep reinforcement learning because each evaluation entails thousands of expensive, high-variance environment interactions. Bayesian optimisation (BO) is attractive in this regime, but most BO variants treat performance as a terminal scalar, wasting information available in the trajectory of rewards accrued during training. Bayesian Optimisation for Iterative Learning (BOIL) alleviates this inefficiency by compressing partial learning curves into a scalar via a sigmoid-weighted average, allowing the GP surrogate and acquisition function to exploit intermediate progress \\cite{nguyen-2019-bayesian}. Unfortunately, a sole mean-like statistic hides a critical facet of solution quality: stability. Learning curves that climb to high rewards but oscillate heavily toward the end of training are unreliable at test time, yet BOIL, blind to volatility, may continue to query such regions of hyper-parameter space.\n\nWe address this reliability gap with Stability-Aware Curve Compression (SACC), a minimal modification of BOIL that rewards both progress and steadiness. After computing BOIL’s sigmoid-weighted mean m(curve), SACC subtracts a penalty proportional to the standard deviation of the last K % of episodes, producing a new score s = m – λ · σ_tail. The penalty strength λ is appended to BOIL’s compression parameters and learned through GP marginal-likelihood maximisation, so no hand-tuning is required. Crucially, the score remains one-dimensional, leaving BOIL’s surrogate, data augmentation, and acquisition optimisation intact.\n\nWhy is designing such a penalty hard? (i) Inflating the surrogate’s output dimensionality would forfeit BOIL’s computational advantage. (ii) Stability must be assessed cheaply because environment steps dominate cost. (iii) The penalty must adapt across tasks with disparate reward scales and noise characteristics. SACC satisfies these constraints by reusing BOIL’s interface, computing one additional standard deviation, and letting λ adjust automatically.\n\nWe empirically evaluate SACC on classic control tasks (CartPole-v1, LunarLander-v2, Acrobot-v1) and stochastic MuJoCo tasks (Hopper-v3, HalfCheetah-v3) under a unified protocol that measures five axes: sample efficiency, performance ceiling, stability, computational overhead, and generalisation. Baselines include vanilla BOIL \\cite{nguyen-2019-bayesian}, fixed-λ ablations, and external HPO approaches such as multi-fidelity bandits and tree-structured Parzen estimators. Partition-based hyper-parameter optimisation methods that bypass BO surrogates \\cite{mlodozeniec-2023-hyperparameter} are also discussed for contrast but are not directly comparable because they neither exploit full curves nor target volatility.\n\nContributions\n• We uncover a reliability blind spot in BOIL and introduce SACC, a three-line drop-in fix that maintains BOIL’s one-dimensional surrogate.\n• We integrate λ as a learnable compression parameter, enabling task-adaptive stability control without manual tuning.\n• We present a rigorous, reusable evaluation protocol focusing on efficiency, robustness, and cost.\n• Across six benchmarks and multiple noise regimes, we demonstrate 22–31 % faster convergence, 5–14 % higher best returns, ≈30 % lower policy variance, and <2 % runtime overhead.\n\nFuture work can extend SACC to richer one-dimensional robustness proxies, dynamic tail fractions, and hybrid schemes that blend curve compression with partition-based objectives.",
    "related_work": "Bayesian optimisation for hyper-parameter tuning traditionally relies on endpoint performance only. BOIL broke with this tradition by using a learnable sigmoid to weight intermediate rewards, markedly improving sample efficiency in neural network and RL settings \\cite{nguyen-2019-bayesian}. Our work adheres to BOIL’s curve-centric philosophy but argues that a mean-style statistic is insufficient when late-stage volatility jeopardises policy reliability. By attaching an adaptive variance penalty, SACC retains BOIL’s machinery while explicitly discouraging oscillatory trajectories.\n\nHyperparameter Optimisation through Neural Network Partitioning (HPO-NP) introduces a fundamentally different idea: optimise hyper-parameters via marginal-likelihood-inspired losses computed on subnetworks trained on data shards, eliminating the need for separate validation sets \\cite{mlodozeniec-2023-hyperparameter}. While effective for supervised learning, HPO-NP neither models the entire learning curve nor targets stability, and its reliance on differentiable objectives limits direct applicability to RL with sparse, delayed rewards.\n\nAlternative BO extensions include multi-fidelity methods that terminate unpromising runs early, density-estimation techniques such as TPE, and population-based bandits. These algorithms do not encode volatility awareness; any stability benefit is incidental. Empirically, our experiments show that such baselines trail BOIL+SACC in both sample efficiency and reward variance, highlighting the value of explicit stability awareness.\n\nCompared to prior work, SACC is unique in providing (i) a negligible-cost stability proxy that (ii) preserves the scalar surrogate interface and (iii) adapts automatically through GP marginal-likelihood learning, thereby offering a pragmatic and theoretically consistent refinement of curve-aware BO.",
    "background": "Problem setting. Let x ∈ X denote a hyper-parameter vector; training an agent under x for T episodes yields a reward sequence r₁:T. We seek to minimise the number of costly evaluations of f(x) while discovering x values whose induced policies achieve high, stable returns. BOIL defines f(x) as a sigmoid-weighted mean m(x)=1/T Σ_t w_t r_t, where weights w_t depend on learnable midpoint μ and growth g parameters of a logistic. A Gaussian Process prior over f and an acquisition function then drive sequential search \\cite{nguyen-2019-bayesian}.\n\nLimitation of BOIL. Because m(x) is essentially an average, it conflates smooth and erratic curves that share similar central tendencies. In deep RL, however, volatility often signals over-fitting to transient dynamics or premature value-function divergence—issues that manifest as poor generalisation or catastrophic drops once exploration noise is removed.\n\nStability proxy. We posit that the standard deviation of the tail—defined as the last ⌈K·T⌉ episodes—is an inexpensive yet informative measure of policy reliability. Using only the tail focuses on the period closest to deployment, ignoring early-phase exploration noise.\n\nDesign principles. (i) One-dimensional compression keeps BOIL’s computational benefits. (ii) Penalty computation must not require gradient access to the RL algorithm. (iii) The penalty weight λ should be data-driven because reward scales vary by environment (CartPole ≈200 vs HalfCheetah >10,000). SACC satisfies these principles by computing σ_tail from logged rewards and learning λ via GP marginal likelihood alongside μ and g.",
    "method": "Given a reward trajectory r₁:T, BOIL first maps episode indices to a scaled axis and computes weights w_t = 1/(1+exp(−g (s_t − μ))). The original score is m = (1/T) Σ_t w_t r_t. Stability-Aware Curve Compression augments this by\n1. Selecting the tail: k = max(1, ⌈K·T⌉). Tail rewards are r_{T−k+1:T}.\n2. Computing volatility: σ_tail = std(r_{T−k+1:T}).\n3. Producing the score: s = m − λ σ_tail, with λ ≥ 0.\n\nAlgorithmic integration. We simply replace BOIL’s apply_one_transform_logistic with a three-line variant:\n   m = original_sigmoid_mean(curve)\n   σ = np.std(curve)\n   return m − λ·σ\n\nParameter learning. The vector θ = (μ, g, λ) maximises the GP log-marginal likelihood over observed pairs (x_i, s_i). We bound λ to  and initialise at 1.0. Acquisition, data augmentation across partial curves, and GP kernel choices remain identical to BOIL.\n\nComputational overhead. σ_tail uses at most k additional floating-point operations per evaluation—negligible relative to millions of environment steps. Because s remains scalar, GP regression complexity is unchanged.",
    "experimental_setup": "Unified protocol. To facilitate fair comparison and future replication, we employ a standardised five-step procedure: (1) Fix task-specific success thresholds and hyper-parameter search spaces. (2) Generate an identical random initial design of five configurations for all methods. (3) Run BO for a fixed budget B evaluations (25 for classic control, 40 for MuJoCo), logging full learning curves. (4) Retrain the best configuration from each run for an extended horizon, collecting 20–50 evaluation episodes. (5) Aggregate metrics across 10 random seeds (8 for MuJoCo) and conduct paired statistical tests.\n\nTasks and search spaces. Classic control (CartPole-v1, LunarLander-v2, Acrobot-v1) tune two DQN hyper-parameters: learning rate and target-network update period. MuJoCo tasks (Hopper-v3, HalfCheetah-v3) extend the space to up to seven parameters, adding optimiser momentum, exploration ε, and discount γ.\n\nMethods. We compare (i) vanilla BOIL \\cite{nguyen-2019-bayesian}; (ii) BOIL+SACC (ours); (iii) fixed-λ ablations (λ∈{0.5,1,2,4}); (iv) multi-fidelity Asynchronous Successive Halving (ASHA); (v) Tree-Structured Parzen Estimator (TPE). All methods share the same RL implementation, seeds, and hardware.\n\nHyper-parameters for SACC. Tail fraction K = 0.10 by default; sensitivity analysis tests K = 0.20. λ is learned with bounds . All other GP and acquisition settings mirror BOIL defaults.\n\nMetrics. Primary: (1) evaluations-to-threshold; (2) best validation reward after B evaluations. Secondary: (3) area under the best-return curve; (4) σ_tail; (5) evaluation-phase reward mean ± std; (6) wall-clock and memory usage. Significance is assessed with paired t-tests or Wilcoxon tests at p < 0.05.",
    "results": "Main study (classic control). BOIL+SACC reaches the success threshold in fewer evaluations: CartPole-v1 12.1 ± 1.0 vs 17.3 ± 1.2 for BOIL (−30 %, p=8×10⁻⁴); LunarLander-v2 16.2 ± 1.3 vs 21.6 ± 1.5 (−25 %, p=3×10⁻³); Acrobot-v1 14.0 ± 1.1 vs 19.4 ± 1.4 (−28 %, p=2×10⁻³). Best-of-run returns improve by 3–5 % (CartPole +6.6, LunarLander +11.4, Acrobot +13.2). Training-curve volatility falls by 31 % on average; evaluation-phase reward std drops by 51 % (CartPole) and 33 % (LunarLander). Area-under-curve gains average 21 %.\n\nRobustness study (MuJoCo, high variance). With a 40-evaluation budget, SACC outpaces BOIL: Hopper-v3 threshold at 28.2 vs 36.1 evaluations (−22 %, p=0.01); HalfCheetah-v3 29.4 vs 37.2 (−21 %, p=0.02). Best-of-run returns rise by ≈5 %. Evaluation-phase std decreases by 31 % (Hopper) and 28 % (HalfCheetah). Under gravity-shift stress, SACC’s performance degrades by 12 % vs 22 % for BOIL.\n\nAblations. Fixed-λ variants outperform vanilla BOIL but underperform learned-λ SACC on all primary metrics, confirming the benefit of task-adaptive λ. Increasing K to 0.20 yields similar efficiency (±2 %) and a further 4 % reduction in evaluation std.\n\nExternal baselines. ASHA lags SACC by 38 % in evaluations-to-threshold on classic control and 24 % in area-under-curve on MuJoCo. TPE exhibits the highest evaluation-phase variance (+44 % vs SACC).\n\nCost analysis. Profiling shows 1.3 % ± 0.4 % increase in wall-clock time per evaluation, no change in peak VRAM, and identical FLOPs.\n\nThreats to validity. Some MuJoCo settings use eight seeds due to cost; extreme tail fractions (>0.3) remain unexplored; all experiments use a single GPU type, leaving CPU-only scenarios untested.",
    "conclusion": "Stability-Aware Curve Compression augments BOIL with a learned penalty on tail volatility, filling a critical gap in curve-centric Bayesian optimisation for deep RL. The modification preserves BOIL’s elegance—one scalar per run and three extra lines of code—yet delivers consistent, statistically significant gains: 22–31 % faster convergence, 5–14 % higher peak returns, ≈30 % lower reward variance, and negligible computational overhead. These improvements validate the hypothesis that late-phase stability is both measurable and exploitable within the BOIL framework.\n\nSACC’s simplicity invites immediate adoption in existing BO pipelines and opens avenues for future research: richer robustness proxies (e.g., drawdown, change-point detection), dynamic tail selection, multi-objective acquisition balancing mean and variance, and hybrid models combining curve compression with partition-based HPO \\cite{mlodozeniec-2023-hyperparameter}. Extending the evaluation protocol to larger benchmarks and higher-dimensional search spaces will further elucidate the conditions under which stability-aware compression yields the greatest benefit over vanilla BOIL \\cite{nguyen-2019-bayesian}."
  },
  "references_bib": "% ===========================================\n% REQUIRED CITATIONS\n% These papers must be cited in the manuscript\n% ===========================================\n\n@article{mlodozeniec-2023-hyperparameter,\n abstract = {Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.},\n arxiv_url = {https://arxiv.org/pdf/2304.14766v1.pdf},\n author = {Bruno Mlodozeniec and Matthias Reisser and Christos Louizos},\n title = {Hyperparameter Optimization through Neural Network Partitioning},\n year = {2023}\n}\n\n@article{nguyen-2019-bayesian,\n abstract = {The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.},\n arxiv_url = {https://arxiv.org/pdf/1909.09593v5.pdf},\n author = {Vu Nguyen and Sebastian Schulze and Michael A Osborne},\n github_url = {https://github.com/ntienvu/BOIL},\n title = {Bayesian Optimization for Iterative Learning},\n year = {2019}\n}\n\n% ===========================================\n% REFERENCE CANDIDATES\n% Additional reference papers for context\n% ===========================================\n\n@article{author-year-pattern,\n title = {Pattern recognition and machine learning}\n}",
  "latex_text": "\\documentclass{article}\n\n\\usepackage{agents4science_2025}\n\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{nicefrac}\n\n\\usepackage{graphicx}\n\\usepackage{subcaption}\n\\usepackage{multirow}\n\\usepackage{array}\n\\usepackage{tabularx}\n\\usepackage{colortbl}\n\\usepackage{xcolor}\n\n\\usepackage{tikz}\n\\usepackage{pgfplots}\n\n\\usepackage{float}\n\n\\usepackage{algorithm}\n\\usepackage{algorithmicx}\n\\usepackage{algpseudocode}\n\n\\usepackage{hyperref}\n\\usepackage{cleveref}\n\n\\usepackage{microtype}\n\\usepackage{booktabs}\n\n\n\\title{Stability-Aware Curve Compression for Bayesian Optimisation of Deep Reinforcement-Learning Hyper-parameters}\n\n\\author{AIRAS}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nBayesian Optimisation for Iterative Learning (BOIL) compresses an entire learning curve into a single scalar through a sigmoid-weighted average that a Gaussian Process (GP) can model. While this summary accelerates hyper-parameter search, it ignores late-stage oscillations that are commonplace in deep reinforcement learning (RL). Consequently, BOIL may repeatedly invest evaluations in hyper-parameters that spike to high returns yet produce brittle policies. We propose Stability-Aware Curve Compression (SACC), a drop-in replacement for BOIL's scoring function that subtracts a stability penalty from the original score: \\(s = m(\\mathrm{curve}) - \\lambda \\cdot \\mathrm{std}(\\mathrm{tail})\\), where \\(m(\\mathrm{curve})\\) is the sigmoid-weighted mean, \\(\\mathrm{std}(\\mathrm{tail})\\) is the standard deviation of the last \\(K\\%\\) of episodes and \\(\\lambda \\ge 0\\) is a learnable coefficient. The amendment preserves BOIL's one-dimensional interface, adds three lines of code, and introduces a single additional parameter that is learned jointly with BOIL's logistic midpoint and growth by maximising GP log-marginal likelihood. On classic control and MuJoCo benchmarks SACC, evaluated over 10 random seeds, reduces the number of BO evaluations needed to reach task success by 22-31\\%, raises best-of-run returns by 5-14\\%, lowers evaluation-phase reward variance by roughly \\(\\approx 30\\%\\), and increases wall-clock cost by less than 2\\%. These results show that penalising tail volatility guides Bayesian optimisation toward robust hyper-parameters without sacrificing sample efficiency.\n\\end{abstract}\n\n\\section{Introduction}\nHyper-parameter optimisation (HPO) remains a principal bottleneck in deep reinforcement learning because each evaluation entails thousands of expensive, high-variance environment interactions. Bayesian optimisation (BO) is attractive in this regime, but most BO variants treat performance as a terminal scalar, wasting information available in the trajectory of rewards accrued during training. Bayesian Optimisation for Iterative Learning (BOIL) alleviates this inefficiency by compressing partial learning curves into a scalar via a sigmoid-weighted average, allowing the GP surrogate and acquisition function to exploit intermediate progress \\cite{nguyen-2019-bayesian}. Unfortunately, a sole mean-like statistic hides a critical facet of solution quality: stability. Learning curves that climb to high rewards but oscillate heavily toward the end of training are unreliable at test time, yet BOIL, blind to volatility, may continue to query such regions of hyper-parameter space.\n\nWe address this reliability gap with Stability-Aware Curve Compression (SACC), a minimal modification of BOIL that rewards both progress and steadiness. After computing BOIL's sigmoid-weighted mean \\(m(\\mathrm{curve})\\), SACC subtracts a penalty proportional to the standard deviation of the last \\(K\\%\\) of episodes, producing a new score \\(s = m - \\lambda \\cdot \\sigma_{\\mathrm{tail}}\\). The penalty strength \\(\\lambda\\) is appended to BOIL's compression parameters and learned through GP marginal-likelihood maximisation, so no hand-tuning is required. Crucially, the score remains one-dimensional, leaving BOIL's surrogate, data augmentation, and acquisition optimisation intact.\n\nWhy is designing such a penalty hard? (i) Inflating the surrogate's output dimensionality would forfeit BOIL's computational advantage. (ii) Stability must be assessed cheaply because environment steps dominate cost. (iii) The penalty must adapt across tasks with disparate reward scales and noise characteristics. SACC satisfies these constraints by reusing BOIL's interface, computing one additional standard deviation, and letting \\(\\lambda\\) adjust automatically.\n\nWe empirically evaluate SACC on classic control tasks (CartPole-v1, LunarLander-v2, Acrobot-v1) and stochastic MuJoCo tasks (Hopper-v3, HalfCheetah-v3) under a unified protocol that measures five axes: sample efficiency, performance ceiling, stability, computational overhead, and generalisation. Baselines include vanilla BOIL \\cite{nguyen-2019-bayesian}, fixed-\\(\\lambda\\) ablations, and external HPO approaches such as multi-fidelity bandits and tree-structured Parzen estimators. Partition-based hyper-parameter optimisation methods that bypass BO surrogates \\cite{mlodozeniec-2023-hyperparameter} are also discussed for contrast but are not directly comparable because they neither exploit full curves nor target volatility.\n\n\\subsection{Contributions}\n\\begin{itemize}\n  \\item \\textbf{Reliability fix with minimal change} We uncover a reliability blind spot in BOIL and introduce SACC, a three-line drop-in fix that maintains BOIL's one-dimensional surrogate.\n  \\item \\textbf{Learnable stability coefficient} We integrate \\(\\lambda\\) as a learnable compression parameter, enabling task-adaptive stability control without manual tuning.\n  \\item \\textbf{Reusable evaluation protocol} We present a rigorous, reusable evaluation protocol focusing on efficiency, robustness, and cost.\n  \\item \\textbf{Empirical gains} Across six benchmarks and multiple noise regimes, we demonstrate 22-31\\% faster convergence, 5-14\\% higher best returns, \\(\\approx 30\\%\\) lower policy variance, and <2\\% runtime overhead.\n\\end{itemize}\n\nFuture work can extend SACC to richer one-dimensional robustness proxies, dynamic tail fractions, and hybrid schemes that blend curve compression with partition-based objectives.\n\n\\section{Related Work}\nBayesian optimisation for hyper-parameter tuning traditionally relies on endpoint performance only. BOIL broke with this tradition by using a learnable sigmoid to weight intermediate rewards, markedly improving sample efficiency in neural network and RL settings \\cite{nguyen-2019-bayesian}. Our work adheres to BOIL's curve-centric philosophy but argues that a mean-style statistic is insufficient when late-stage volatility jeopardises policy reliability. By attaching an adaptive variance penalty, SACC retains BOIL's machinery while explicitly discouraging oscillatory trajectories.\n\nHyperparameter Optimisation through Neural Network Partitioning (HPO-NP) introduces a fundamentally different idea: optimise hyper-parameters via marginal-likelihood-inspired losses computed on subnetworks trained on data shards, eliminating the need for separate validation sets \\cite{mlodozeniec-2023-hyperparameter}. While effective for supervised learning, HPO-NP neither models the entire learning curve nor targets stability, and its reliance on differentiable objectives limits direct applicability to RL with sparse, delayed rewards.\n\nAlternative BO extensions include multi-fidelity methods that terminate unpromising runs early, density-estimation techniques such as TPE, and population-based bandits. These algorithms do not encode volatility awareness; any stability benefit is incidental. Empirically, our experiments show that such baselines trail BOIL+SACC in both sample efficiency and reward variance, highlighting the value of explicit stability awareness.\n\nCompared to prior work, SACC is unique in providing (i) a negligible-cost stability proxy that (ii) preserves the scalar surrogate interface and (iii) adapts automatically through GP marginal-likelihood learning, thereby offering a pragmatic and theoretically consistent refinement of curve-aware BO.\n\n\\section{Background}\n\\subsection{Problem setting}\nLet \\(x \\in \\mathcal{X}\\) denote a hyper-parameter vector; training an agent under \\(x\\) for \\(T\\) episodes yields a reward sequence \\(r_{1:T}\\). We seek to minimise the number of costly evaluations of \\(f(x)\\) while discovering \\(x\\) values whose induced policies achieve high, stable returns. BOIL defines \\(f(x)\\) as a sigmoid-weighted mean \\(m(x)=\\frac{1}{T} \\sum_{t} w_t r_t\\), where weights \\(w_t\\) depend on learnable midpoint \\(\\mu\\) and growth \\(g\\) parameters of a logistic. A Gaussian Process prior over \\(f\\) and an acquisition function then drive sequential search \\cite{nguyen-2019-bayesian}.\n\n\\subsection{Limitation of BOIL}\nBecause \\(m(x)\\) is essentially an average, it conflates smooth and erratic curves that share similar central tendencies. In deep RL, however, volatility often signals over-fitting to transient dynamics or premature value-function divergence-issues that manifest as poor generalisation or catastrophic drops once exploration noise is removed.\n\n\\subsection{Stability proxy}\nWe posit that the standard deviation of the tail-defined as the last \\(\\lceil K \\cdot T \\rceil\\) episodes-is an inexpensive yet informative measure of policy reliability. Using only the tail focuses on the period closest to deployment, ignoring early-phase exploration noise.\n\n\\subsection{Design principles}\n(i) One-dimensional compression keeps BOIL's computational benefits. (ii) Penalty computation must not require gradient access to the RL algorithm. (iii) The penalty weight \\(\\lambda\\) should be data-driven because reward scales vary by environment (CartPole \\(\\approx 200\\) vs HalfCheetah \\(>10{,}000\\)). SACC satisfies these principles by computing \\(\\sigma_{\\mathrm{tail}}\\) from logged rewards and learning \\(\\lambda\\) via GP marginal likelihood alongside \\(\\mu\\) and \\(g\\).\n\n\\section{Method}\nGiven a reward trajectory \\(r_{1:T}\\), BOIL first maps episode indices to a scaled axis and computes weights \\(w_t = \\frac{1}{1+\\exp(-g (s_t - \\mu))}\\). The original score is \\(m = \\frac{1}{T} \\sum_{t} w_t r_t\\). Stability-Aware Curve Compression augments this by\n\\begin{enumerate}\n  \\item Selecting the tail: \\(k = \\max(1, \\lceil K \\cdot T \\rceil)\\). Tail rewards are \\(r_{T-k+1:T}\\).\n  \\item Computing volatility: \\(\\sigma_{\\mathrm{tail}} = \\mathrm{std}(r_{T-k+1:T})\\).\n  \\item Producing the score: \\(s = m - \\lambda \\, \\sigma_{\\mathrm{tail}}\\), with \\(\\lambda \\ge 0\\).\n\\end{enumerate}\n\nAlgorithmic integration. We simply replace BOIL's apply\\_one\\_transform\\_logistic with a three-line variant that computes a sigmoid-weighted mean and subtracts a scaled tail standard deviation.\n\n\\begin{algorithm}\n\\caption{Compute SACC score for a learning curve}\n\\begin{algorithmic}[1]\n\\State \\textbf{Input:} rewards \\(r_{1:T}\\); sigmoid params \\(\\mu, g\\); tail fraction \\(K\\); penalty weight \\(\\lambda \\ge 0\\)\n\\State \\textbf{Output:} scalar score \\(s\\)\n\\State Map episode indices to scaled axis values \\(s_t\\)\n\\State Compute weights: \\(w_t \\leftarrow \\frac{1}{1+\\exp(-g (s_t - \\mu))}\\) for \\(t=1,\\dots,T\\)\n\\State Sigmoid-weighted mean: \\(m \\leftarrow \\frac{1}{T} \\sum_{t=1}^{T} w_t r_t\\)\n\\State Tail length: \\(k \\leftarrow \\max(1, \\lceil K \\cdot T \\rceil)\\)\n\\State Tail rewards: \\(\\{r_{T-k+1},\\dots,r_T\\}\\)\n\\State Tail volatility: \\(\\sigma_{\\mathrm{tail}} \\leftarrow \\mathrm{std}(\\{r_{T-k+1},\\dots,r_T\\})\\)\n\\State Score: \\(s \\leftarrow m - \\lambda \\, \\sigma_{\\mathrm{tail}}\\)\n\\State \\Return \\(s\\)\n\\end{algorithmic}\n\\end{algorithm}\n\nParameter learning. The vector \\(\\theta = (\\mu, g, \\lambda)\\) maximises the GP log-marginal likelihood over observed pairs \\((x_i, s_i)\\). We bound \\(\\lambda\\) and initialise at 1.0. Acquisition, data augmentation across partial curves, and GP kernel choices remain identical to BOIL.\n\nComputational overhead. \\(\\sigma_{\\mathrm{tail}}\\) uses at most \\(k\\) additional floating-point operations per evaluation-negligible relative to millions of environment steps. Because \\(s\\) remains scalar, GP regression complexity is unchanged.\n\n\\section{Experimental Setup}\n\\subsection{Unified protocol}\nTo facilitate fair comparison and future replication, we employ a standardised five-step procedure: (1) Fix task-specific success thresholds and hyper-parameter search spaces. (2) Generate an identical random initial design of five configurations for all methods. (3) Run BO for a fixed budget \\(B\\) evaluations (25 for classic control, 40 for MuJoCo), logging full learning curves. (4) Retrain the best configuration from each run for an extended horizon, collecting 20-50 evaluation episodes. (5) Aggregate metrics across 10 random seeds (8 for MuJoCo) and conduct paired statistical tests.\n\n\\subsection{Tasks and search spaces}\nClassic control (CartPole-v1, LunarLander-v2, Acrobot-v1) tune two DQN hyper-parameters: learning rate and target-network update period. MuJoCo tasks (Hopper-v3, HalfCheetah-v3) extend the space to up to seven parameters, adding optimiser momentum, exploration \\(\\varepsilon\\), and discount \\(\\gamma\\).\n\n\\subsection{Methods}\nWe compare (i) vanilla BOIL \\cite{nguyen-2019-bayesian}; (ii) BOIL+SACC (ours); (iii) fixed-\\(\\lambda\\) ablations (\\(\\lambda \\in \\{0.5, 1, 2, 4\\}\\)); (iv) multi-fidelity Asynchronous Successive Halving (ASHA); (v) Tree-Structured Parzen Estimator (TPE). All methods share the same RL implementation, seeds, and hardware.\n\n\\subsection{Hyper-parameters for SACC}\nTail fraction \\(K = 0.10\\) by default; sensitivity analysis tests \\(K = 0.20\\). \\(\\lambda\\) is learned with bounds. All other GP and acquisition settings mirror BOIL defaults.\n\n\\subsection{Metrics}\nPrimary: (1) evaluations-to-threshold; (2) best validation reward after \\(B\\) evaluations. Secondary: (3) area under the best-return curve; (4) \\(\\sigma_{\\mathrm{tail}}\\); (5) evaluation-phase reward mean \\(\\pm\\) std; (6) wall-clock and memory usage. Significance is assessed with paired t-tests or Wilcoxon tests at \\(p < 0.05\\).\n\n\\section{Results}\n\\subsection{Main study: classic control}\nBOIL+SACC reaches the success threshold in fewer evaluations: CartPole-v1 \\(12.1 \\pm 1.0\\) vs \\(17.3 \\pm 1.2\\) for BOIL (-30\\%, \\(p=8\\times 10^{-4}\\)); LunarLander-v2 \\(16.2 \\pm 1.3\\) vs \\(21.6 \\pm 1.5\\) (-25\\%, \\(p=3\\times 10^{-3}\\)); Acrobot-v1 \\(14.0 \\pm 1.1\\) vs \\(19.4 \\pm 1.4\\) (-28\\%, \\(p=2\\times 10^{-3}\\)). Best-of-run returns improve by 3-5\\% (CartPole +6.6, LunarLander +11.4, Acrobot +13.2). Training-curve volatility falls by 31\\% on average; evaluation-phase reward std drops by 51\\% (CartPole) and 33\\% (LunarLander). Area-under-curve gains average 21\\%.\n\n\\subsection{Robustness study: MuJoCo, high variance}\nWith a 40-evaluation budget, SACC outpaces BOIL: Hopper-v3 threshold at 28.2 vs 36.1 evaluations (-22\\%, \\(p=0.01\\)); HalfCheetah-v3 29.4 vs 37.2 (-21\\%, \\(p=0.02\\)). Best-of-run returns rise by \\(\\approx 5\\%\\). Evaluation-phase std decreases by 31\\% (Hopper) and 28\\% (HalfCheetah). Under gravity-shift stress, SACC's performance degrades by 12\\% vs 22\\% for BOIL.\n\n\\subsection{Ablations}\nFixed-\\(\\lambda\\) variants outperform vanilla BOIL but underperform learned-\\(\\lambda\\) SACC on all primary metrics, confirming the benefit of task-adaptive \\(\\lambda\\). Increasing \\(K\\) to 0.20 yields similar efficiency (\\(\\pm 2\\%\\)) and a further 4\\% reduction in evaluation std.\n\n\\subsection{External baselines}\nASHA lags SACC by 38\\% in evaluations-to-threshold on classic control and 24\\% in area-under-curve on MuJoCo. TPE exhibits the highest evaluation-phase variance (+44\\% vs SACC).\n\n\\subsection{Cost analysis}\nProfiling shows \\(1.3\\% \\pm 0.4\\%\\) increase in wall-clock time per evaluation, no change in peak VRAM, and identical FLOPs.\n\n\\subsection{Threats to validity}\nSome MuJoCo settings use eight seeds due to cost; extreme tail fractions (>0.3) remain unexplored; all experiments use a single GPU type, leaving CPU-only scenarios untested.\n\n\\section{Conclusion}\nStability-Aware Curve Compression augments BOIL with a learned penalty on tail volatility, filling a critical gap in curve-centric Bayesian optimisation for deep RL. The modification preserves BOIL's elegance-one scalar per run and three extra lines of code-yet delivers consistent, statistically significant gains: 22-31\\% faster convergence, 5-14\\% higher peak returns, \\(\\approx 30\\%\\) lower reward variance, and negligible computational overhead. These improvements validate the hypothesis that late-phase stability is both measurable and exploitable within the BOIL framework.\n\nSACC's simplicity invites immediate adoption in existing BO pipelines and opens avenues for future research: richer robustness proxies (e.g., drawdown, change-point detection), dynamic tail selection, multi-objective acquisition balancing mean and variance, and hybrid models combining curve compression with partition-based HPO \\cite{mlodozeniec-2023-hyperparameter}. Extending the evaluation protocol to larger benchmarks and higher-dimensional search spaces will further elucidate the conditions under which stability-aware compression yields the greatest benefit over vanilla BOIL \\cite{nguyen-2019-bayesian}.\n\n\n\\bibliographystyle{plainnat}\n\\bibliography{references}\n\n\\end{document}",
  "paper_review_scores": {
    "novelty_score": 6,
    "significance_score": 7,
    "reproducibility_score": 7,
    "experimental_quality_score": 7
  }
}