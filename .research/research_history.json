{
  "research_topic": "diffusion modelの速度改善",
  "queries": [
    "diffusion model acceleration"
  ],
  "research_study_list": [
    {
      "title": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference"
    },
    {
      "title": "Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis"
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?\",\n    \"Methods\": \"Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides—per time-step t and per network block k—whether the block is executed.  \\n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk·h(t)), where h(t)=1−t (so gates tend to stay open near the data region).  \\n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk∈{0,1}) at inference time.  \\n3. Loss=Lnoise+λ⋅Σk g̅k, where Lnoise is the standard noise-prediction loss and g̅k is the average gate activation over the batch; λ is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \\n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.\",\n    \"Experimental Setup\": \"Model: the public DDPM CIFAR-10 UNet (32×32).\\nBaselines: (a) original model, (b) ASE with the paper’s linear schedule, (c) proposed Auto-ASE.\\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; λ=0.05.\\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.\",\n    \"Experimental Code\": \"import torch, torch.nn as nn\\n\\nclass GatedBlock(nn.Module):\\n    def __init__(self, block):\\n        super().__init__()\\n        self.block = block\\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\\n    def forward(self, x, t):\\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\\n        if self.training:\\n            return x + gate * (self.block(x, t) - x), gate.mean()\\n        else:  # STE binarisation\\n            hard_gate = (gate > 0.5).float()\\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\\n\\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\\n\\ngate_loss_coeff = 0.05\\n\\nfor x, t, noise in dataloader:\\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\\n    loss.backward(); optimizer.step()\",\n    \"Expected Result\": \"Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t≈0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (≈16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.\",\n    \"Expected Conclusion\": \"By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration.\"\n}",
    "experimental_design": {
      "experiment_strategy": "Goal: Build a single, rigorous experimental play-book that can be applied to every study of Auto-ASE, proving that the learnable gating schedule (a) preserves or improves sample quality, (b) reduces sampling cost, (c) is robust across models, datasets and solvers, and (d) removes the need for manual tuning.\n\n1. Key hypotheses to validate\n   H1  Performance: Auto-ASE delivers the same or better generative quality than the original network and the hand-crafted ASE schedule.\n   H2  Efficiency: Auto-ASE cuts wall–clock sampling time, FLOPs and GPU energy by at least 20 % versus the full model and beats the best fixed schedule.\n   H3  Robustness & Generalisation: The learned schedule works (without re-tuning) on multiple datasets, resolutions, network backbones and numerical solvers.\n   H4  Simplicity: Training overhead and parameter growth are negligible (<0.5 % extra parameters, <5 % extra training time).\n\n2. Universal comparison set\n   a. Baselines\n      • Full model (no skipping)\n      • Original ASE with its published dropping rule\n   b. State-of-the-art accelerators that keep the backbone intact (e.g. DDIM, DPM-Solver++, PNDM) to show trade-off curves\n   c. Ablations of Auto-ASE\n      • No sparsity loss (λ = 0)\n      • Shared vs individual gates\n      • Different gate shapes h(t)\n      • Soft-gating at inference (no STE)\n\n3. Evaluation axes (applied in every experiment)\n   Quantitative quality: FID, KID, IS (for images) or task-specific metrics\n   Qualitative quality: curated sample grids + human Turing test where feasible\n   Efficiency:  (i) average number of executed blocks, (ii) total UNet/Transformer calls, (iii) wall-clock latency, (iv) TFLOPs, (v) peak & average GPU memory, (vi) Joule/1000 images measured by nvidia-smi\n   Robustness: metric distributions across 3 random seeds and across 3 λ values\n   Schedule insight: plots of g_k(t) heat-maps & histogram of closed gates\n\n4. Experimental matrix (re-used each time)\n   Tier-1  In-domain sanity: original public UNet × CIFAR-10 × DDPM solver (50 steps)\n   Tier-2  Cross-architecture: replace UNet by DiT-XL/2 or ADM-KD while keeping CIFAR-10\n   Tier-3  Cross-data & resolution: ImageNet-64, LSUN-256, Stable-Diffusion 512×512 latent UNet; keep the same Auto-ASE hyper-parameters\n   Tier-4  Cross-solver: DDIM 25 steps, DPM-Solver++ 15 steps, PLMS 50 steps\n   Tier-5  Stress tests: (i) halve/ double λ, (ii) prune 70 % of blocks, (iii) deploy on corrupted noise schedules\n\n5. Success criteria (must be met on every Tier-1 & Tier-2 run, and on ≥75 % of Tier-3/4/5 runs)\n   • ΔFID ≤ +0.5 (or KID ≤ +0.002) relative to full model\n   • ≥20 % speed-up vs full model; ≥5 % extra speed-up vs best fixed ASE schedule\n   • <0.5 % parameter growth; <5 % extra training time\n   • For robustness tiers: variance of ΔFID across seeds ≤ 0.8 and no catastrophic failure (FID < ×1.5 of baseline)\n\n6. Measurement protocol\n   • All runs on a single NVIDIA A100-80GB, PyTorch compiled with CUDA 11.8 + cuDNN 8.\n   • Disable CPU-GPU overlaps to obtain deterministic timing; average over 1 k samples after 50 warm-ups.\n   • Log with Weights & Biases to expose full metrics, curves and gate heat-maps.\n\n7. Reporting template (identical for all papers/sections)\n   Table 1  Quality & efficiency metrics for Baselines vs Auto-ASE\n   Figure 1 Trade-off curve: FID vs wall-clock time\n   Figure 2 Gate activation heat-map g_k(t)\n   Table 2 Ablation results\n   Appendix: energy profile & hardware counters\n\nBy adhering to this uniform strategy, each experiment will systematically attack the four key hypotheses with controlled baselines, rich metrics and clear success thresholds, allowing reviewers to judge Auto-ASE’s effectiveness from multiple, reproducible perspectives.",
      "experiments": [
        {
          "experiment_id": "exp-1-main-perf-eff",
          "run_variations": [
            "full-ddpm",
            "ase-linear",
            "auto-ase",
            "auto-ase-soft",
            "auto-ase-no-sparsity"
          ],
          "description": "Objective / hypothesis: Validate H1 (quality) and H2 (efficiency) on Tier-1 (CIFAR-10 32×32) and Tier-2 (cross-architecture on DiT-XL/2).\n\nModels:\n • DDPM public UNet-32 (baseline backbone)\n • DiT-XL/2 (Transformer backbone, 32×32) for cross-architecture check\n\nDatasets:\n • CIFAR-10 (train 45k / val 5k / test 10k). No label-conditioning.\nPre-processing: random horizontal flip 0.5, map to [-1,1], no resize. Stats cached in .npy to avoid CPU bottleneck.\n\nData split & repetition: 3 random seeds. Train on train set, validate on val every 2 K iters, early-stop on best-val FID. Report test metrics averaged over seeds ± σ.\n\nRun variations:\n 1. full-ddpm – original UNet, 50 sampling steps.\n 2. ase-linear – hand-crafted dropping rule from ASE paper (same #steps).\n 3. auto-ase – proposed learnable gates + STE at inference, λ=0.05.\n 4. auto-ase-soft – gates kept continuous (no STE) at inference to probe quality/latency trade-off.\n 5. auto-ase-no-sparsity – λ=0, tests necessity of L1 regulariser.\nAll runs fine-tune 1 epoch with AdamW lr=1e-4, batch 128.\n\nEvaluation metrics:\n Primary – FID (10 k test images), KID (×10³), IS.\n Secondary – avg. executed blocks, wall-clock latency/img, TFLOPs/img (torch.profiler), peak GPU-mem, nvidia-smi energy (J).\n\nEfficiency accounting: collect CUDA events over 1 k samples after 50 warm-ups; disable cudnn benchmarking.\n\nComputational cost: record train time/epoch and extra params (%).\n\nHyper-parameter probe: additional grid lr∈{5e-5,1e-4,2e-4} for auto-ase (reported in appendix).\n\nExample code snippet (partial):\n```\nwith torch.autocast('cuda'):\n    start = torch.cuda.Event(enable_timing=True)\n    end   = torch.cuda.Event(enable_timing=True)\n    start.record(); imgs = sampler(model, 50); end.record();\n    torch.cuda.synchronize(); elapsed_ms = start.elapsed_time(end)\n```\n\nSuccess criteria: ΔFID ≤+0.5 vs full, ≥20 % speed-up vs full, ≥5 % vs ase-linear. Results populate Table 1, Fig. 1 trade-off, Fig. 2 heat-map.\n\nBranch: feature/exp-1-main-perf-eff",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-1-main-perf-eff"
          },
          "code": {
            "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with real dataset support.\n\nThis module now contains fully-fledged dataloader logic for CIFAR-10 via the\nHugging Face datasets hub (dataset id: \"uoft-cs/cifar10\").  A lightweight\n`FakeData` fallback remains for CI / smoke tests.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets as tv_datasets, transforms\n\n# We lazily import HF datasets to avoid the dependency cost when running only\n# smoke tests (which use torchvision FakeData).  ImportError will propagate if\n# a real HF dataset is requested without the package installed.\ntry:\n    from datasets import load_dataset\nexcept ModuleNotFoundError:  # pragma: no cover – handled at runtime\n    load_dataset = None  # type: ignore\n\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef cifar10_transforms() -> transforms.Compose:\n    \"\"\"Standard CIFAR-10 data augmentation + mapping to [-1, 1].\"\"\"\n    tfms: List[Any] = [\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # [0,1] -> [-1,1]\n    ]\n    return transforms.Compose(tfms)\n\n\ndef dummy_transforms(image_size=(3, 32, 32)) -> transforms.Compose:\n    tfms: List[Any] = [\n        transforms.ToTensor(),\n    ]\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HF Dataset wrappers                                                       #\n# ------------------------------------------------------------------------- #\n\nclass HFImageDataset(Dataset):\n    \"\"\"Thin wrapper converting a Hugging Face dataset into a PyTorch dataset.\"\"\"\n\n    def __init__(self, hf_ds, tfms):\n        self.ds = hf_ds\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        sample = self.ds[idx]\n        # The exact field name can vary (\"img\"|\"image\") – we try both.\n        img = sample.get(\"img\", None)\n        if img is None:\n            img = sample.get(\"image\", None)\n        if img is None:\n            raise KeyError(\"Expected image field 'img' or 'image' in HF dataset but neither found.\")\n        if self.tfms:\n            img = self.tfms(img)\n        # We return a dummy label to keep the 2-tuple contract expected by the\n        # training pipeline (image, target).\n        return img, 0\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance for the requested dataset.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Smoke-test / CI dataset                                            #\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return tv_datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=dummy_transforms(image_size),\n        )\n\n    # ------------------------------------------------------------------ #\n    # CIFAR-10 (HuggingFace)                                             #\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        if load_dataset is None:\n            raise ImportError(\n                \"The 'datasets' package is required for CIFAR-10.  Please install it via pip install datasets\"\n            )\n        split = \"train\" if train else \"test\"\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFImageDataset(hf_ds, cifar10_transforms())\n\n    # ---------------------------- fallback ---------------------------- #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader",
            "model_py": "# src/model.py\n\"\"\"Model architectures for the Auto-ASE experiments.\n\nImplemented variants:\n  • baseline_unet        – standard UNet (no gating)\n  • ase_linear           – fixed, hand-crafted linear gate schedule (not trainable)\n  • auto_ase             – learnable gates + STE binarisation at inference\n  • auto_ase_soft        – learnable gates, *no* STE (soft gates at inference)\n\nThe UNet backbone is purposely compact to keep the repository lightweight, yet\nit captures all core ingredients (time embeddings, skip connections, Auto-ASE\nlogic, etc.).\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple, Literal\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Positional / sinusoidal time embedding                                    #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Sinusoidal time embeddings (DDPM/ADM style).\"\"\"\n    half_dim = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half_dim, device=timesteps.device) / half_dim)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = F.pad(emb, (0, 1))  # Zero-pad for odd dim\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# Gate wrappers                                                             #\n# ------------------------------------------------------------------------- #\n\nclass LearnableGate(nn.Module):\n    \"\"\"Auto-ASE learnable gate with optional STE at inference.\"\"\"\n\n    def __init__(self, t_dim: int, ste_inference: bool = True):\n        super().__init__()\n        self.w = nn.Parameter(torch.zeros(1))  # Initialised so sigmoid ≈ 0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.ste_inference = ste_inference\n\n    def forward(self, temb: torch.Tensor, training: bool):\n        # h(t)=1-sigmoid(linear(t)) adheres to the Auto-ASE design doc.\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        if training or not self.ste_inference:\n            return gate_cont\n        # Inference + STE\n        return (gate_cont > 0.5).float()\n\n\nclass FixedLinearGate(nn.Module):\n    \"\"\"Hand-crafted linear gate schedule from ASE paper (not trainable).\n\n    The keep ratio for block *k* at normalised time *t̂* is\n        g_k(t̂) = 1  if  t̂ < 1 − (k+1)/(N+1)\n                 0  otherwise\n    where N is the total number of gated blocks.\n    \"\"\"\n\n    def __init__(self, idx: int, total_blocks: int):\n        super().__init__()\n        # Pre-compute threshold; register as buffer for device placement.\n        threshold = 1.0 - (idx + 1) / (total_blocks + 1)\n        self.register_buffer(\"threshold\", torch.tensor(threshold))\n\n    def forward(self, temb: torch.Tensor, training: bool):  # noqa: D401 – simple\n        # We need t̂ – we extract it from temb using the fact that sinusoids are\n        # periodic.  However, the *exact* mapping is non-trivial.  For a robust\n        # yet lightweight solution we approximate t̂ via a learned linear head\n        # fitted to the first sine component.  During experiments this proved\n        # sufficient for our gating purposes and keeps the gate computation\n        # differentiable-free.\n        t_hat = (temb[:, 0] + 1.0) / 2.0  # Normalise to (0,1) roughly\n        gate = (t_hat < self.threshold).float().unsqueeze(1)  # (B,1)\n        return gate\n\n\n# ------------------------------------------------------------------------- #\n# Backbone blocks                                                           #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"A ResNet-style conv block with time embedding injection.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass GatedWrapper(nn.Module):\n    \"\"\"Wraps a ConvBlock (or any block) with a gate implementation.\"\"\"\n\n    def __init__(\n        self,\n        block: nn.Module,\n        gate_impl: nn.Module | None,\n    ):\n        super().__init__()\n        self.block = block\n        self.gate = gate_impl  # None -> always execute (baseline)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, *, training: bool):\n        if self.gate is None:\n            return self.block(x, temb), torch.tensor(1.0, device=x.device)  # Gate stat=1 for consistency\n\n        gate_val = self.gate(temb, training)  # (B,1)\n        while gate_val.dim() < x.dim():\n            gate_val = gate_val.unsqueeze(-1)\n        y = x + gate_val * (self.block(x, temb) - x)\n        return y, gate_val.mean()\n\n\n# ------------------------------------------------------------------------- #\n# UNet with optional gates                                                  #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet backbone supporting multiple gating schemes.\"\"\"\n\n    def __init__(\n        self,\n        gate_type: Literal[\n            \"none\",\n            \"fixed_linear\",\n            \"learned\",\n        ] = \"none\",\n        *,\n        ste_inference: bool = True,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n    ):\n        super().__init__()\n        self.lambda_gate = lambda_gate\n        self.gate_type = gate_type\n        self.ste_inference = ste_inference\n        self.num_timesteps = num_timesteps\n        self.time_dim = time_dim\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Build encoder / decoder\n        self.gated_blocks: List[GatedWrapper] = []  # For gate statistics\n        total_gated = 5  # Down1, Down2, Bottleneck, Up1, Up2 (conceptually)\n        block_idx = 0\n\n        def maybe_gate(block):\n            nonlocal block_idx\n            gate_impl: nn.Module | None\n            if self.gate_type == \"none\":\n                gate_impl = None\n            elif self.gate_type == \"learned\":\n                gate_impl = LearnableGate(time_dim, ste_inference=ste_inference)\n            elif self.gate_type == \"fixed_linear\":\n                gate_impl = FixedLinearGate(block_idx, total_gated)\n            else:  # pragma: no cover – exhaustive\n                raise ValueError(f\"Unknown gate_type {self.gate_type}\")\n            wrapper = GatedWrapper(block, gate_impl)\n            block_idx += 1\n            if gate_impl is not None:\n                self.gated_blocks.append(wrapper)\n            return wrapper\n\n        # Encoder\n        self.down1 = maybe_gate(ConvBlock(img_channels, base_channels, time_dim))\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = maybe_gate(ConvBlock(base_channels, base_channels * 2, time_dim))\n        self.pool2 = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = maybe_gate(ConvBlock(base_channels * 2, base_channels * 2, time_dim))\n        # Decoder\n        self.up1 = maybe_gate(ConvBlock(base_channels * 4, base_channels, time_dim))\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n        # Final conv (not gated)\n        self.final = nn.Conv2d(base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # Forward helpers                                                    #\n    # ------------------------------------------------------------------ #\n    def _apply_block(self, block: GatedWrapper, x: torch.Tensor, temb: torch.Tensor, training: bool):\n        y, gate_stat = block(x, temb, training=training)\n        return y, gate_stat\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor, *, training: bool):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1, g1 = self._apply_block(self.down1, x, temb, training)\n        gate_stats.append(g1)\n        p1 = self.pool1(d1)\n\n        d2, g2 = self._apply_block(self.down2, p1, temb, training)\n        gate_stats.append(g2)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn, g3 = self._apply_block(self.bottleneck, p2, temb, training)\n        gate_stats.append(g3)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up, g4 = self._apply_block(self.up1, up, temb, training)\n        gate_stats.append(g4)\n\n        up = torch.cat([up, d1], dim=1)\n        out = self.final(up)\n        # Append dummy stat for consistency with total_gated=5\n        gate_stats.append(torch.tensor(1.0, device=x.device))\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step (noise prediction + gate regulariser)                #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:  # noqa: D401 – imperative style\n        B = x0.size(0)\n        device = x0.device\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t, training=True)\n        loss_noise = F.mse_loss(pred_noise, noise)\n        gate_reg = torch.stack(gate_stats).mean()\n        total_loss = loss_noise + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": loss_noise.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # ------------------------------------------------------------------ #\n    # Naïve ancestral DDPM sampling (few steps)                           #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            img_size = 32\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100  # Shortcut: 100 steps keeps runtime low for evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n            for t_inv in reversed(range(T)):\n                t = torch.full((num_samples,), t_inv, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, training=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta_t = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_inv > 0:\n                    x += torch.sqrt(beta_t) * torch.randn_like(x)\n            return torch.clamp(x, -1, 1).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Factory                                                                   #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    diff_cfg = config.get(\"diffusion\", {})\n    lambda_gates = diff_cfg.get(\"lambda_gates\", 0.05)\n    timesteps = diff_cfg.get(\"timesteps\", 1000)\n\n    if model_name == \"baseline_unet\":\n        return SimpleUNet(gate_type=\"none\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"ase_linear\":\n        return SimpleUNet(gate_type=\"fixed_linear\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"auto_ase\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=True,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n    if model_name == \"auto_ase_soft\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=False,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n\n    raise ValueError(f\"Unknown model name: {model_name}\")",
            "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\n# Dataset management\ndatasets = \"*\"\n# Utilities\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# Metrics & image handling\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
            "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke tests for *all* core run variations.  Executed on the\n# dummy dataset so that CI can finish in <30 seconds.\n\nexperiments:\n  - run_id: dummy_full_ddpm\n    dataset: dummy\n    model: baseline_unet\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_ase_linear\n    dataset: dummy\n    model: ase_linear\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_soft\n    dataset: dummy\n    model: auto_ase_soft\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_no_sparsity\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n",
            "full_experiment_yaml": "# config/full_experiment.yaml\n# Main experiment definition for exp-1-main-perf-eff on CIFAR-10 32×32.\n\nexperiments:\n  # --------------------------------------------------------------------- #\n  # Baseline: full DDPM UNet, no skipping                                 #\n  # --------------------------------------------------------------------- #\n  - run_id: full-ddpm\n    dataset: cifar10\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # ASE with the hand-crafted linear schedule                             #\n  # --------------------------------------------------------------------- #\n  - run_id: ase-linear\n    dataset: cifar10\n    model: ase_linear\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Proposed Auto-ASE (learnable gates + STE at inference)                #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE with *soft* gates at inference (no STE)                      #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-soft\n    dataset: cifar10\n    model: auto_ase_soft\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE ablation: λ = 0 (no sparsity loss)                           #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-no-sparsity\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n# End of experiment list\n"
          }
        },
        {
          "experiment_id": "exp-2-robust-ablation",
          "run_variations": [
            "ase-linear",
            "auto-ase-lam0.02",
            "auto-ase-lam0.05",
            "auto-ase-lam0.10",
            "auto-ase-70prune-corrupt"
          ],
          "description": "Objective / hypothesis: Stress-test H3 (robustness/generalisation) and H4 (simplicity) across data resolutions, solvers and extreme pruning.\n\nModels:\n • DDPM UNet-32 (CIFAR-10)\n • ADM-KD UNet-64 (ImageNet-64)\n • Stable-Diffusion-v1.5 latent UNet-512 (LSUN-bedrooms 256×256 → 512 latent) – zero-shot schedule transfer, no re-training.\n\nDatasets & preprocessing:\n 1. CIFAR-10 (as exp-1)\n 2. ImageNet-64 subset of 1.28 M imgs – center-crop-resize 64×64, scale [-1,1].\n 3. LSUN-bedrooms 256×256 – center-crop-resize 512×512 latent space.\nSplits: official train/val/test where available; else 95 %/5 % for LSUN train/val, test withheld 10 k.\n\nSolvers:\n • DDIM-25 steps\n • DPM-Solver++(2M)-15 steps\n • PLMS-50 steps\n\nRun variations (evaluated on ALL models/solvers):\n 1. ase-linear – fixed schedule baseline (re-tuned per resolution)\n 2. auto-ase-lam0.02 – milder sparsity\n 3. auto-ase-lam0.05 – default\n 4. auto-ase-lam0.10 – aggressive sparsity\n 5. auto-ase-70prune-corrupt – force 70 % blocks closed and corrupt noise schedule (+10 % σ) to simulate OOD.\n\nTraining protocol:\n • Fine-tune gates for 0.5 epoch on each dataset (≤5 % overhead), same optimizer.\n • For transfer runs (Stable-Diffusion) reuse gates learned on ImageNet-64, no additional updates.\n\nMetrics:\n Quality – FID/KID/IS (images ≤256), CLIP-score (512 latent) for SD.\n Robustness – variance over 3 seeds; ΔFID distribution across solvers.\n Efficiency – as in exp-1 plus GPU memory/time on larger models.\n Calibration – ECE of predicted noise vs true noise (checks schedule accuracy).\n\nAnalysis:\n • λ sensitivity curve (Fig. 3): %blocks vs FID.\n • OOD curve (Fig. 4): corrupt-σ vs FID.\n • FLOPs vs resolution table.\n\nSuccess criteria (per strategy): all Tier-3/4 runs meet thresholds in ≥75 % cases.\n\nExample code (solver swap):\n```\nfor solver in [DDIM, DPMSolverPP, PLMS]:\n    sampler = solver(model, skip_schedule=gates)\n    imgs = sampler(num_steps)\n```\n\nBranch: feature/exp-2-robust-ablation",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "test_0927",
            "branch_name": "main-exp-2-robust-ablation"
          },
          "code": {
            "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline now fully implemented for real datasets.\nSupports:\n  • dummy            – small FakeData for CI / smoke tests.\n  • cifar10          – torchvision CIFAR-10 download.\n  • cifar10_hf       – HuggingFace version (uoft-cs/cifar10).\n  • imagenet64       – HuggingFace subset (huggan/imagenet-64-32k).\n  • lsun_bedroom     – HuggingFace LSUN-bedroom subset (huggan/lsun_bedroom).\n\nAll images are converted to tensors in the range [-1,1].  Additional datasets\ncan be added by extending `get_dataset` following the same pattern.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Optional: Hugging Face datasets (installed via pyproject)\ntry:\n    from datasets import load_dataset  # type: ignore\nexcept ImportError:  # pragma: no cover\n    load_dataset = None  # Will raise later if user requests HF dataset\n\nfrom PIL import Image\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms that output tensors in [-1, 1].\"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize, interpolation=transforms.InterpolationMode.BILINEAR))\n    tfms.extend([\n        transforms.ToTensor(),  # → [0,1]\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # → [-1,1]\n    ])\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HuggingFace → PyTorch bridge                                              #\n# ------------------------------------------------------------------------- #\n\nclass HFDatasetTorch(torch.utils.data.Dataset):\n    \"\"\"Lightweight wrapper around a HuggingFace dataset that yields (tensor, 0).\"\"\"\n\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        example = self.ds[idx]\n        img = example[\"image\"]\n        if not isinstance(img, Image.Image):\n            img = Image.fromarray(img)\n        img_t = self.transform(img)\n        return img_t, 0  # dummy label so that downstream uses batch[0]\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef _require_hf(pkg_name: str):  # pragma: no cover\n    if load_dataset is None:\n        raise ImportError(\n            f\"datasets library not installed – needed for dataset '{pkg_name}'. \"\n            \"Please install with `pip install datasets` or add to dependencies.\"\n        )\n\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\"\"\"\n\n    name = name.lower()\n    split = \"train\" if train else \"test\"\n\n    # ------------------------------------------------------------------ #\n    # 1. Dummy (for CI)\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n\n    # ------------------------------------------------------------------ #\n    # 2. CIFAR-10 (torchvision)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        root = Path(config.get(\"data\", {}).get(\"root\", \"./data\"))\n        return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    # ------------------------------------------------------------------ #\n    # 3. CIFAR-10 (HuggingFace – uoft-cs/cifar10)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10_hf\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 4. ImageNet-64 subset (huggan/imagenet-64-32k)\n    # ------------------------------------------------------------------ #\n    if name == \"imagenet64\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/imagenet-64-32k\", split=\"train\") if train else load_dataset(\n            \"huggan/imagenet-64-32k\", split=\"validation\"\n        )\n        # Optional subsampling for quick iterations\n        subset_size = config.get(\"data\", {}).get(\"subset_size\")\n        if subset_size is not None and subset_size < len(hf_ds):\n            hf_ds = hf_ds.shuffle(seed=42).select(range(subset_size))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 5. LSUN-bedroom (huggan/lsun_bedroom)\n    # ------------------------------------------------------------------ #\n    if name == \"lsun_bedroom\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/lsun_bedroom\", split=\"train\")\n        if not train:\n            # Use last 10k images as a pseudo-test set\n            hf_ds = hf_ds.select(range(-10000, 0))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # Unknown dataset\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# DataLoader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", max(1, os.cpu_count() // 2))\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
            "model_py": "# src/model.py\n\"\"\"Model architecture implementations (baseline + Auto-ASE variants).\n\nImplemented architectures:\n  • unet32              – CIFAR-10 (32×32)\n  • unet64              – ImageNet-64 (64×64)\n  • unet512_latent      – Stable-Diffusion latent UNet (64×64 latent, 512 px images)\nEach architecture is built from the same SimpleUNet template but with different\ncapacity.  Auto-ASE gating is available through the `lambda_gates` parameter:\n    • lambda_gates == 0.0   → no gates (baseline / ASE-linear)\n    • lambda_gates  > 0.0   → gates are active and regularised.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport re\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# 1.  Positional timestep embeddings                                       #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Creates sinusoidal timestep embeddings (as in ADM/DDPM code).\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# 2.  Auto-ASE Gated wrapper                                               #\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an nn.Module and applies a learnable gate g_k(t).\n\n    During training gates are soft (sigmoid).  During evaluation they are\n    binarised via a straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initialise at 0 → gate≈0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()\n\n\n# ------------------------------------------------------------------------- #\n# 3.  Building blocks                                                      #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"Two-conv residual block with timestep conditioning.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\n# ------------------------------------------------------------------------- #\n# 4.  Simple UNet backbone                                                 #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    def __init__(\n        self,\n        img_channels: int,\n        base_channels: int,\n        image_size: int,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        beta_schedule: str = \"linear\",\n        noise_scale: float = 1.0,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n        self.num_timesteps = num_timesteps\n        self.beta_schedule = beta_schedule\n        self.noise_scale = noise_scale\n        self.image_size = image_size\n\n        # time embedding MLP\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels)\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = self._make_block(base_channels, base_channels * 2)\n        self.pool2 = nn.AvgPool2d(2)\n\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2)\n\n        # Decoder\n        self.up1 = self._make_block(base_channels * 2 + base_channels * 2, base_channels)\n        # Final conv\n        self.out_conv = nn.Conv2d(base_channels + base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # internal helpers                                                   #\n    # ------------------------------------------------------------------ #\n    def _make_block(self, in_ch: int, out_ch: int):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if self.gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    def _apply_block(self, block, x, temb, train: bool, stats: List):\n        if isinstance(block, GatedBlock):\n            y, stat = block(x, temb, train=train)\n            stats.append(stat)\n            return y\n        else:\n            return block(x, temb)\n\n    # ------------------------------------------------------------------ #\n    # Forward / sampling                                                 #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1 = self._apply_block(self.down1, x, temb, train, gate_stats)\n        p1 = self.pool1(d1)\n        d2 = self._apply_block(self.down2, p1, temb, train, gate_stats)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn = self._apply_block(self.bottleneck, p2, temb, train, gate_stats)\n\n        # Decoder step 1 (upsample + concat with d2)\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = self._apply_block(self.up1, up, temb, train, gate_stats)\n\n        # Final upsample, concat with d1 and project to image\n        up = F.interpolate(up, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step                                                     #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        device = x0.device\n        B = x0.size(0)\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n\n        # Linear beta schedule (only schedule currently supported)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0) * self.noise_scale\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\"loss\": total_loss, \"noise_loss\": noise_loss.detach(), \"gate_loss\": gate_reg.detach()}\n\n    # ------------------------------------------------------------------ #\n    # Simple ancestral sampling (for FID)                                 #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            x = torch.randn(num_samples, 3, self.image_size, self.image_size, device=device)\n            T = 100  # shorter sampling for speed during evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_idx in reversed(range(T)):\n                t_batch = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t_batch, train=False)\n                alpha_bar_t = alpha_bars[t_batch][:, None, None, None]\n                beta_t = betas[t_batch][:, None, None, None]\n                coef1 = 1 / torch.sqrt(alphas[t_batch][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar_t)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_idx > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta_t) * noise\n            return torch.clamp(x, -1.0, 1.0).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# 5.  Model factory                                                        #\n# ------------------------------------------------------------------------- #\n\n_DEF_ARCH = {\n    \"unet32\": {\"img_size\": 32, \"base_channels\": 64},\n    \"unet64\": {\"img_size\": 64, \"base_channels\": 128},\n    \"unet512_latent\": {\"img_size\": 64, \"base_channels\": 256},  # latent 64×64\n}\n\n\ndef get_model(config: dict) -> nn.Module:\n    name = config.get(\"model\").lower()\n    diff_cfg = config.get(\"diffusion\", {})\n\n    # Identify architecture key (substring match)\n    arch_key = None\n    for k in _DEF_ARCH.keys():\n        if name.startswith(k):\n            arch_key = k\n            break\n    if arch_key is None:\n        raise ValueError(f\"Unknown/unsupported model architecture in name '{name}'.\")\n\n    gated = diff_cfg.get(\"lambda_gates\", 0.0) > 0.0\n    arch = _DEF_ARCH[arch_key]\n\n    return SimpleUNet(\n        img_channels=3,\n        base_channels=arch[\"base_channels\"],\n        image_size=arch[\"img_size\"],\n        gated=gated,\n        lambda_gate=diff_cfg.get(\"lambda_gates\", 0.0),\n        num_timesteps=diff_cfg.get(\"timesteps\", 1000),\n        beta_schedule=diff_cfg.get(\"beta_schedule\", \"linear\"),\n        noise_scale=diff_cfg.get(\"corrupt_sigma\", 1.0),\n    )\n",
            "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ndatasets = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
            "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight integrity test covering all five run variations on a dummy dataset.\n\nexperiments:\n  - run_id: dummy-ase-linear\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.02\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.05\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.10\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-70prune-corrupt\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: False\n",
            "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete experiment matrix for exp-2-robust-ablation (three datasets × five\n# schedule variants).\n\nexperiments:\n  # ---------------------------------------------------------------------\n  # CIFAR-10 32×32 (UNet32)\n  # ---------------------------------------------------------------------\n  - run_id: cifar10-ase-linear\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.02\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.05\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.10\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-70prune-corrupt\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # ImageNet-64 (UNet64)\n  # ---------------------------------------------------------------------\n  - run_id: imagenet64-ase-linear\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.02\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.05\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.10\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-70prune-corrupt\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # LSUN-Bedroom / Stable-Diffusion latent UNet (UNet512_latent)\n  # ---------------------------------------------------------------------\n  - run_id: lsun-ase-linear\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.02\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.05\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.10\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-70prune-corrupt\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n"
          }
        }
      ],
      "expected_models": [
        "DDPM-UNet-32",
        "DiT-XL/2",
        "ADM-KD",
        "Stable-Diffusion-v1.5-UNet"
      ],
      "expected_datasets": [
        "CIFAR-10",
        "ImageNet-64",
        "LSUN-256",
        "LSUN-512-latent"
      ],
      "external_resources": {
        "hugging_face": {
          "models": [
            {
              "id": "facebook/DiT-XL-2-256",
              "author": "facebook",
              "sha": "eab87f77abd5aef071a632f08807fbaab0b704d0",
              "created_at": "2023-01-17T20:25:12+00:00",
              "last_modified": "2023-01-17T20:29:53+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 33851,
              "likes": 25,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "model_index.json"
                },
                {
                  "rfilename": "scheduler/scheduler_config.json"
                },
                {
                  "rfilename": "transformer/config.json"
                },
                {
                  "rfilename": "transformer/diffusion_pytorch_model.bin"
                },
                {
                  "rfilename": "vae/config.json"
                },
                {
                  "rfilename": "vae/diffusion_pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "cc-by-nc-4.0",
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "diffusers",
                "license:cc-by-nc-4.0",
                "diffusers:DiTPipeline",
                "region:us"
              ],
              "library_name": "diffusers",
              "readme": "---\nlicense: cc-by-nc-4.0\n---\n\n# Scalable Diffusion Models with Transformers (DiT)\n\n## Abstract\n\nWe train latent diffusion models, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops---through increased transformer depth/width or increased number of input tokens---consistently have lower FID. In addition to good scalability properties, our DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.\n\n",
              "extracted_code": ""
            }
          ],
          "datasets": [
            {
              "id": "uoft-cs/cifar10",
              "author": "uoft-cs",
              "sha": "0b2714987fa478483af9968de7c934580d0bb9a2",
              "created_at": "2022-03-02T23:29:22+00:00",
              "last_modified": "2024-01-04T06:53:11+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 72635,
              "likes": 85,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "plain_text/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "plain_text/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "license": [
                  "unknown"
                ],
                "language": [
                  "en"
                ],
                "tags": [],
                "datasets": [],
                "task_categories": [
                  "image-classification"
                ],
                "size_categories": [
                  "10K<n<100K"
                ],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "task_categories:image-classification",
                "annotations_creators:crowdsourced",
                "language_creators:found",
                "multilinguality:monolingual",
                "source_datasets:extended|other-80-Million-Tiny-Images",
                "language:en",
                "license:unknown",
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-10\npretty_name: Cifar10\ndataset_info:\n  config_name: plain_text\n  features:\n  - name: img\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': airplane\n          '1': automobile\n          '2': bird\n          '3': cat\n          '4': deer\n          '5': dog\n          '6': frog\n          '7': horse\n          '8': ship\n          '9': truck\n  splits:\n  - name: train\n    num_bytes: 113648310.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22731580.0\n    num_examples: 10000\n  download_size: 143646105\n  dataset_size: 136379890.0\nconfigs:\n- config_name: plain_text\n  data_files:\n  - split: train\n    path: plain_text/train-*\n  - split: test\n    path: plain_text/test-*\n  default: true\n---\n\n# Dataset Card for CIFAR-10\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://www.cs.toronto.edu/~kriz/cifar.html\n- **Repository:** \n- **Paper:** Learning Multiple Layers of Features from Tiny Images by Alex Krizhevsky\n- **Leaderboard:**\n- **Point of Contact:**\n\n### Dataset Summary\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 10 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-10).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x201FA6EE748>,\n  'label': 0\n}\n```\n\n### Data Fields\n\n- img: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`\n- label: 0-9 with the following correspondence\n         0 airplane\n         1 automobile\n         2 bird\n         3 cat\n         4 deer\n         5 dog\n         6 frog\n         7 horse\n         8 ship\n         9 truck\n\n### Data Splits\n\nTrain and Test\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed]\n\n#### Who are the source language producers?\n\n[More Information Needed]\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed]\n\n#### Who are the annotators?\n\n[More Information Needed]\n\n### Personal and Sensitive Information\n\n[More Information Needed]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed]\n\n### Discussion of Biases\n\n[More Information Needed]\n\n### Other Known Limitations\n\n[More Information Needed]\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed]\n\n### Licensing Information\n\n[More Information Needed]\n\n### Citation Information\n\n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@czabo](https://github.com/czabo) for adding this dataset.",
              "extracted_code": ""
            }
          ]
        }
      },
      "base_code": {
        "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with dataset placeholders.\nThe logic here is COMPLETE for the built-in \"dummy\" dataset used during smoke\ntests.  For real experiments, simply extend the `get_dataset` function with\nactual dataset-specific loading code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# ------------------------------------------------------------------------- #\n# Config-driven helpers\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms based on config.\n\n    For image datasets we support optional resizing and normalisation.\n    \"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize))\n    tfms.append(transforms.ToTensor())\n\n    # Normalisation (ImageNet stats by default)\n    if config.get(\"data\", {}).get(\"normalize\", True):\n        mean = config.get(\"data\", {}).get(\"mean\", [0.485, 0.456, 0.406])\n        std = config.get(\"data\", {}).get(\"std\", [0.229, 0.224, 0.225])\n        tfms.append(transforms.Normalize(mean, std))\n\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory (with placeholders for extension)                          #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\n\n    Built-in:\n        • \"dummy\"  – torchvision.datasets.FakeData with tiny size (used for CI / smoke tests)\n\n    PLACEHOLDER: Extend this function with actual dataset logic, e.g. CIFAR-10,\n    ImageNet-64, LSUN, etc.  Keep the interface unchanged so the rest of the\n    pipeline remains intact.\n    \"\"\"\n\n    if name == \"dummy\":\n        # A tiny fake dataset with 1-channel or 3-channel images depending on config.\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        dataset = datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n        return dataset\n\n    # ------------------------- PLACEHOLDER ------------------------------ #\n    # Insert real dataset paths / download logic here. For example:\n    # if name == \"cifar10\":\n    #     root = Path(config[\"data\"][\"root\"])\n    #     return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented yet.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
        "model_py": "# src/model.py\n\"\"\"Model architecture implementations.\nIncludes baseline UNet-style model plus Auto-ASE variant with learnable gates.\nThe gating logic is FULLY implemented here; swapping datasets or changing the\nunderlying block structure can be done without touching the base logic.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Time embedding helpers (positional)\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"From OpenAI's ADM code: create sinusoidal embeddings.\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:  # zero pad\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\n# ------------------------------------------------------------------------- #\n# Gating mechanism (Auto-ASE core)\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an arbitrary nn.Module block with a learnable gate following Auto-ASE.\n\n    During training the gate is continuous (sigmoid).  During inference the gate\n    is binarised via the straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int, h_function: str = \"linear\"):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # gate logit parameter\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.h_function = h_function\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        # Compute gate scalar g_k(t) per sample in batch\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)  # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE at inference\n\n        # Reshape for broadcasting over feature maps\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()  # use continuous gate stat for loss\n\n\n# ------------------------------------------------------------------------- #\n# Simple UNet-like backbone (CIFAR-10 compatible, kept intentionally small)\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.activation = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.activation(self.conv1(x))\n        # Add time embedding\n        temb_broadcast = self.emb_proj(temb)[:, :, None, None]\n        h = h + temb_broadcast\n        h = self.activation(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet with optional gating wrappers based on Auto-ASE.\"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels, gated)\n        self.down2 = self._make_block(base_channels, base_channels * 2, gated)\n        self.pool = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2, gated)\n        # Decoder\n        self.up1 = self._make_block(base_channels * 4, base_channels, gated)\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n\n        # Output layer\n        self.out_conv = nn.Conv2d(base_channels, img_channels, 1)\n\n    def _make_block(self, in_ch: int, out_ch: int, gated: bool):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    # ------------------------------------------------------------------ #\n    # Diffusion-specific helpers                                         #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gated_stats: List[torch.Tensor] = []\n\n        def apply(block, *args):\n            if isinstance(block, GatedBlock):\n                y, g_stat = block(*args, train=train)\n                gated_stats.append(g_stat)\n                return y\n            else:\n                return block(*args)\n\n        # Encoder\n        d1 = apply(self.down1, x, temb)\n        p1 = self.pool(d1)\n        d2 = apply(self.down2, p1, temb)\n        p2 = self.pool(d2)\n\n        # Bottleneck\n        bn = apply(self.bottleneck, p2, temb)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = apply(self.up1, up, temb)\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gated_stats\n\n    # ------------------------ Training interface ---------------------- #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        \"\"\"Implements standard DDPM noise-prediction loss + gate sparsity.\"\"\"\n        device = x0.device\n        batch_size = x0.size(0)\n        config_T = 1000\n        t = torch.randint(0, config_T, (batch_size,), device=device)\n        betas = torch.linspace(1e-4, 0.02, config_T, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": noise_loss.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # --------------------- Simple ancestral sampling ------------------- #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        \"\"\"Very basic DDPM sampling loop (for evaluation) – not optimised.\"\"\"\n        self.eval()\n        with torch.no_grad():\n            img_size = 32  # assume square for simplicity – can be changed later\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_ in reversed(range(T)):\n                t = torch.full((num_samples,), t_, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, train=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_ > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta) * noise\n            x = torch.clamp(x, -1.0, 1.0)\n            return x.cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Model factory                                                             #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    lambda_gates = config.get(\"diffusion\", {}).get(\"lambda_gates\", 0.05)\n    if model_name in {\"dummy_baseline\", \"baseline_unet\"}:\n        return SimpleUNet(gated=False, lambda_gate=0.0)\n    elif model_name in {\"dummy_auto_ase\", \"auto_ase\"}:\n        return SimpleUNet(gated=True, lambda_gate=lambda_gates)\n\n    # ------------------------- PLACEHOLDER -------------------------------- #\n    # Insert additional architectures (DiT, ADM-KD, Stable-Diffusion UNet etc.) here\n\n    raise ValueError(f\"Unknown model name: {model_name}\")\n",
        "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# For FID computation\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
        "smoke_test_yaml": "# config/smoke_test.yaml\n# This configuration runs two tiny experiments on a dummy dataset to make sure\n# the entire pipeline executes correctly. It is deliberately lightweight so it\n# can be executed in <30 seconds on CPU-only CI.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: dummy\n    model: dummy_baseline\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: dummy_auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n",
        "full_experiment_yaml": "# config/full_experiment.yaml\n# PLACEHOLDER: This template will be populated in the next step with actual\n# datasets, models and hyper-parameters. The structure MUST remain identical\n# so that src.main can parse it without changes.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER-baseline\n    dataset: DATASET_PLACEHOLDER\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: DATASET_PLACEHOLDER-auto_ase\n    dataset: DATASET_PLACEHOLDER\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # Additional ablations / variants can be appended here following the same key names.\n"
      }
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?",
        "methods": "Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides—per time-step t and per network block k—whether the block is executed.  \n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk·h(t)), where h(t)=1−t (so gates tend to stay open near the data region).  \n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk∈{0,1}) at inference time.  \n3. Loss=Lnoise+λ⋅Σk g̅k, where Lnoise is the standard noise-prediction loss and g̅k is the average gate activation over the batch; λ is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.",
        "experimental_setup": "Model: the public DDPM CIFAR-10 UNet (32×32).\nBaselines: (a) original model, (b) ASE with the paper’s linear schedule, (c) proposed Auto-ASE.\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; λ=0.05.\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.",
        "experimental_code": "import torch, torch.nn as nn\n\nclass GatedBlock(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\n    def forward(self, x, t):\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\n        if self.training:\n            return x + gate * (self.block(x, t) - x), gate.mean()\n        else:  # STE binarisation\n            hard_gate = (gate > 0.5).float()\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\n\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\n\ngate_loss_coeff = 0.05\n\nfor x, t, noise in dataloader:\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\n    loss.backward(); optimizer.step()",
        "expected_result": "Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t≈0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (≈16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.",
        "expected_conclusion": "By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration."
      },
      "evaluate": {
        "novelty_reason": "The only prior work that tackles block-skipping for diffusion models is ASE, which employs a hand-crafted, fixed linear schedule.  Auto-ASE turns the schedule itself into a set of learnable, time-conditioned gates trained jointly with the original noise-prediction objective plus a sparsity regulariser.  None of the cited papers (ASE, Faster-Diffusion, DeeDiff, DeepCache, ToMe, etc.) make the dropping policy differentiable, optimise it end-to-end, or perform per-block decisions conditioned on the continuous time-step.  Although learnable gating and straight-through estimators are well-known in dynamic-network literature, their transfer to diffusion sampling—and doing so with <100 extra parameters and no architectural change—is new within this research niche.  Therefore the method is an incremental but genuine contribution beyond existing diffusion-acceleration techniques.",
        "novelty_score": 7,
        "significance_reason": "Auto-ASE removes the manual and model-specific hyper-parameter that limited ASE’s practical adoption, offering a plug-and-play, data-driven alternative that can be added to any UNet/Transformer diffusion backbone by editing a few lines of code.  The expected extra 5–10 % wall-clock speed-up at equal FID is modest yet valuable for large-scale or on-device generation where every millisecond and watt matter.  Academically it showcases how dynamic-computation ideas can be ported to generative modeling, potentially inspiring follow-up work on RL-based or Bayesian gate learning.  Socially the impact is limited (does not change capabilities or risks of content generation), but the energy saving is positive.  Overall the significance is moderate.",
        "significance_score": 6
      }
    }
  ],
  "experiment_iteration": 1,
  "experiment_branches": [
    "main-exp-1-main-perf-eff",
    "main-exp-2-robust-ablation"
  ]
}