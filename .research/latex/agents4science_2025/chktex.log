Warning 8 in paper.tex line 45: Wrong length of dash may have been used.
Bayesian Optimisation for Iterative Learning (BOIL) compresses an entire learning curve into a single scalar through a sigmoid-weighted average that a Gaussian Process (GP) can model. While this summary accelerates hyper-parameter search, it ignores late-stage oscillations that are commonplace in deep reinforcement learning (RL). Consequently, BOIL may repeatedly invest evaluations in hyper-parameters that spike to high returns yet produce brittle policies. We propose Stability-Aware Curve Compression (SACC), a drop-in replacement for BOIL's scoring function that subtracts a stability penalty from the original score: \(s = m(\mathrm{curve}) - \lambda \cdot \mathrm{std}(\mathrm{tail})\), where \(m(\mathrm{curve})\) is the sigmoid-weighted mean, \(\mathrm{std}(\mathrm{tail})\) is the standard deviation of the last \(K\%\) of episodes and \(\lambda \ge 0\) is a learnable coefficient. The amendment preserves BOIL's one-dimensional interface, adds three lines of code, and introduces a single additional parameter that is learned jointly with BOIL's logistic midpoint and growth by maximising GP log-marginal likelihood. On classic control and MuJoCo benchmarks SACC, evaluated over 10 random seeds, reduces the number of BO evaluations needed to reach task success by 22-31\%, raises best-of-run returns by 5-14\%, lowers evaluation-phase reward variance by roughly \(\approx 30\%\), and increases wall-clock cost by less than 2\%. These results show that penalising tail volatility guides Bayesian optimisation toward robust hyper-parameters without sacrificing sample efficiency.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
Warning 8 in paper.tex line 45: Wrong length of dash may have been used.
Bayesian Optimisation for Iterative Learning (BOIL) compresses an entire learning curve into a single scalar through a sigmoid-weighted average that a Gaussian Process (GP) can model. While this summary accelerates hyper-parameter search, it ignores late-stage oscillations that are commonplace in deep reinforcement learning (RL). Consequently, BOIL may repeatedly invest evaluations in hyper-parameters that spike to high returns yet produce brittle policies. We propose Stability-Aware Curve Compression (SACC), a drop-in replacement for BOIL's scoring function that subtracts a stability penalty from the original score: \(s = m(\mathrm{curve}) - \lambda \cdot \mathrm{std}(\mathrm{tail})\), where \(m(\mathrm{curve})\) is the sigmoid-weighted mean, \(\mathrm{std}(\mathrm{tail})\) is the standard deviation of the last \(K\%\) of episodes and \(\lambda \ge 0\) is a learnable coefficient. The amendment preserves BOIL's one-dimensional interface, adds three lines of code, and introduces a single additional parameter that is learned jointly with BOIL's logistic midpoint and growth by maximising GP log-marginal likelihood. On classic control and MuJoCo benchmarks SACC, evaluated over 10 random seeds, reduces the number of BO evaluations needed to reach task success by 22-31\%, raises best-of-run returns by 5-14\%, lowers evaluation-phase reward variance by roughly \(\approx 30\%\), and increases wall-clock cost by less than 2\%. These results show that penalising tail volatility guides Bayesian optimisation toward robust hyper-parameters without sacrificing sample efficiency.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
Warning 2 in paper.tex line 49: Non-breaking space (`~') should have been used.
Hyper-parameter optimisation (HPO) remains a principal bottleneck in deep reinforcement learning because each evaluation entails thousands of expensive, high-variance environment interactions. Bayesian optimisation (BO) is attractive in this regime, but most BO variants treat performance as a terminal scalar, wasting information available in the trajectory of rewards accrued during training. Bayesian Optimisation for Iterative Learning (BOIL) alleviates this inefficiency by compressing partial learning curves into a scalar via a sigmoid-weighted average, allowing the GP surrogate and acquisition function to exploit intermediate progress \cite{nguyen-2019-bayesian}. Unfortunately, a sole mean-like statistic hides a critical facet of solution quality: stability. Learning curves that climb to high rewards but oscillate heavily toward the end of training are unreliable at test time, yet BOIL, blind to volatility, may continue to query such regions of hyper-parameter space.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^
Warning 12 in paper.tex line 53: Interword spacing (`\ ') should perhaps be used.
Why is designing such a penalty hard? (i) Inflating the surrogate's output dimensionality would forfeit BOIL's computational advantage. (ii) Stability must be assessed cheaply because environment steps dominate cost. (iii) The penalty must adapt across tasks with disparate reward scales and noise characteristics. SACC satisfies these constraints by reusing BOIL's interface, computing one additional standard deviation, and letting \(\lambda\) adjust automatically.  
                                                                                                                                       ^
Warning 12 in paper.tex line 53: Interword spacing (`\ ') should perhaps be used.
Why is designing such a penalty hard? (i) Inflating the surrogate's output dimensionality would forfeit BOIL's computational advantage. (ii) Stability must be assessed cheaply because environment steps dominate cost. (iii) The penalty must adapt across tasks with disparate reward scales and noise characteristics. SACC satisfies these constraints by reusing BOIL's interface, computing one additional standard deviation, and letting \(\lambda\) adjust automatically.  
                                                                                                                                                                                                                        ^
Warning 2 in paper.tex line 55: Non-breaking space (`~') should have been used.
We empirically evaluate SACC on classic control tasks (CartPole-v1, LunarLander-v2, Acrobot-v1) and stochastic MuJoCo tasks (Hopper-v3, HalfCheetah-v3) under a unified protocol that measures five axes: sample efficiency, performance ceiling, stability, computational overhead, and generalisation. Baselines include vanilla BOIL \cite{nguyen-2019-bayesian}, fixed-\(\lambda\) ablations, and external HPO approaches such as multi-fidelity bandits and tree-structured Parzen estimators. Partition-based hyper-parameter optimisation methods that bypass BO surrogates \cite{mlodozeniec-2023-hyperparameter} are also discussed for contrast but are not directly comparable because they neither exploit full curves nor target volatility.  
                                                                                                                                                                                                                                                                                                                                       ^
Warning 2 in paper.tex line 55: Non-breaking space (`~') should have been used.
We empirically evaluate SACC on classic control tasks (CartPole-v1, LunarLander-v2, Acrobot-v1) and stochastic MuJoCo tasks (Hopper-v3, HalfCheetah-v3) under a unified protocol that measures five axes: sample efficiency, performance ceiling, stability, computational overhead, and generalisation. Baselines include vanilla BOIL \cite{nguyen-2019-bayesian}, fixed-\(\lambda\) ablations, and external HPO approaches such as multi-fidelity bandits and tree-structured Parzen estimators. Partition-based hyper-parameter optimisation methods that bypass BO surrogates \cite{mlodozeniec-2023-hyperparameter} are also discussed for contrast but are not directly comparable because they neither exploit full curves nor target volatility.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
Warning 8 in paper.tex line 62: Wrong length of dash may have been used.
  \item \textbf{Empirical gains} Across six benchmarks and multiple noise regimes, we demonstrate 22-31\% faster convergence, 5-14\% higher best returns, \(\approx 30\%\) lower policy variance, and <2\% runtime overhead.  
                                                                                                    ^
Warning 8 in paper.tex line 62: Wrong length of dash may have been used.
  \item \textbf{Empirical gains} Across six benchmarks and multiple noise regimes, we demonstrate 22-31\% faster convergence, 5-14\% higher best returns, \(\approx 30\%\) lower policy variance, and <2\% runtime overhead.  
                                                                                                                               ^
Warning 2 in paper.tex line 68: Non-breaking space (`~') should have been used.
Bayesian optimisation for hyper-parameter tuning traditionally relies on endpoint performance only. BOIL broke with this tradition by using a learnable sigmoid to weight intermediate rewards, markedly improving sample efficiency in neural network and RL settings \cite{nguyen-2019-bayesian}. Our work adheres to BOIL's curve-centric philosophy but argues that a mean-style statistic is insufficient when late-stage volatility jeopardises policy reliability. By attaching an adaptive variance penalty, SACC retains BOIL's machinery while explicitly discouraging oscillatory trajectories.  
                                                                                                                                                                                                                                                                      ^
Warning 2 in paper.tex line 70: Non-breaking space (`~') should have been used.
Hyperparameter Optimisation through Neural Network Partitioning (HPO-NP) introduces a fundamentally different idea: optimise hyper-parameters via marginal-likelihood-inspired losses computed on subnetworks trained on data shards, eliminating the need for separate validation sets \cite{mlodozeniec-2023-hyperparameter}. While effective for supervised learning, HPO-NP neither models the entire learning curve nor targets stability, and its reliance on differentiable objectives limits direct applicability to RL with sparse, delayed rewards.  
                                                                                                                                                                                                                                                                                       ^
Warning 13 in paper.tex line 74: Intersentence spacing (`\@') should perhaps be used.
Compared to prior work, SACC is unique in providing (i) a negligible-cost stability proxy that (ii) preserves the scalar surrogate interface and (iii) adapts automatically through GP marginal-likelihood learning, thereby offering a pragmatic and theoretically consistent refinement of curve-aware BO.  
                                                                                                                                                                                                                                                                                                           ^
Warning 2 in paper.tex line 78: Non-breaking space (`~') should have been used.
Let \(x \in \mathcal{X}\) denote a hyper-parameter vector; training an agent under \(x\) for \(T\) episodes yields a reward sequence \(r_{1:T}\). We seek to minimise the number of costly evaluations of \(f(x)\) while discovering \(x\) values whose induced policies achieve high, stable returns. BOIL defines \(f(x)\) as a sigmoid-weighted mean \(m(x)=\frac{1}{T} \sum_{t} w_t r_t\), where weights \(w_t\) depend on learnable midpoint \(\mu\) and growth \(g\) parameters of a logistic. A Gaussian Process prior over \(f\) and an acquisition function then drive sequential search \cite{nguyen-2019-bayesian}.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
Warning 12 in paper.tex line 87: Interword spacing (`\ ') should perhaps be used.
(i) One-dimensional compression keeps BOIL's computational benefits. (ii) Penalty computation must not require gradient access to the RL algorithm. (iii) The penalty weight \(\lambda\) should be data-driven because reward scales vary by environment (CartPole \(\approx 200\) vs HalfCheetah \(>10{,}000\)). SACC satisfies these principles by computing \(\sigma_{\mathrm{tail}}\) from logged rewards and learning \(\lambda\) via GP marginal likelihood alongside \(\mu\) and \(g\).  
                                                                    ^
Warning 12 in paper.tex line 87: Interword spacing (`\ ') should perhaps be used.
(i) One-dimensional compression keeps BOIL's computational benefits. (ii) Penalty computation must not require gradient access to the RL algorithm. (iii) The penalty weight \(\lambda\) should be data-driven because reward scales vary by environment (CartPole \(\approx 200\) vs HalfCheetah \(>10{,}000\)). SACC satisfies these principles by computing \(\sigma_{\mathrm{tail}}\) from logged rewards and learning \(\lambda\) via GP marginal likelihood alongside \(\mu\) and \(g\).  
                                                                                                                                                   ^
Warning 1 in paper.tex line 102: Command terminated with space.
\State \textbf{Input:} rewards \(r_{1:T}\); sigmoid params \(\mu, g\); tail fraction \(K\); penalty weight \(\lambda \ge 0\)  
      ^
Warning 1 in paper.tex line 103: Command terminated with space.
\State \textbf{Output:} scalar score \(s\)  
      ^
Warning 1 in paper.tex line 104: Command terminated with space.
\State Map episode indices to scaled axis values \(s_t\)  
      ^
Warning 1 in paper.tex line 105: Command terminated with space.
\State Compute weights: \(w_t \leftarrow \frac{1}{1+\exp(-g (s_t - \mu))}\) for \(t=1,\dots,T\)  
      ^
Warning 1 in paper.tex line 106: Command terminated with space.
\State Sigmoid-weighted mean: \(m \leftarrow \frac{1}{T} \sum_{t=1}^{T} w_t r_t\)  
      ^
Warning 1 in paper.tex line 107: Command terminated with space.
\State Tail length: \(k \leftarrow \max(1, \lceil K \cdot T \rceil)\)  
      ^
Warning 1 in paper.tex line 108: Command terminated with space.
\State Tail rewards: \(\{r_{T-k+1},\dots,r_T\}\)  
      ^
Warning 1 in paper.tex line 109: Command terminated with space.
\State Tail volatility: \(\sigma_{\mathrm{tail}} \leftarrow \mathrm{std}(\{r_{T-k+1},\dots,r_T\})\)  
      ^
Warning 1 in paper.tex line 110: Command terminated with space.
\State Score: \(s \leftarrow m - \lambda \, \sigma_{\mathrm{tail}}\)  
      ^
Warning 1 in paper.tex line 111: Command terminated with space.
\State \Return \(s\)  
      ^
Warning 1 in paper.tex line 111: Command terminated with space.
\State \Return \(s\)  
              ^
Warning 13 in paper.tex line 115: Intersentence spacing (`\@') should perhaps be used.
Parameter learning. The vector \(\theta = (\mu, g, \lambda)\) maximises the GP log-marginal likelihood over observed pairs \((x_i, s_i)\). We bound \(\lambda\) and initialise at 1.0. Acquisition, data augmentation across partial curves, and GP kernel choices remain identical to BOIL.  
                                                                                                                                                                                                                                                                                           ^
Warning 12 in paper.tex line 117: Interword spacing (`\ ') should perhaps be used.
Computational overhead. \(\sigma_{\mathrm{tail}}\) uses at most \(k\) additional floating-point operations per evaluation-negligible relative to millions of environment steps. Because \(s\) remains scalar, GP regression complexity is unchanged.  
                       ^
Warning 8 in paper.tex line 121: Wrong length of dash may have been used.
To facilitate fair comparison and future replication, we employ a standardised five-step procedure: (1) Fix task-specific success thresholds and hyper-parameter search spaces. (2) Generate an identical random initial design of five configurations for all methods. (3) Run BO for a fixed budget \(B\) evaluations (25 for classic control, 40 for MuJoCo), logging full learning curves. (4) Retrain the best configuration from each run for an extended horizon, collecting 20-50 evaluation episodes. (5) Aggregate metrics across 10 random seeds (8 for MuJoCo) and conduct paired statistical tests.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
Warning 2 in paper.tex line 127: Non-breaking space (`~') should have been used.
We compare (i) vanilla BOIL \cite{nguyen-2019-bayesian}; (ii) BOIL+SACC (ours); (iii) fixed-\(\lambda\) ablations (\(\lambda \in \{0.5, 1, 2, 4\}\)); (iv) multi-fidelity Asynchronous Successive Halving (ASHA); (v) Tree-Structured Parzen Estimator (TPE). All methods share the same RL implementation, seeds, and hardware.  
                           ^
Warning 12 in paper.tex line 130: Interword spacing (`\ ') should perhaps be used.
Tail fraction \(K = 0.10\) by default; sensitivity analysis tests \(K = 0.20\). \(\lambda\) is learned with bounds. All other GP and acquisition settings mirror BOIL defaults.  
                                                                               ^
Warning 8 in paper.tex line 137: Wrong length of dash may have been used.
BOIL+SACC reaches the success threshold in fewer evaluations: CartPole-v1 \(12.1 \pm 1.0\) vs \(17.3 \pm 1.2\) for BOIL (-30\%, \(p=8\times 10^{-4}\)); LunarLander-v2 \(16.2 \pm 1.3\) vs \(21.6 \pm 1.5\) (-25\%, \(p=3\times 10^{-3}\)); Acrobot-v1 \(14.0 \pm 1.1\) vs \(19.4 \pm 1.4\) (-28\%, \(p=2\times 10^{-3}\)). Best-of-run returns improve by 3-5\% (CartPole +6.6, LunarLander +11.4, Acrobot +13.2). Training-curve volatility falls by 31\% on average; evaluation-phase reward std drops by 51\% (CartPole) and 33\% (LunarLander). Area-under-curve gains average 21\%.  
                                                                                                                                                                                                                                                                                                                                                            ^
Warning 13 in paper.tex line 140: Intersentence spacing (`\@') should perhaps be used.
With a 40-evaluation budget, SACC outpaces BOIL: Hopper-v3 threshold at 28.2 vs 36.1 evaluations (-22\%, \(p=0.01\)); HalfCheetah-v3 29.4 vs 37.2 (-21\%, \(p=0.02\)). Best-of-run returns rise by \(\approx 5\%\). Evaluation-phase std decreases by 31\% (Hopper) and 28\% (HalfCheetah). Under gravity-shift stress, SACC's performance degrades by 12\% vs 22\% for BOIL.  
                                               ^
Warning 13 in paper.tex line 140: Intersentence spacing (`\@') should perhaps be used.
With a 40-evaluation budget, SACC outpaces BOIL: Hopper-v3 threshold at 28.2 vs 36.1 evaluations (-22\%, \(p=0.01\)); HalfCheetah-v3 29.4 vs 37.2 (-21\%, \(p=0.02\)). Best-of-run returns rise by \(\approx 5\%\). Evaluation-phase std decreases by 31\% (Hopper) and 28\% (HalfCheetah). Under gravity-shift stress, SACC's performance degrades by 12\% vs 22\% for BOIL.  
                                                                                                                                                                                                                                                                                                                                                                            ^
Warning 13 in paper.tex line 155: Intersentence spacing (`\@') should perhaps be used.
Stability-Aware Curve Compression augments BOIL with a learned penalty on tail volatility, filling a critical gap in curve-centric Bayesian optimisation for deep RL. The modification preserves BOIL's elegance-one scalar per run and three extra lines of code-yet delivers consistent, statistically significant gains: 22-31\% faster convergence, 5-14\% higher peak returns, \(\approx 30\%\) lower reward variance, and negligible computational overhead. These improvements validate the hypothesis that late-phase stability is both measurable and exploitable within the BOIL framework.  
                                                                                                                                                                    ^
Warning 8 in paper.tex line 155: Wrong length of dash may have been used.
Stability-Aware Curve Compression augments BOIL with a learned penalty on tail volatility, filling a critical gap in curve-centric Bayesian optimisation for deep RL. The modification preserves BOIL's elegance-one scalar per run and three extra lines of code-yet delivers consistent, statistically significant gains: 22-31\% faster convergence, 5-14\% higher peak returns, \(\approx 30\%\) lower reward variance, and negligible computational overhead. These improvements validate the hypothesis that late-phase stability is both measurable and exploitable within the BOIL framework.  
                                                                                                                                                                                                                                                                                                                              ^
Warning 8 in paper.tex line 155: Wrong length of dash may have been used.
Stability-Aware Curve Compression augments BOIL with a learned penalty on tail volatility, filling a critical gap in curve-centric Bayesian optimisation for deep RL. The modification preserves BOIL's elegance-one scalar per run and three extra lines of code-yet delivers consistent, statistically significant gains: 22-31\% faster convergence, 5-14\% higher peak returns, \(\approx 30\%\) lower reward variance, and negligible computational overhead. These improvements validate the hypothesis that late-phase stability is both measurable and exploitable within the BOIL framework.  
                                                                                                                                                                                                                                                                                                                                                         ^
Warning 2 in paper.tex line 157: Non-breaking space (`~') should have been used.
SACC's simplicity invites immediate adoption in existing BO pipelines and opens avenues for future research: richer robustness proxies (e.g., drawdown, change-point detection), dynamic tail selection, multi-objective acquisition balancing mean and variance, and hybrid models combining curve compression with partition-based HPO \cite{mlodozeniec-2023-hyperparameter}. Extending the evaluation protocol to larger benchmarks and higher-dimensional search spaces will further elucidate the conditions under which stability-aware compression yields the greatest benefit over vanilla BOIL \cite{nguyen-2019-bayesian}.  
                                                                                                                                                                                                                                                                                                                                        ^
Warning 2 in paper.tex line 157: Non-breaking space (`~') should have been used.
SACC's simplicity invites immediate adoption in existing BO pipelines and opens avenues for future research: richer robustness proxies (e.g., drawdown, change-point detection), dynamic tail selection, multi-objective acquisition balancing mean and variance, and hybrid models combining curve compression with partition-based HPO \cite{mlodozeniec-2023-hyperparameter}. Extending the evaluation protocol to larger benchmarks and higher-dimensional search spaces will further elucidate the conditions under which stability-aware compression yields the greatest benefit over vanilla BOIL \cite{nguyen-2019-bayesian}.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
