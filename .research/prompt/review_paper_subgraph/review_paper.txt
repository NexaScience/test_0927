
Input:
You are an expert reviewer for a top-tier international conference.
Please conduct a comprehensive review of the research paper provided, evaluating it according to the standards of venues like NeurIPS, ICML, ICLR, or AAAI.

Your task is to evaluate the paper on four key dimensions and provide scores from 1-10 for each:

## Evaluation Dimensions:

### 1. Novelty (1-10)
- How original and innovative is the proposed approach?
- Does it introduce new concepts, methods, or insights?
- Is there sufficient differentiation from existing work?

### 2. Significance (1-10)
- What is the potential impact of this work on the field?
- Does it address an important problem?
- Are the contributions meaningful and substantial?

### 3. Reproducibility (1-10)
- Are the experimental details sufficient for reproduction?
- Is the methodology clearly described?
- Are datasets, hyperparameters, and implementation details provided?

### 4. Experimental Quality (1-10)
- Are the experiments well-designed and comprehensive?
- Are appropriate baselines and evaluation metrics used?
- Is statistical significance properly assessed?
- Are the results convincing and well-analyzed?

## Section-by-Section Analysis:

For each section of the paper, provide:
- Key strengths
- Areas for improvement
- Specific comments on quality and completeness

## Overall Assessment:

Provide your scores for each dimension, followed by an overall recommendation.

## Paper Content:


**Title:** Stability-Aware Curve Compression for Bayesian Optimisation of Deep Reinforcement-Learning Hyper-parameters


**Abstract:** Bayesian Optimisation for Iterative Learning (BOIL) compresses an entire learning curve into a single scalar through a sigmoid-weighted average that a Gaussian Process (GP) can model. While this summary accelerates hyper-parameter search, it ignores late-stage oscillations that are commonplace in deep reinforcement learning (RL). Consequently, BOIL may repeatedly invest evaluations in hyper-parameters that spike to high returns yet produce brittle policies. We propose Stability-Aware Curve Compression (SACC), a drop-in replacement for BOIL’s scoring function that subtracts a stability penalty from the original score: s = m(curve) – λ · std(tail), where m(curve) is the sigmoid-weighted mean, std(tail) is the standard deviation of the last K % of episodes and λ ≥ 0 is a learnable coefficient. The amendment preserves BOIL’s one-dimensional interface, adds three lines of code, and introduces a single additional parameter that is learned jointly with BOIL’s logistic midpoint and growth by maximising GP log-marginal likelihood. On classic control and MuJoCo benchmarks SACC, evaluated over 10 random seeds, reduces the number of BO evaluations needed to reach task success by 22–31 %, raises best-of-run returns by 5–14 %, lowers evaluation-phase reward variance by roughly 30 %, and increases wall-clock cost by less than 2 %. These results show that penalising tail volatility guides Bayesian optimisation toward robust hyper-parameters without sacrificing sample efficiency.


**Introduction:** Hyper-parameter optimisation (HPO) remains a principal bottleneck in deep reinforcement learning because each evaluation entails thousands of expensive, high-variance environment interactions. Bayesian optimisation (BO) is attractive in this regime, but most BO variants treat performance as a terminal scalar, wasting information available in the trajectory of rewards accrued during training. Bayesian Optimisation for Iterative Learning (BOIL) alleviates this inefficiency by compressing partial learning curves into a scalar via a sigmoid-weighted average, allowing the GP surrogate and acquisition function to exploit intermediate progress [nguyen-2019-bayesian]. Unfortunately, a sole mean-like statistic hides a critical facet of solution quality: stability. Learning curves that climb to high rewards but oscillate heavily toward the end of training are unreliable at test time, yet BOIL, blind to volatility, may continue to query such regions of hyper-parameter space.

We address this reliability gap with Stability-Aware Curve Compression (SACC), a minimal modification of BOIL that rewards both progress and steadiness. After computing BOIL’s sigmoid-weighted mean m(curve), SACC subtracts a penalty proportional to the standard deviation of the last K % of episodes, producing a new score s = m – λ · σ_tail. The penalty strength λ is appended to BOIL’s compression parameters and learned through GP marginal-likelihood maximisation, so no hand-tuning is required. Crucially, the score remains one-dimensional, leaving BOIL’s surrogate, data augmentation, and acquisition optimisation intact.

Why is designing such a penalty hard? (i) Inflating the surrogate’s output dimensionality would forfeit BOIL’s computational advantage. (ii) Stability must be assessed cheaply because environment steps dominate cost. (iii) The penalty must adapt across tasks with disparate reward scales and noise characteristics. SACC satisfies these constraints by reusing BOIL’s interface, computing one additional standard deviation, and letting λ adjust automatically.

We empirically evaluate SACC on classic control tasks (CartPole-v1, LunarLander-v2, Acrobot-v1) and stochastic MuJoCo tasks (Hopper-v3, HalfCheetah-v3) under a unified protocol that measures five axes: sample efficiency, performance ceiling, stability, computational overhead, and generalisation. Baselines include vanilla BOIL [nguyen-2019-bayesian], fixed-λ ablations, and external HPO approaches such as multi-fidelity bandits and tree-structured Parzen estimators. Partition-based hyper-parameter optimisation methods that bypass BO surrogates [mlodozeniec-2023-hyperparameter] are also discussed for contrast but are not directly comparable because they neither exploit full curves nor target volatility.

Contributions
• We uncover a reliability blind spot in BOIL and introduce SACC, a three-line drop-in fix that maintains BOIL’s one-dimensional surrogate.
• We integrate λ as a learnable compression parameter, enabling task-adaptive stability control without manual tuning.
• We present a rigorous, reusable evaluation protocol focusing on efficiency, robustness, and cost.
• Across six benchmarks and multiple noise regimes, we demonstrate 22–31 % faster convergence, 5–14 % higher best returns, ≈30 % lower policy variance, and <2 % runtime overhead.

Future work can extend SACC to richer one-dimensional robustness proxies, dynamic tail fractions, and hybrid schemes that blend curve compression with partition-based objectives.


**Related Work:** Bayesian optimisation for hyper-parameter tuning traditionally relies on endpoint performance only. BOIL broke with this tradition by using a learnable sigmoid to weight intermediate rewards, markedly improving sample efficiency in neural network and RL settings [nguyen-2019-bayesian]. Our work adheres to BOIL’s curve-centric philosophy but argues that a mean-style statistic is insufficient when late-stage volatility jeopardises policy reliability. By attaching an adaptive variance penalty, SACC retains BOIL’s machinery while explicitly discouraging oscillatory trajectories.

Hyperparameter Optimisation through Neural Network Partitioning (HPO-NP) introduces a fundamentally different idea: optimise hyper-parameters via marginal-likelihood-inspired losses computed on subnetworks trained on data shards, eliminating the need for separate validation sets [mlodozeniec-2023-hyperparameter]. While effective for supervised learning, HPO-NP neither models the entire learning curve nor targets stability, and its reliance on differentiable objectives limits direct applicability to RL with sparse, delayed rewards.

Alternative BO extensions include multi-fidelity methods that terminate unpromising runs early, density-estimation techniques such as TPE, and population-based bandits. These algorithms do not encode volatility awareness; any stability benefit is incidental. Empirically, our experiments show that such baselines trail BOIL+SACC in both sample efficiency and reward variance, highlighting the value of explicit stability awareness.

Compared to prior work, SACC is unique in providing (i) a negligible-cost stability proxy that (ii) preserves the scalar surrogate interface and (iii) adapts automatically through GP marginal-likelihood learning, thereby offering a pragmatic and theoretically consistent refinement of curve-aware BO.


**Background:** Problem setting. Let x ∈ X denote a hyper-parameter vector; training an agent under x for T episodes yields a reward sequence r₁:T. We seek to minimise the number of costly evaluations of f(x) while discovering x values whose induced policies achieve high, stable returns. BOIL defines f(x) as a sigmoid-weighted mean m(x)=1/T Σ_t w_t r_t, where weights w_t depend on learnable midpoint μ and growth g parameters of a logistic. A Gaussian Process prior over f and an acquisition function then drive sequential search [nguyen-2019-bayesian].

Limitation of BOIL. Because m(x) is essentially an average, it conflates smooth and erratic curves that share similar central tendencies. In deep RL, however, volatility often signals over-fitting to transient dynamics or premature value-function divergence—issues that manifest as poor generalisation or catastrophic drops once exploration noise is removed.

Stability proxy. We posit that the standard deviation of the tail—defined as the last ⌈K·T⌉ episodes—is an inexpensive yet informative measure of policy reliability. Using only the tail focuses on the period closest to deployment, ignoring early-phase exploration noise.

Design principles. (i) One-dimensional compression keeps BOIL’s computational benefits. (ii) Penalty computation must not require gradient access to the RL algorithm. (iii) The penalty weight λ should be data-driven because reward scales vary by environment (CartPole ≈200 vs HalfCheetah >10,000). SACC satisfies these principles by computing σ_tail from logged rewards and learning λ via GP marginal likelihood alongside μ and g.


**Method:** Given a reward trajectory r₁:T, BOIL first maps episode indices to a scaled axis and computes weights w_t = 1/(1+exp(−g (s_t − μ))). The original score is m = (1/T) Σ_t w_t r_t. Stability-Aware Curve Compression augments this by
1. Selecting the tail: k = max(1, ⌈K·T⌉). Tail rewards are r_{T−k+1:T}.
2. Computing volatility: σ_tail = std(r_{T−k+1:T}).
3. Producing the score: s = m − λ σ_tail, with λ ≥ 0.

Algorithmic integration. We simply replace BOIL’s apply_one_transform_logistic with a three-line variant:
   m = original_sigmoid_mean(curve)
   σ = np.std(curve[-k:])
   return m − λ·σ

Parameter learning. The vector θ = (μ, g, λ) maximises the GP log-marginal likelihood over observed pairs (x_i, s_i). We bound λ to [0,5] and initialise at 1.0. Acquisition, data augmentation across partial curves, and GP kernel choices remain identical to BOIL.

Computational overhead. σ_tail uses at most k additional floating-point operations per evaluation—negligible relative to millions of environment steps. Because s remains scalar, GP regression complexity is unchanged.


**Experimental Setup:** Unified protocol. To facilitate fair comparison and future replication, we employ a standardised five-step procedure: (1) Fix task-specific success thresholds and hyper-parameter search spaces. (2) Generate an identical random initial design of five configurations for all methods. (3) Run BO for a fixed budget B evaluations (25 for classic control, 40 for MuJoCo), logging full learning curves. (4) Retrain the best configuration from each run for an extended horizon, collecting 20–50 evaluation episodes. (5) Aggregate metrics across 10 random seeds (8 for MuJoCo) and conduct paired statistical tests.

Tasks and search spaces. Classic control (CartPole-v1, LunarLander-v2, Acrobot-v1) tune two DQN hyper-parameters: learning rate and target-network update period. MuJoCo tasks (Hopper-v3, HalfCheetah-v3) extend the space to up to seven parameters, adding optimiser momentum, exploration ε, and discount γ.

Methods. We compare (i) vanilla BOIL [nguyen-2019-bayesian]; (ii) BOIL+SACC (ours); (iii) fixed-λ ablations (λ∈{0.5,1,2,4}); (iv) multi-fidelity Asynchronous Successive Halving (ASHA); (v) Tree-Structured Parzen Estimator (TPE). All methods share the same RL implementation, seeds, and hardware.

Hyper-parameters for SACC. Tail fraction K = 0.10 by default; sensitivity analysis tests K = 0.20. λ is learned with bounds [0,5]. All other GP and acquisition settings mirror BOIL defaults.

Metrics. Primary: (1) evaluations-to-threshold; (2) best validation reward after B evaluations. Secondary: (3) area under the best-return curve; (4) σ_tail; (5) evaluation-phase reward mean ± std; (6) wall-clock and memory usage. Significance is assessed with paired t-tests or Wilcoxon tests at p < 0.05.


**Results:** Main study (classic control). BOIL+SACC reaches the success threshold in fewer evaluations: CartPole-v1 12.1 ± 1.0 vs 17.3 ± 1.2 for BOIL (−30 %, p=8×10⁻⁴); LunarLander-v2 16.2 ± 1.3 vs 21.6 ± 1.5 (−25 %, p=3×10⁻³); Acrobot-v1 14.0 ± 1.1 vs 19.4 ± 1.4 (−28 %, p=2×10⁻³). Best-of-run returns improve by 3–5 % (CartPole +6.6, LunarLander +11.4, Acrobot +13.2). Training-curve volatility falls by 31 % on average; evaluation-phase reward std drops by 51 % (CartPole) and 33 % (LunarLander). Area-under-curve gains average 21 %.

Robustness study (MuJoCo, high variance). With a 40-evaluation budget, SACC outpaces BOIL: Hopper-v3 threshold at 28.2 vs 36.1 evaluations (−22 %, p=0.01); HalfCheetah-v3 29.4 vs 37.2 (−21 %, p=0.02). Best-of-run returns rise by ≈5 %. Evaluation-phase std decreases by 31 % (Hopper) and 28 % (HalfCheetah). Under gravity-shift stress, SACC’s performance degrades by 12 % vs 22 % for BOIL.

Ablations. Fixed-λ variants outperform vanilla BOIL but underperform learned-λ SACC on all primary metrics, confirming the benefit of task-adaptive λ. Increasing K to 0.20 yields similar efficiency (±2 %) and a further 4 % reduction in evaluation std.

External baselines. ASHA lags SACC by 38 % in evaluations-to-threshold on classic control and 24 % in area-under-curve on MuJoCo. TPE exhibits the highest evaluation-phase variance (+44 % vs SACC).

Cost analysis. Profiling shows 1.3 % ± 0.4 % increase in wall-clock time per evaluation, no change in peak VRAM, and identical FLOPs.

Threats to validity. Some MuJoCo settings use eight seeds due to cost; extreme tail fractions (>0.3) remain unexplored; all experiments use a single GPU type, leaving CPU-only scenarios untested.


**Conclusion:** Stability-Aware Curve Compression augments BOIL with a learned penalty on tail volatility, filling a critical gap in curve-centric Bayesian optimisation for deep RL. The modification preserves BOIL’s elegance—one scalar per run and three extra lines of code—yet delivers consistent, statistically significant gains: 22–31 % faster convergence, 5–14 % higher peak returns, ≈30 % lower reward variance, and negligible computational overhead. These improvements validate the hypothesis that late-phase stability is both measurable and exploitable within the BOIL framework.

SACC’s simplicity invites immediate adoption in existing BO pipelines and opens avenues for future research: richer robustness proxies (e.g., drawdown, change-point detection), dynamic tail selection, multi-objective acquisition balancing mean and variance, and hybrid models combining curve compression with partition-based HPO [mlodozeniec-2023-hyperparameter]. Extending the evaluation protocol to larger benchmarks and higher-dimensional search spaces will further elucidate the conditions under which stability-aware compression yields the greatest benefit over vanilla BOIL [nguyen-2019-bayesian].


Output:
{
    "novelty_score": 6,
    "significance_score": 7,
    "reproducibility_score": 7,
    "experimental_quality_score": 7
}
