
Input:
You are a cutting-edge AI researcher. Based on the new research method described in # New Methods, please design an overall experimental strategy that will be applied across all experiments to demonstrate the effectiveness of this method.

# Instructions
- Define a comprehensive experimental strategy that will guide multiple experiments.
- This strategy should be common to all experiments that will be conducted.
- The strategy should address:
    - What aspects of the proposed method need to be validated (e.g., performance improvement, efficiency, robustness, generalization)
    - What types of comparisons are necessary (e.g., baselines, ablations, state-of-the-art methods)
    - What experimental angles will be used to validate the claims (e.g., quantitative performance, qualitative analysis, computational cost)
    - How to demonstrate the method's effectiveness from multiple perspectives
    - What validation criteria will determine success
- The strategy should be realistic and take into account the experimental environment.
- Focus on the overall approach rather than specific experiment details (which will be defined in subsequent steps).

## Output Format
Please provide:
- experiment_strategy: A comprehensive strategy statement that describes the overall approach for validating the proposed method across all experiments

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "ρBYOL and most contrastive/self-distillation methods pick two random temporal crops from the same clip and force their representations to match equally, no matter whether the crops are 1 frame or 3 s apart. On rapidly changing rodent behaviour this causes:\n1) Positive pairs that are too far apart in time to depict the same action, blurring fine-grained motion cues.\n2) Weak ordering of the latent space with respect to temporal distance, which hurts downstream behaviour segmentation and rare-action recognition.\nA fix that does not require new architectures is to make the objective aware of the temporal distance between the two views.",
    "Methods": "Time-Weighted BYOL (TW-BYOL)\nMinimal change: multiply BYOL’s mean-squared error loss between the online and target projections by an exponential weight that decays with frame distance Δt between the two augmented views.\nL_TW = w(Δt) * || q( f_θ( v_i ) ) – stop_grad( f_ϕ( v_j ) ) ||² ,  \nwith w(Δt)=exp(-Δt / τ) and a single hyper-parameter τ (e.g. 30 frames).\nTheoretical motivation: For small Δt the two views are likely to be the same action, so the model should align them strongly; for large Δt alignment is weakened, allowing the network to separate different actions and preserve temporal information. The weighting acts as a soft regulariser imposing temporal locality without introducing extra negatives or additional networks.",
    "Experimental Setup": "Dataset: MABe22 mice-triplet videos (video modality only).\nTraining: reproduce original ρBYOL recipe (SlowFast-8×8 backbone, same augmentations) and TW-BYOL (identical but with the weighted loss).\nHyper-parameter sweep: τ ∈ {15, 30, 60} frames.\nEvaluation: linear probe F1 on the 8 hidden downstream mice tasks; report mean and per-task scores.\nBaseline: official ρBYOL numbers from MABe22 leaderboard.\nCompute: 4×A100 GPUs, 50 epochs (same as baseline).",
    "Experimental Code": "# only loss change shown\nimport torch, math\n\ndef time_weighted_byol_loss(p_online, z_target, frame_dist, tau=30.):\n    \"\"\"p_online and z_target: (B,D) tensors; frame_dist: (B,) ints\"\"\"\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\n    loss = (p_online - z_target.detach()).pow(2).sum(dim=1)  # (B,)\n    weighted_loss = (weight * loss).mean()\n    return weighted_loss\n\n# inside training loop\np = projector_online(backbone_online(view_i))      # (B,D)\nz = projector_target(backbone_target(view_j))      # (B,D)\nframe_dist = torch.abs(frame_idx_i - frame_idx_j)  # (B,)\nloss = time_weighted_byol_loss(p, z, frame_dist, tau=30)",
    "Expected Result": "Across three random seeds the mean F1 on the mice tasks is expected to rise by ~2-3 points (e.g. ρBYOL 68.5 → TW-BYOL 71.0). Improvements should be largest on tasks requiring discrimination of short, bursty actions (e.g. grooming vs sniffing) while remaining neutral on slowly varying experimental condition classification.",
    "Expected Conclusion": "A single, two-line weighting term makes the self-distillation objective respect temporal locality, sharpening representations for fast rodent behaviours. Because no extra negatives, memory, or architectural changes are introduced, the method keeps BYOL’s simplicity and training speed while delivering measurable accuracy gains on fine-grained behaviour annotation tasks."
}

---


# Reference Information from Previous Iteration

Output:
{
    "experiment_strategy": "Global Goal\nProve that the proposed Time-Weighted BYOL (TW-BYOL) yields temporally better-ordered, more behaviour-discriminative and equally efficient video representations than existing self-supervised alternatives, while remaining robust to hyper-parameter choices and generalising across rodent datasets.\n\n1. Core Hypotheses to Validate\n   H1 – Performance: TW-BYOL improves downstream behaviour recognition (overall F1, rare-action recall, few-shot transfer).\n   H2 – Temporal Awareness: embeddings respect temporal proximity (distance in latent space grows with frame gap).\n   H3 – Efficiency: training speed, GPU memory and wall-clock cost stay within ±5 % of ρBYOL.\n   H4 – Robustness: gains hold under different τ values, random seeds, crop strategies and limited labelled data.\n   H5 – Generalisation: improvements transfer to unseen rodents/tasks and to a second behaviour dataset.\n\n2. Comparative Framework\n   a. Baseline: reproduced ρBYOL recipe.\n   b. State-of-the-art self-supervised video baselines: MoCo-v3, SimCLR-v2, TimeContrast.\n   c. Supervised upper bound: same backbone trained with full labels (for context only).\n   d. Ablations:\n      • No weighting (ρBYOL loss) – “Uniform”.\n      • Hard cut-off weighting – “Binary”.\n      • Alternative decays (linear, inverse square) to test the importance of exponential form.\n      • τ sweep (15, 30, 60 frames).\n\n3. Experimental Angles\n   3.1 Quantitative Performance\n       • Linear-probe F1 per task and averaged (primary metric).\n       • k-NN accuracy (label-free evaluation of representation quality).\n       • Rare-action recall (top-20 % least frequent labels).\n   3.2 Temporal Locality Analysis\n       • Spearman correlation ρ between embedding distance and frame gap Δt.\n       • Temporal retrieval: mean reciprocal rank when querying a frame for its 5 nearest temporal neighbours.\n   3.3 Efficiency Metrics\n       • GPU hours per pre-training run.\n       • Samples / sec and peak VRAM.\n   3.4 Qualitative\n       • t-SNE / UMAP plots coloured by action and by timestamp.\n       • Video retrieval demos.\n   3.5 Robustness & Generalisation\n       • Sensitivity curves over τ and crop policies.\n       • Subset-of-data training (25 %, 50 % of unlabelled video) to test data efficiency.\n       • Cross-dataset transfer: pre-train on MABe22, evaluate on a second rodent-behaviour set (e.g., RatSI).\n\n4. Validation Criteria for Success\n   Pass if ALL are met:\n   • +2 F1 absolute (≥ p<0.05, paired t-test over 3 seeds) versus ρBYOL on mean of 8 tasks.\n   • At least 6/8 tasks individually improve or remain equal.\n   • Embedding–time correlation improves by ≥10 % over ρBYOL.\n   • GPU hours increase ≤5 %.\n   • Variance of F1 across seeds not higher than ρBYOL.\n   • Improvements persist (≥75 % retained) when τ∈[15,60] or when only 50 % of unlabelled video is available.\n\n5. Experimental Protocol\n   • Hardware: up to 4×A100 80 GB per run; mixed-precision training; identical data-loading pipeline for all methods.\n   • Controlled compute: fix batch size, epochs (50), optimiser and augmentation suite; record seeds.\n   • Run each configuration 3× for statistics.\n   • Hyper-parameter grid executed with identical wall-clock budget; schedule runs via SLURM to exploit 2 TB RAM node.\n   • Evaluation code placed in a separate repo; blind-test labels kept hidden until final metrics are logged to ensure fairness.\n\n6. Multi-Perspective Demonstration Strategy\n   a. Start with baseline vs TW-BYOL to establish headline gains.\n   b. Add ablation study to attribute gains specifically to exponential weighting and to choice of τ.\n   c. Compare against external SOTA to position method in field.\n   d. Present temporal locality analyses to back mechanistic claim.\n   e. Provide efficiency table to show “no free lunch” avoided.\n   f. Show robustness curves and cross-dataset transfer to argue for broad applicability.\n\nThis unified strategy will be executed for every subsequent experiment, ensuring that each study supplies comparable evidence along performance, temporal fidelity, efficiency, robustness and generalisation axes while sharing compute budgets and evaluation protocols across the research programme."
}
