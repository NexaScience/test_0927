
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 2 distinct Experiment objects.

# Instructions
- Generate 2 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 2 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "ρBYOL and most contrastive/self-distillation methods pick two random temporal crops from the same clip and force their representations to match equally, no matter whether the crops are 1 frame or 3 s apart. On rapidly changing rodent behaviour this causes:\n1) Positive pairs that are too far apart in time to depict the same action, blurring fine-grained motion cues.\n2) Weak ordering of the latent space with respect to temporal distance, which hurts downstream behaviour segmentation and rare-action recognition.\nA fix that does not require new architectures is to make the objective aware of the temporal distance between the two views.",
    "Methods": "Time-Weighted BYOL (TW-BYOL)\nMinimal change: multiply BYOL’s mean-squared error loss between the online and target projections by an exponential weight that decays with frame distance Δt between the two augmented views.\nL_TW = w(Δt) * || q( f_θ( v_i ) ) – stop_grad( f_ϕ( v_j ) ) ||² ,  \nwith w(Δt)=exp(-Δt / τ) and a single hyper-parameter τ (e.g. 30 frames).\nTheoretical motivation: For small Δt the two views are likely to be the same action, so the model should align them strongly; for large Δt alignment is weakened, allowing the network to separate different actions and preserve temporal information. The weighting acts as a soft regulariser imposing temporal locality without introducing extra negatives or additional networks.",
    "Experimental Setup": "Dataset: MABe22 mice-triplet videos (video modality only).\nTraining: reproduce original ρBYOL recipe (SlowFast-8×8 backbone, same augmentations) and TW-BYOL (identical but with the weighted loss).\nHyper-parameter sweep: τ ∈ {15, 30, 60} frames.\nEvaluation: linear probe F1 on the 8 hidden downstream mice tasks; report mean and per-task scores.\nBaseline: official ρBYOL numbers from MABe22 leaderboard.\nCompute: 4×A100 GPUs, 50 epochs (same as baseline).",
    "Experimental Code": "# only loss change shown\nimport torch, math\n\ndef time_weighted_byol_loss(p_online, z_target, frame_dist, tau=30.):\n    \"\"\"p_online and z_target: (B,D) tensors; frame_dist: (B,) ints\"\"\"\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\n    loss = (p_online - z_target.detach()).pow(2).sum(dim=1)  # (B,)\n    weighted_loss = (weight * loss).mean()\n    return weighted_loss\n\n# inside training loop\np = projector_online(backbone_online(view_i))      # (B,D)\nz = projector_target(backbone_target(view_j))      # (B,D)\nframe_dist = torch.abs(frame_idx_i - frame_idx_j)  # (B,)\nloss = time_weighted_byol_loss(p, z, frame_dist, tau=30)",
    "Expected Result": "Across three random seeds the mean F1 on the mice tasks is expected to rise by ~2-3 points (e.g. ρBYOL 68.5 → TW-BYOL 71.0). Improvements should be largest on tasks requiring discrimination of short, bursty actions (e.g. grooming vs sniffing) while remaining neutral on slowly varying experimental condition classification.",
    "Expected Conclusion": "A single, two-line weighting term makes the self-distillation objective respect temporal locality, sharpening representations for fast rodent behaviours. Because no extra negatives, memory, or architectural changes are introduced, the method keeps BYOL’s simplicity and training speed while delivering measurable accuracy gains on fine-grained behaviour annotation tasks."
}

# Experiment Strategy
Global Goal
Prove that the proposed Time-Weighted BYOL (TW-BYOL) yields temporally better-ordered, more behaviour-discriminative and equally efficient video representations than existing self-supervised alternatives, while remaining robust to hyper-parameter choices and generalising across rodent datasets.

1. Core Hypotheses to Validate
   H1 – Performance: TW-BYOL improves downstream behaviour recognition (overall F1, rare-action recall, few-shot transfer).
   H2 – Temporal Awareness: embeddings respect temporal proximity (distance in latent space grows with frame gap).
   H3 – Efficiency: training speed, GPU memory and wall-clock cost stay within ±5 % of ρBYOL.
   H4 – Robustness: gains hold under different τ values, random seeds, crop strategies and limited labelled data.
   H5 – Generalisation: improvements transfer to unseen rodents/tasks and to a second behaviour dataset.

2. Comparative Framework
   a. Baseline: reproduced ρBYOL recipe.
   b. State-of-the-art self-supervised video baselines: MoCo-v3, SimCLR-v2, TimeContrast.
   c. Supervised upper bound: same backbone trained with full labels (for context only).
   d. Ablations:
      • No weighting (ρBYOL loss) – “Uniform”.
      • Hard cut-off weighting – “Binary”.
      • Alternative decays (linear, inverse square) to test the importance of exponential form.
      • τ sweep (15, 30, 60 frames).

3. Experimental Angles
   3.1 Quantitative Performance
       • Linear-probe F1 per task and averaged (primary metric).
       • k-NN accuracy (label-free evaluation of representation quality).
       • Rare-action recall (top-20 % least frequent labels).
   3.2 Temporal Locality Analysis
       • Spearman correlation ρ between embedding distance and frame gap Δt.
       • Temporal retrieval: mean reciprocal rank when querying a frame for its 5 nearest temporal neighbours.
   3.3 Efficiency Metrics
       • GPU hours per pre-training run.
       • Samples / sec and peak VRAM.
   3.4 Qualitative
       • t-SNE / UMAP plots coloured by action and by timestamp.
       • Video retrieval demos.
   3.5 Robustness & Generalisation
       • Sensitivity curves over τ and crop policies.
       • Subset-of-data training (25 %, 50 % of unlabelled video) to test data efficiency.
       • Cross-dataset transfer: pre-train on MABe22, evaluate on a second rodent-behaviour set (e.g., RatSI).

4. Validation Criteria for Success
   Pass if ALL are met:
   • +2 F1 absolute (≥ p<0.05, paired t-test over 3 seeds) versus ρBYOL on mean of 8 tasks.
   • At least 6/8 tasks individually improve or remain equal.
   • Embedding–time correlation improves by ≥10 % over ρBYOL.
   • GPU hours increase ≤5 %.
   • Variance of F1 across seeds not higher than ρBYOL.
   • Improvements persist (≥75 % retained) when τ∈[15,60] or when only 50 % of unlabelled video is available.

5. Experimental Protocol
   • Hardware: up to 4×A100 80 GB per run; mixed-precision training; identical data-loading pipeline for all methods.
   • Controlled compute: fix batch size, epochs (50), optimiser and augmentation suite; record seeds.
   • Run each configuration 3× for statistics.
   • Hyper-parameter grid executed with identical wall-clock budget; schedule runs via SLURM to exploit 2 TB RAM node.
   • Evaluation code placed in a separate repo; blind-test labels kept hidden until final metrics are logged to ensure fairness.

6. Multi-Perspective Demonstration Strategy
   a. Start with baseline vs TW-BYOL to establish headline gains.
   b. Add ablation study to attribute gains specifically to exponential weighting and to choice of τ.
   c. Compare against external SOTA to position method in field.
   d. Present temporal locality analyses to back mechanistic claim.
   e. Provide efficiency table to show “no free lunch” avoided.
   f. Show robustness curves and cross-dataset transfer to argue for broad applicability.

This unified strategy will be executed for every subsequent experiment, ensuring that each study supplies comparable evidence along performance, temporal fidelity, efficiency, robustness and generalisation axes while sharing compute budgets and evaluation protocols across the research programme.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-main-perf-ablation",
            "run_variations": [
                "rhoBYOL-uniform",
                "TW-BYOL-tau15",
                "TW-BYOL-tau30",
                "TW-BYOL-tau60",
                "TW-BYOL-binary-cutoff"
            ],
            "description": "Objective / Hypothesis: Quantitatively demonstrate that the proposed exponential time–weighted loss (TW-BYOL) yields higher behaviour-recognition performance and stronger temporal ordering than the original ρBYOL, and that these gains are attributable to the weighting scheme rather than chance. The experiment also positions TW-BYOL against a strong non-BYOL baseline (MoCo-v3) while keeping backbone, data pipeline and compute identical.\n\nModels\n• SlowFast-8×8 backbone for all BYOL variants.  \n• ResNet-50 + MoCo-v3 (image→video adaptation via clip-level averaging) for an external SSL baseline.\n\nDatasets\n• MABe22 mice-triplet videos (RGB, 1024×512). 30-frame clips at 15 fps.  \nData split: official train/val/test (70 / 15 / 15 %). Cross-validation not used to keep GPU budget manageable.\n\nPre-processing\n• Random spatial crop (80–100 %), horizontal flip, colour jitter, grayscale, Gaussian blur.  \n• Two temporal crops: sample starting indices i,j; frame gap Δt recorded to compute weight.  \n• Clips are normalised with per-channel dataset mean/σ.\n\nTraining Protocol\n• 50 epochs, batch = 64 clips/GPU, AdamW (lr = 3e-4, cosine decay).  \n• 3 random seeds per variation (total 15 BYOL + 3 MoCo runs).  \n• Online/target momentum 0.996; projector MLP 2048→256.\n\nEvaluation\n1. Linear probe: train a single-layer logistic classifier on frozen features (20 epochs, lr = 0.1, SGD) – Primary metric: macro-F1.  \n2. k-NN (k = 20) accuracy – secondary.  \n3. Temporal locality: Spearman ρ between ‖z_t − z_{t+Δt}‖₂ and Δt on 10 K sampled pairs.  \n4. Efficiency: GPU hours, peak VRAM, throughput (clips/s), measured with PyTorch profiler.\n\nHyper-parameter Analysis\n• τ is swept implicitly via variations.  \n• For each seed, save F1 vs epoch to detect early/late overfitting.  \n• Learning-rate restart at 80 % of training to test stability (single pilot run).\n\nRobustness Checks\n• Inject 5 % salt-and-pepper noise into 10 % of frames during evaluation; record ΔF1.  \n• Distribution shift: evaluate on night-vision subset (4 % of test videos) unseen during training.\n\nSuccess Criteria (linked to H1–H3)\n• TW-BYOL-tau30 ≥ +2 F1 over ρBYOL (p < 0.05 paired t-test across seeds).  \n• ρ(embedding-distance,Δt) ≥ +10 % over ρBYOL.  \n• GPU hours within +5 % of baseline.\n\nExample Code Snippet (abbreviated)\n```python\nfor clips, frame_idxs in loader:      # clips: (B,3,T,H,W)\n    (v_i,v_j), Δt = temporal_augment(clips, frame_idxs)\n    p = online_proj(online_backbone(v_i))\n    z = target_proj(target_backbone(v_j))\n    loss = time_weighted_byol_loss(p, z, Δt, tau=args.tau)\n```\n\nOutputs stored per run in exp-main-perf-ablation/<variation>/<seed>/ with JSON logs for automated aggregation."
        },
        {
            "experiment_id": "exp-robustness-transfer-efficiency",
            "run_variations": [
                "rhoBYOL-full",
                "TW-BYOL-tau30-full",
                "TW-BYOL-tau30-half-unlabelled",
                "TW-BYOL-tau30-crop-jitter"
            ],
            "description": "Objective / Hypothesis: Validate TW-BYOL’s robustness to data quantity, augmentation policy and domain shift, and verify that training/inference efficiency remains on par with ρBYOL while generalising to a second rodent dataset (RatSI). Addresses H4–H5.\n\nModels\n• SlowFast-8×8 backbone for all runs to isolate effects of data/augmentations.  \n• Supervised upper bound (SlowFast-8×8 trained with labels) evaluated once for context, not part of variations.\n\nDatasets & Domain Transfer\n• Pre-train on MABe22 for all variations:\n  – full (100 % unlabelled) or half (50 %) subsets as per run_variations.\n• Cross-dataset evaluation: Frozen features tested on RatSI (rat social-interaction)  → linear probe F1.\n• Both datasets pre-processed identically: 32-frame, 224² spatial resolution, same augmentations.\n\nVariation Details\n1. rhoBYOL-full – baseline.\n2. TW-BYOL-tau30-full – default method.\n3. TW-BYOL-tau30-half-unlabelled – unsupervised data reduced to 50 % to test data-efficiency.\n4. TW-BYOL-tau30-crop-jitter – temporal crop strategy changed to uniform ±5 frames jitter (instead of random any-frame) to test robustness to view sampling.\n\nTraining & Repetition\n• As in exp-1 otherwise; 3 seeds each (total 12 runs).  \n• Early stopping on validation F1 with patience = 5 epochs to mimic real-world training budget cuts.\n\nEvaluation Metrics\nPrimary: macro-F1 on MABe22 tasks and on RatSI transfer.  \nSecondary: rare-action recall, temporal MRR for neighbour retrieval, and equality of efficiency (GPU h, VRAM, wall-clock).  \nCalibration: Brier score on linear probe outputs.\n\nRobustness / Stress Tests\n• Gaussian noise σ = 0.05 added to 20 % of test clips.  \n• Frame-drop OOD shift: randomly delete 30 % frames and re-interpolate; re-evaluate F1.\n• Adversarial PGD (ε = 2/255, 3 steps) on 128 random clips; report ΔF1.\n\nHyper-parameter Sensitivity\n• For TW-BYOL-crop-jitter run, τ swept inline (15, 30, 60) on reduced 10-epoch jobs to plot F1 vs τ curve (not counted in main variations; auxiliary grid re-uses checkpoints).\n\nEfficiency Accounting\n• FLOPs measured with fvcore; memory via torch.cuda.max_memory_allocated(); inference latency over 256-clip batches. Acceptable increase threshold: ≤1.05× of baseline.\n\nSuccess Criteria\n• TW-BYOL-tau30-full retains ≥75 % of F1 gain when only 50 % unlabelled data is available.  \n• Under crop-jitter, F1 drop ≤1 pt.  \n• Cross-dataset transfer: ≥+1.5 F1 vs ρBYOL.  \n• Efficiency deltas within ±5 %.\n\nExample Code Snippet (data subset logic)\n```python\nif args.data_fraction < 1.0:\n    train_indices = random.sample(full_indices, int(len(full_indices)*args.data_fraction))\n    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n```\n\nAll logs + profiler traces pushed to branch `exp-robustness-transfer-efficiency` for reproducibility."
        }
    ],
    "expected_models": [
        "SlowFast-8×8",
        "ResNet-50",
        "ViT-B/16"
    ],
    "expected_datasets": [
        "MABe22",
        "RatSI"
    ]
}
