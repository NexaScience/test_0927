
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 2 distinct Experiment objects.

# Instructions
- Generate 2 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 2 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "In BOIL the whole learning curve is compressed into a single scalar through a sigmoid‐weighted average of the rewards.  This scalar ignores instability of the curve.  In Deep RL it is common that a hyper-parameter setting reaches a high reward but with large oscillations – such settings are often unreliable at test time.  Because the GP sees no penalty for instability it may keep sampling such noisy settings, wasting evaluations and slowing convergence.",
    "Methods": "Stability–Aware Curve Compression (SACC).\nModification to BOIL: after computing the sigmoid–weighted average m(curve) we subtract a stability penalty proportional to the standard deviation of the last K% of the curve.\n    score = m(curve) – λ · std(curve[ -K: ])\nOnly three extra lines are needed inside apply_one_transform_logistic.  λ ≥ 0 is an additional scalar hyper-parameter that is learned together with the sigmoid midpoint and growth by maximising the GP log-marginal likelihood (just append λ to the vector that is optimised).\nMotivation:   std(curve_tail) is an inexpensive proxy for reliability; subtracting it keeps the objective one-dimensional so BOIL’s GP, acquisition function and data-augmentation remain untouched.",
    "Experimental Setup": "Task: tune learning rate and target-network update period of a DQN agent on CartPole-v0.\nBaselines: (1) Original BOIL, (2) BOIL+SACC (ours).\nBudget: 25 BO iterations, 5 random initial points.\nK: last 10 % of episodes, λ initialised to 1.0 with bounds [0,5].\nMetrics:\n  • Best validation reward after 25 evaluations.\n  • Number of evaluations required to reach an average reward ≥ 195.\n  • Post-training stability: std of reward over 20 extra evaluation episodes.\nSame random seeds are used for both methods.",
    "Experimental Code": "import numpy as np\n\ndef transform_logistic_sacc(curve, midpoint, growth, max_ep, lam=1.0, tail_frac=0.1):\n    # original sigmoid weighting\n    x_scaled = np.linspace(-6, 6, len(curve))\n    weights = 1/(1+np.exp(-growth*(x_scaled-midpoint)))\n    m = np.mean(curve*weights)\n    # stability penalty (std of last K% of curve)\n    k = max(1, int(len(curve)*tail_frac))\n    stability = np.std(curve[-k:])\n    return m - lam*stability\n\n# drop-in replacement inside BOIL\n# in apply_one_transform_logistic simply call transform_logistic_sacc with lam learned from GP\n",
    "Expected Result": "Across 10 independent runs the proposed BOIL+SACC is expected to:\n  • Reach the success threshold (avg reward 195) after ≈12 evaluations versus ≈17 for BOIL.\n  • Achieve ~5-10 % higher best-of-run reward.\n  • Produce policies whose evaluation-phase reward std is roughly 30 % lower than those from vanilla BOIL, showing better stability.",
    "Expected Conclusion": "Penalising late-phase reward variability with one extra term guides BOIL away from hyper-parameters that merely spike in performance, concentrating budget on genuinely robust settings.  The change touches only the curve-compression function, keeps BOIL’s remaining machinery intact, adds one learnable scalar, and empirically yields faster and more reliable hyper-parameter optimisation."
}

# Experiment Strategy
Overall Objective
Design a single, modular validation protocol that can be reused on every experiment (Cart-Pole → Atari → MuJoCo, small → large HP spaces, low → high reward noise) to prove that Stability-Aware Curve Compression (SACC) brings (1) faster convergence, (2) higher ultimate performance, (3) greater policy reliability, (4) negligible overhead, and (5) good generalization across tasks and search regimes.

I. Core Hypotheses To Validate
1. Sample-efficiency: BOIL+SACC requires fewer BO evaluations to reach a target performance.
2. Performance ceiling: BOIL+SACC attains a higher best-of-run return than baselines.
3. Stability / robustness:
   a. Training stability – learning curves show less oscillation.
   b. Evaluation stability – final policy reward std is lower.
4. Computational cost: wall-clock and GPU hours are not significantly higher than vanilla BOIL.
5. Generalization: the λ learned by GP adapts automatically to a variety of reward scales and noise levels without retuning.

II. Comparison Matrix (applied in every experiment)
A. Baselines
   • Vanilla BOIL (identical surrogate, no penalty)
   • BOIL with human-set λ (constant, no learning) – ablation
   • Alternative curve compressors (e.g., simple last-N averaging, BOIL-MAX) – sanity check
   • External state-of-the-art HPO: ASHA, TPE – competitive bar
B. Ablations / Sensitivity
   1. Vary tail fraction K and observe effect.
   2. Optimizer without λ in GP vector (λ fixed to 0) – isolates impact of learning λ.
C. Stress Settings
   • High-variance environment (stochastic CartPole, randomized seeds)
   • Large search space (add optimizer momentum, epsilon, etc.)

III. Evaluation Angles & Metrics (recorded for every run)
1. Quantitative
   a. Best validation reward vs #evaluations curve (primary) – Area-Under-Curve
   b. Time-to-threshold (first hit of task-specific success)
   c. Final policy test reward mean ± std over 30 episodes
   d. Std of last K% training rewards (same K for fairness)
   e. CPU/GPU time & memory footprint (profiling hooks)
2. Qualitative
   a. Plot learning curves of representative runs (median, 25/75 percentile shading)
   b. Acquisition trajectories – how λ evolves, sample dispersion
3. Statistical Validation
   • 10 independent seeds per setting
   • Report mean, 95% CI; use paired t-tests or Wilcoxon on matched seeds
   • Success criterion: BOIL+SACC beats every baseline on at least 3/4 primary metrics with p<0.05.

IV. Experimental Procedure (identical template)
Step 1: Fix task-specific success threshold & search space.
Step 2: Generate identical initial random design for all methods.
Step 3: Run BO for B iterations (budget fixed across methods) logging full learning curve at each eval.
Step 4: After BO terminates, retrain best hyper-params for T extra episodes, collect evaluation stats.
Step 5: Aggregate across seeds, compute metrics, statistical tests, produce plots & cost table.

V. Resource & Reproducibility Controls
• All runs limited to 1×A100; concurrency chosen so peak VRAM ≤80 GB and RAM ≤2 TB.
• Deterministic CuDNN + fixed numpy/PyTorch seeds stored.
• Codebase uses the same call-paths; SACC flag toggles extra 3-line penalty.
• Auto-logging: JSON + TensorBoard + csv for downstream analysis scripts.

VI. Success Criteria for the Whole Study
The method will be declared effective if, on a diverse benchmark suite (≥3 tasks, ≥2 noise regimes), it consistently:
1. Reduces evaluations-to-threshold by ≥20 % on average.
2. Improves best-of-run reward by ≥5 % on ≥70 % of tasks.
3. Cuts evaluation-phase reward std by ≥25 %.
4. Adds <2 % overhead in wall-clock time.
5. Shows no catastrophic regressions relative to any baseline.

This unified strategy ensures every forthcoming experiment follows a consistent, statistically sound, and multi-angle protocol, providing compelling evidence for SACC’s benefits while remaining practical within the available computational environment.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-1-main-performance",
            "run_variations": [
                "boil",
                "boil+sacc",
                "boil+sacc-fixedλ",
                "last10-average",
                "asha"
            ],
            "description": "Objective / Hypothesis: Quantitatively verify that the Stability-Aware Curve Compression (SACC) term accelerates hyper-parameter optimisation and improves final policy quality on classic discrete-action control tasks, while introducing negligible overhead.  We additionally include a fixed-λ ablation and an alternative curve compressor to isolate the effect of learning λ, and compare against a band-popular early-stopping HPO method (ASHA) to set a competitive bar.\n\nTasks (treated as datasets): CartPole-v1, LunarLander-v2, Acrobot-v1.\n\nModels / RL agents:\n• Deep Q-Network (DQN, 2-layer MLP 128-128, ReLU) — identical architecture for all variations.\n• Gaussian-Process surrogate with Matérn-5/2 kernel (GPyTorch) for all BO methods.\n\nSearch space: learning-rate∈[1e-5,1e-2] (log), target-network update τ∈[100,2000], exploration ε_final∈[0.01,0.2].  5 random initial points + 25 BO evaluations.\n\nPre-processing: rewards normalised to [0,1] per task for surrogate stability; observation features left untouched.\n\nData split: each evaluation = full 500-episode training run; last 10 % of episodes held out for stability penalty.  Post-optimisation we retrain the best HPs for 30k additional frames and test over 30 episodes.\n\nSeeds / repetitions: 10 independent seeds.  Report mean ±95 % CI; paired t-test vs vanilla BOIL.\n\nMetrics:\nPrimary –\n1) Evaluations-to-threshold (≥195 CartPole, ≥200 LunarLander, ≤−100 Acrobot),\n2) Best validation reward after 25 evaluations.\nSecondary –\n3) Std of reward in last 50 training episodes,\n4) Test-phase reward mean±std,\n5) Wall-clock time & GPU hours.\n\nHyper-parameter analysis: grid over λ∈{0,0.5,1,2,4} for “boil+sacc-fixedλ”.  Visualise λ learned by GP across iterations.\n\nRobustness checks: repeat CartPole with sticky-actions (0.25) noise; measure change in rankings.\n\nEfficiency measures: torch.profiler hooks compute forward/backward FLOPs, peak VRAM.  Time recorded via Python time.perf_counter().  Should add <2 % overhead relative to vanilla BOIL.\n\nExample code excerpt:\n\"\"\"python\nfor hp_cfg in candidate_cfgs:\n    curve = run_dqn(env, hp_cfg, seed)\n    score = transform_logistic_sacc(curve, mid, growth, lam) if variant=='boil+sacc' else other_compress(curve)\n    gp.update(hp_cfg, score)\n\"\"\"\n\nExpected outcome: BOIL+SACC beats all baselines on ≥3/4 primary metrics with p<0.05 and keeps runtime within 1.5 % of vanilla BOIL."
        },
        {
            "experiment_id": "exp-2-robustness-efficiency",
            "run_variations": [
                "boil",
                "boil+sacc",
                "boil+sacc-tail0.2",
                "tpe"
            ],
            "description": "Objective / Hypothesis: Stress-test SACC under high-variance, continuous-action environments and a larger hyper-parameter search space, evaluating robustness to reward noise, sensitivity to tail-fraction K, and computational efficiency.\n\nTasks (datasets): Hopper-v3, HalfCheetah-v3 (MuJoCo) plus Stochastic-CartPole (sticky-actions 0.5).\n\nModels:\n• Proximal Policy Optimisation (PPO, 3-layer MLP 256-256-128, tanh) for MuJoCo tasks.\n• DQN for Stochastic-CartPole (architecture as in exp-1).\n\nSearch space (7 D): learning-rate, γ, GAE-λ, clip-ε, entropy-coef, batch-size, target-network update (DQN only).  Same 5 initial random points + 40 BO evaluations (budget ↑ because space larger).\n\nPre-processing: reward clipping (±10) for MuJoCo, min-max scaling to [0,1] before GP fit.  Observations standardised online with running mean/var.\n\nData split & evaluation: 1 training episode = 1 M environment steps (MuJoCo) or 500 episodes (CartPole).  Stability measured over last 5 % of steps.  After HPO, retrain best HPs for 3 M steps, evaluate over 50 episodes.\n\nSeeds / repetitions: 8 seeds due to longer runs; statistics via Wilcoxon signed-rank.\n\nMetrics:\nPrimary –\n1) AU-Curve of best validation return vs evaluations,\n2) Time-to-threshold (≥3500 Hopper, ≥9000 HalfCheetah, ≥195 Stoch-CartPole),\n3) Test-phase reward std.\nSecondary – FLOPs/step, VRAM, wall-clock per evaluation, λ trajectory plots.\n\nHyper-parameter sensitivity: compare tail_frac=0.1 vs 0.2 (run_variation \"boil+sacc-tail0.2\"); sweep shown in appendix.\n\nRobustness analyses:\n• Noise injection: add Gaussian noise N(0,0.1) to rewards during training and re-evaluate.\n• OOD shift: retrain best HPs on modified gravity (MuJoCo +10 %).  Measure performance drop.\n• Adversarial perturbation (Fast Gradient Sign) on CartPole observations during evaluation; compute worst-case reward.\n\nComputational efficiency tracking: PyTorch profiler for FLOPs; nvidia-smi logging every 10 s for memory; shared script writes csv.\n\nExample code snippet:\n\"\"\"bash\npython run_hpo.py \\\n  --algo PPO \\\n  --method boil+sacc \\\n  --env Hopper-v3 \\\n  --budget 40 \\\n  --tail_frac 0.2 \\\n  --log_dir logs/exp2/hopper/seed${SEED}\n\"\"\"\n\nExpected outcome: BOIL+SACC maintains ≥20 % fewer evaluations-to-threshold and ≥25 % lower test-reward std under all stress settings, while adding ≤2 % compute cost.  Tail-fraction 0.2 shows slightly stronger stability but similar sample-efficiency, confirming moderate sensitivity."
        }
    ],
    "expected_models": [
        "DQN",
        "PPO",
        "GaussianProcessSurrogate"
    ],
    "expected_datasets": [
        "CartPole-v1",
        "LunarLander-v2",
        "Acrobot-v1",
        "Stochastic-CartPole",
        "Hopper-v3",
        "HalfCheetah-v3"
    ]
}
