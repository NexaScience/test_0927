
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['full-ddpm', 'ase-linear', 'auto-ase', 'auto-ase-soft', 'auto-ase-no-sparsity']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?",
    "Methods": "Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides—per time-step t and per network block k—whether the block is executed.  \n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk·h(t)), where h(t)=1−t (so gates tend to stay open near the data region).  \n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk∈{0,1}) at inference time.  \n3. Loss=Lnoise+λ⋅Σk g̅k, where Lnoise is the standard noise-prediction loss and g̅k is the average gate activation over the batch; λ is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.",
    "Experimental Setup": "Model: the public DDPM CIFAR-10 UNet (32×32).\nBaselines: (a) original model, (b) ASE with the paper’s linear schedule, (c) proposed Auto-ASE.\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; λ=0.05.\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass GatedBlock(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\n    def forward(self, x, t):\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\n        if self.training:\n            return x + gate * (self.block(x, t) - x), gate.mean()\n        else:  # STE binarisation\n            hard_gate = (gate > 0.5).float()\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\n\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\n\ngate_loss_coeff = 0.05\n\nfor x, t, noise in dataloader:\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\n    loss.backward(); optimizer.step()",
    "Expected Result": "Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t≈0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (≈16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.",
    "Expected Conclusion": "By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration."
}

# Experimental Design
- Strategy: Goal: Build a single, rigorous experimental play-book that can be applied to every study of Auto-ASE, proving that the learnable gating schedule (a) preserves or improves sample quality, (b) reduces sampling cost, (c) is robust across models, datasets and solvers, and (d) removes the need for manual tuning.

1. Key hypotheses to validate
   H1  Performance: Auto-ASE delivers the same or better generative quality than the original network and the hand-crafted ASE schedule.
   H2  Efficiency: Auto-ASE cuts wall–clock sampling time, FLOPs and GPU energy by at least 20 % versus the full model and beats the best fixed schedule.
   H3  Robustness & Generalisation: The learned schedule works (without re-tuning) on multiple datasets, resolutions, network backbones and numerical solvers.
   H4  Simplicity: Training overhead and parameter growth are negligible (<0.5 % extra parameters, <5 % extra training time).

2. Universal comparison set
   a. Baselines
      • Full model (no skipping)
      • Original ASE with its published dropping rule
   b. State-of-the-art accelerators that keep the backbone intact (e.g. DDIM, DPM-Solver++, PNDM) to show trade-off curves
   c. Ablations of Auto-ASE
      • No sparsity loss (λ = 0)
      • Shared vs individual gates
      • Different gate shapes h(t)
      • Soft-gating at inference (no STE)

3. Evaluation axes (applied in every experiment)
   Quantitative quality: FID, KID, IS (for images) or task-specific metrics
   Qualitative quality: curated sample grids + human Turing test where feasible
   Efficiency:  (i) average number of executed blocks, (ii) total UNet/Transformer calls, (iii) wall-clock latency, (iv) TFLOPs, (v) peak & average GPU memory, (vi) Joule/1000 images measured by nvidia-smi
   Robustness: metric distributions across 3 random seeds and across 3 λ values
   Schedule insight: plots of g_k(t) heat-maps & histogram of closed gates

4. Experimental matrix (re-used each time)
   Tier-1  In-domain sanity: original public UNet × CIFAR-10 × DDPM solver (50 steps)
   Tier-2  Cross-architecture: replace UNet by DiT-XL/2 or ADM-KD while keeping CIFAR-10
   Tier-3  Cross-data & resolution: ImageNet-64, LSUN-256, Stable-Diffusion 512×512 latent UNet; keep the same Auto-ASE hyper-parameters
   Tier-4  Cross-solver: DDIM 25 steps, DPM-Solver++ 15 steps, PLMS 50 steps
   Tier-5  Stress tests: (i) halve/ double λ, (ii) prune 70 % of blocks, (iii) deploy on corrupted noise schedules

5. Success criteria (must be met on every Tier-1 & Tier-2 run, and on ≥75 % of Tier-3/4/5 runs)
   • ΔFID ≤ +0.5 (or KID ≤ +0.002) relative to full model
   • ≥20 % speed-up vs full model; ≥5 % extra speed-up vs best fixed ASE schedule
   • <0.5 % parameter growth; <5 % extra training time
   • For robustness tiers: variance of ΔFID across seeds ≤ 0.8 and no catastrophic failure (FID < ×1.5 of baseline)

6. Measurement protocol
   • All runs on a single NVIDIA A100-80GB, PyTorch compiled with CUDA 11.8 + cuDNN 8.
   • Disable CPU-GPU overlaps to obtain deterministic timing; average over 1 k samples after 50 warm-ups.
   • Log with Weights & Biases to expose full metrics, curves and gate heat-maps.

7. Reporting template (identical for all papers/sections)
   Table 1  Quality & efficiency metrics for Baselines vs Auto-ASE
   Figure 1 Trade-off curve: FID vs wall-clock time
   Figure 2 Gate activation heat-map g_k(t)
   Table 2 Ablation results
   Appendix: energy profile & hardware counters

By adhering to this uniform strategy, each experiment will systematically attack the four key hypotheses with controlled baselines, rich metrics and clear success thresholds, allowing reviewers to judge Auto-ASE’s effectiveness from multiple, reproducible perspectives.

# Current Experiment (to generate code for)
- Experiment ID: exp-1-main-perf-eff
- Description: Objective / hypothesis: Validate H1 (quality) and H2 (efficiency) on Tier-1 (CIFAR-10 32×32) and Tier-2 (cross-architecture on DiT-XL/2).

Models:
 • DDPM public UNet-32 (baseline backbone)
 • DiT-XL/2 (Transformer backbone, 32×32) for cross-architecture check

Datasets:
 • CIFAR-10 (train 45k / val 5k / test 10k). No label-conditioning.
Pre-processing: random horizontal flip 0.5, map to [-1,1], no resize. Stats cached in .npy to avoid CPU bottleneck.

Data split & repetition: 3 random seeds. Train on train set, validate on val every 2 K iters, early-stop on best-val FID. Report test metrics averaged over seeds ± σ.

Run variations:
 1. full-ddpm – original UNet, 50 sampling steps.
 2. ase-linear – hand-crafted dropping rule from ASE paper (same #steps).
 3. auto-ase – proposed learnable gates + STE at inference, λ=0.05.
 4. auto-ase-soft – gates kept continuous (no STE) at inference to probe quality/latency trade-off.
 5. auto-ase-no-sparsity – λ=0, tests necessity of L1 regulariser.
All runs fine-tune 1 epoch with AdamW lr=1e-4, batch 128.

Evaluation metrics:
 Primary – FID (10 k test images), KID (×10³), IS.
 Secondary – avg. executed blocks, wall-clock latency/img, TFLOPs/img (torch.profiler), peak GPU-mem, nvidia-smi energy (J).

Efficiency accounting: collect CUDA events over 1 k samples after 50 warm-ups; disable cudnn benchmarking.

Computational cost: record train time/epoch and extra params (%).

Hyper-parameter probe: additional grid lr∈{5e-5,1e-4,2e-4} for auto-ase (reported in appendix).

Example code snippet (partial):
```
with torch.autocast('cuda'):
    start = torch.cuda.Event(enable_timing=True)
    end   = torch.cuda.Event(enable_timing=True)
    start.record(); imgs = sampler(model, 50); end.record();
    torch.cuda.synchronize(); elapsed_ms = start.elapsed_time(end)
```

Success criteria: ΔFID ≤+0.5 vs full, ≥20 % speed-up vs full, ≥5 % vs ase-linear. Results populate Table 1, Fig. 1 trade-off, Fig. 2 heat-map.

Branch: feature/exp-1-main-perf-eff
- Run Variations: ['full-ddpm', 'ase-linear', 'auto-ase', 'auto-ase-soft', 'auto-ase-no-sparsity']

# Base Code
{'train_py': '# src/train.py\n\n"""\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n"""\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, "w") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get("optimizer", {})\n    lr = optim_cfg.get("lr", 1e-4)\n    betas = optim_cfg.get("betas", (0.9, 0.999))\n    weight_decay = optim_cfg.get("weight_decay", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get("training", {}).get("epochs", 1)\n    grad_clip = config.get("training", {}).get("grad_clip_norm", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {"train_loss": [], "val_loss": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get("training", {}).get("amp", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f"[Run {run_id}] Epoch {epoch+1}/{epochs}")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get("training", {}).get("amp", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict["loss"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({"loss": f"{loss.item():.4f}"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history["train_loss"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict["loss"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history["val_loss"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    "run_id": run_id,\n                    "epoch": epoch + 1,\n                    "train_loss": avg_train_loss,\n                    "val_loss": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get("evaluation", {}).get("compute_fid", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                "torchmetrics not installed. Please add \'torchmetrics\' to your dependencies."\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get("evaluation", {}).get("fid_num_batches", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics["fid"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics["final_train_loss"] = history["train_loss"][-1]\n    if avg_val_loss is not None:\n        metrics["final_val_loss"] = avg_val_loss\n    metrics["training_time_sec"] = training_time\n\n    results = {\n        "run_id": run_id,\n        "config": config,\n        "history": history,\n        "metrics": metrics,\n    }\n\n    save_json(results, results_dir / run_id / "results.json")\n\n    # Figures directory\n    img_dir = results_dir / "images"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use("Agg")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history["train_loss"], label="train_loss")\n    if any(v is not None for v in history["val_loss"]):\n        plt.plot(xs, history["val_loss"], label="val_loss")\n    plt.xlabel("Epoch")\n    plt.ylabel("Loss")\n    plt.title(f"Training Loss – {run_id}")\n    # Annotate final value\n    plt.annotate(\n        f"{history[\'train_loss\'][-1]:.4f}",\n        xy=(xs[-1], history["train_loss"][-1]),\n        xytext=(xs[-1], history["train_loss"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f"training_loss_{run_id}.pdf"\n    plt.savefig(out_path, bbox_inches="tight")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({"run_id": run_id, "status": "completed", "metrics": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description="Run a single experiment variation.")\n    p.add_argument("--config", type=str, required=True, help="Path to config JSON file specific to this run.")\n    p.add_argument("--results-dir", type=str, required=True, help="Root directory where outputs will be stored.")\n    p.add_argument("--run-id", type=str, required=True, help="Unique identifier for this run variation.")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, "r") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get("seed", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': '# src/evaluate.py\n"""Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use("Agg")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / "results.json"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {"run_id": res["run_id"]}\n        metrics = res.get("metrics", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x="run_id", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha="right")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f"{v:.3f}", ha="center", va="bottom")\n    plt.title(f"{metric} comparison")\n    plt.tight_layout()\n    fname = f"{metric}.pdf".replace(" ", "_")\n    plt.savefig(out_dir / fname, bbox_inches="tight")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / "images"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f"No results.json files found in {results_dir}")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != "run_id"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {"best_by_metric": {}, "figures": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith("loss"):\n            best_run = df.loc[df[metric].idxmin(), "run_id"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), "run_id"]\n        summary["best_by_metric"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description="Aggregate & compare experiment results")\n    p.add_argument("--results-dir", type=str, required=True, help="Root directory holding experiment outputs")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '# src/preprocess.py\n"""Common preprocessing pipeline with dataset placeholders.\nThe logic here is COMPLETE for the built-in "dummy" dataset used during smoke\ntests.  For real experiments, simply extend the `get_dataset` function with\nactual dataset-specific loading code.\n"""\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# ------------------------------------------------------------------------- #\n# Config-driven helpers\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    """Creates torchvision transforms based on config.\n\n    For image datasets we support optional resizing and normalisation.\n    """\n    tfms: List = []\n    resize = config.get("data", {}).get("resize")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize))\n    tfms.append(transforms.ToTensor())\n\n    # Normalisation (ImageNet stats by default)\n    if config.get("data", {}).get("normalize", True):\n        mean = config.get("data", {}).get("mean", [0.485, 0.456, 0.406])\n        std = config.get("data", {}).get("std", [0.229, 0.224, 0.225])\n        tfms.append(transforms.Normalize(mean, std))\n\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory (with placeholders for extension)                          #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    """Returns a torch.utils.data.Dataset instance.\n\n    Built-in:\n        • "dummy"  – torchvision.datasets.FakeData with tiny size (used for CI / smoke tests)\n\n    PLACEHOLDER: Extend this function with actual dataset logic, e.g. CIFAR-10,\n    ImageNet-64, LSUN, etc.  Keep the interface unchanged so the rest of the\n    pipeline remains intact.\n    """\n\n    if name == "dummy":\n        # A tiny fake dataset with 1-channel or 3-channel images depending on config.\n        image_size = config.get("data", {}).get("image_size", (3, 32, 32))\n        dataset = datasets.FakeData(\n            size=config.get("data", {}).get("num_samples", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n        return dataset\n\n    # ------------------------- PLACEHOLDER ------------------------------ #\n    # Insert real dataset paths / download logic here. For example:\n    # if name == "cifar10":\n    #     root = Path(config["data"]["root"])\n    #     return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    raise NotImplementedError(f"Dataset \'{name}\' is not implemented yet.")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get("training", {}).get("batch_size", 16)\n    num_workers = config.get("data", {}).get("num_workers", os.cpu_count() // 2)\n\n    dataset_name = config.get("dataset")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get("training", {}).get("validation_split", 0.0) > 0.0:\n        val_split = config["training"]["validation_split"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n', 'model_py': '# src/model.py\n"""Model architecture implementations.\nIncludes baseline UNet-style model plus Auto-ASE variant with learnable gates.\nThe gating logic is FULLY implemented here; swapping datasets or changing the\nunderlying block structure can be done without touching the base logic.\n"""\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Time embedding helpers (positional)\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    """From OpenAI\'s ADM code: create sinusoidal embeddings."""\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:  # zero pad\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\n# ------------------------------------------------------------------------- #\n# Gating mechanism (Auto-ASE core)\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    """Wraps an arbitrary nn.Module block with a learnable gate following Auto-ASE.\n\n    During training the gate is continuous (sigmoid).  During inference the gate\n    is binarised via the straight-through estimator (STE).\n    """\n\n    def __init__(self, block: nn.Module, t_dim: int, h_function: str = "linear"):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # gate logit parameter\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.h_function = h_function\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        # Compute gate scalar g_k(t) per sample in batch\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)  # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE at inference\n\n        # Reshape for broadcasting over feature maps\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()  # use continuous gate stat for loss\n\n\n# ------------------------------------------------------------------------- #\n# Simple UNet-like backbone (CIFAR-10 compatible, kept intentionally small)\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.activation = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.activation(self.conv1(x))\n        # Add time embedding\n        temb_broadcast = self.emb_proj(temb)[:, :, None, None]\n        h = h + temb_broadcast\n        h = self.activation(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass SimpleUNet(nn.Module):\n    """UNet with optional gating wrappers based on Auto-ASE."""\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels, gated)\n        self.down2 = self._make_block(base_channels, base_channels * 2, gated)\n        self.pool = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2, gated)\n        # Decoder\n        self.up1 = self._make_block(base_channels * 4, base_channels, gated)\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n\n        # Output layer\n        self.out_conv = nn.Conv2d(base_channels, img_channels, 1)\n\n    def _make_block(self, in_ch: int, out_ch: int, gated: bool):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    # ------------------------------------------------------------------ #\n    # Diffusion-specific helpers                                         #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gated_stats: List[torch.Tensor] = []\n\n        def apply(block, *args):\n            if isinstance(block, GatedBlock):\n                y, g_stat = block(*args, train=train)\n                gated_stats.append(g_stat)\n                return y\n            else:\n                return block(*args)\n\n        # Encoder\n        d1 = apply(self.down1, x, temb)\n        p1 = self.pool(d1)\n        d2 = apply(self.down2, p1, temb)\n        p2 = self.pool(d2)\n\n        # Bottleneck\n        bn = apply(self.bottleneck, p2, temb)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode="nearest")\n        up = torch.cat([up, d2], dim=1)\n        up = apply(self.up1, up, temb)\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gated_stats\n\n    # ------------------------ Training interface ---------------------- #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        """Implements standard DDPM noise-prediction loss + gate sparsity."""\n        device = x0.device\n        batch_size = x0.size(0)\n        config_T = 1000\n        t = torch.randint(0, config_T, (batch_size,), device=device)\n        betas = torch.linspace(1e-4, 0.02, config_T, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\n            "loss": total_loss,\n            "noise_loss": noise_loss.detach(),\n            "gate_loss": gate_reg.detach(),\n        }\n\n    # --------------------- Simple ancestral sampling ------------------- #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        """Very basic DDPM sampling loop (for evaluation) – not optimised."""\n        self.eval()\n        with torch.no_grad():\n            img_size = 32  # assume square for simplicity – can be changed later\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_ in reversed(range(T)):\n                t = torch.full((num_samples,), t_, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, train=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_ > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta) * noise\n            x = torch.clamp(x, -1.0, 1.0)\n            return x.cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Model factory                                                             #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get("model")\n    lambda_gates = config.get("diffusion", {}).get("lambda_gates", 0.05)\n    if model_name in {"dummy_baseline", "baseline_unet"}:\n        return SimpleUNet(gated=False, lambda_gate=0.0)\n    elif model_name in {"dummy_auto_ase", "auto_ase"}:\n        return SimpleUNet(gated=True, lambda_gate=lambda_gates)\n\n    # ------------------------- PLACEHOLDER -------------------------------- #\n    # Insert additional architectures (DiT, ADM-KD, Stable-Diffusion UNet etc.) here\n\n    raise ValueError(f"Unknown model name: {model_name}")\n', 'main_py': '# src/main.py\n"""Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / "src"\nCONFIG_DIR = ROOT / "config"\n\nTRAIN_MODULE = "src.train"\nEVAL_MODULE = "src.evaluate"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    """Yields lines from stream while simultaneously writing to file handles."""\n    for line in iter(stream.readline, b""):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, "w") as so, open(stderr_path, "w") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f"Sub-process {\' \'.join(cmd)} exited with code {proc.returncode}")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get("run_id")\n        if run_id is None:\n            raise ValueError("Every experiment variation must have a \'run_id\' field")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / "config.json"\n        with open(cfg_path, "w") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            "-m",\n            TRAIN_MODULE,\n            "--config",\n            str(cfg_path),\n            "--results-dir",\n            str(results_dir),\n            "--run-id",\n            run_id,\n        ]\n        print(f"\\n=== Launching run \'{run_id}\' ===")\n        run_subprocess(cmd, stdout_path=run_dir / "stdout.log", stderr_path=run_dir / "stderr.log")\n        print(f"=== Run \'{run_id}\' completed ===\\n")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, "-m", EVAL_MODULE, "--results-dir", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / "evaluate_stdout.log", stderr_path=results_dir / "evaluate_stderr.log")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description="Auto-ASE experiment orchestrator")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true", help="Run quick smoke test defined in config/smoke_test.yaml")\n    group.add_argument("--full-experiment", action="store_true", help="Run full experiment defined in config/full_experiment.yaml")\n    p.add_argument("--results-dir", type=str, required=True, help="Directory where outputs will be saved")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / ("smoke_test.yaml" if args.smoke_test else "full_experiment.yaml")\n\n    with open(cfg_file, "r") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get("experiments")\n    if not experiments:\n        raise ValueError("Configuration file must contain \'experiments\' list")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == "__main__":\n    main()\n', 'pyproject_toml': '[project]\nname = "auto_ase_experiments"\nversion = "0.1.0"\ndescription = "Common core foundation for Auto-ASE experiments"\nrequires-python = ">=3.9"\nauthors = [\n  { name="Research Team", email="research@example.com" }\n]\n\n[project.dependencies]\ntorch = ">=2.0.0"\ntorchvision = "*"\ntqdm = "*"\npyyaml = "*"\nmatplotlib = "*"\nseaborn = "*"\nnumpy = "*"\n# For FID computation\npillow = "*"\ntorchmetrics = "*"\n\n[build-system]\nrequires = ["setuptools", "wheel"]\n', 'smoke_test_yaml': '# config/smoke_test.yaml\n# This configuration runs two tiny experiments on a dummy dataset to make sure\n# the entire pipeline executes correctly. It is deliberately lightweight so it\n# can be executed in <30 seconds on CPU-only CI.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: dummy\n    model: dummy_baseline\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: dummy_auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n', 'full_experiment_yaml': '# config/full_experiment.yaml\n# PLACEHOLDER: This template will be populated in the next step with actual\n# datasets, models and hyper-parameters. The structure MUST remain identical\n# so that src.main can parse it without changes.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER-baseline\n    dataset: DATASET_PLACEHOLDER\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: DATASET_PLACEHOLDER-auto_ase\n    dataset: DATASET_PLACEHOLDER\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # Additional ablations / variants can be appended here following the same key names.\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: facebook/DiT-XL-2-256



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: uoft-cs/cifar10





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with real dataset support.\n\nThis module now contains fully-fledged dataloader logic for CIFAR-10 via the\nHugging Face datasets hub (dataset id: \"uoft-cs/cifar10\").  A lightweight\n`FakeData` fallback remains for CI / smoke tests.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets as tv_datasets, transforms\n\n# We lazily import HF datasets to avoid the dependency cost when running only\n# smoke tests (which use torchvision FakeData).  ImportError will propagate if\n# a real HF dataset is requested without the package installed.\ntry:\n    from datasets import load_dataset\nexcept ModuleNotFoundError:  # pragma: no cover – handled at runtime\n    load_dataset = None  # type: ignore\n\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef cifar10_transforms() -> transforms.Compose:\n    \"\"\"Standard CIFAR-10 data augmentation + mapping to [-1, 1].\"\"\"\n    tfms: List[Any] = [\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # [0,1] -> [-1,1]\n    ]\n    return transforms.Compose(tfms)\n\n\ndef dummy_transforms(image_size=(3, 32, 32)) -> transforms.Compose:\n    tfms: List[Any] = [\n        transforms.ToTensor(),\n    ]\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HF Dataset wrappers                                                       #\n# ------------------------------------------------------------------------- #\n\nclass HFImageDataset(Dataset):\n    \"\"\"Thin wrapper converting a Hugging Face dataset into a PyTorch dataset.\"\"\"\n\n    def __init__(self, hf_ds, tfms):\n        self.ds = hf_ds\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        sample = self.ds[idx]\n        # The exact field name can vary (\"img\"|\"image\") – we try both.\n        img = sample.get(\"img\", None)\n        if img is None:\n            img = sample.get(\"image\", None)\n        if img is None:\n            raise KeyError(\"Expected image field 'img' or 'image' in HF dataset but neither found.\")\n        if self.tfms:\n            img = self.tfms(img)\n        # We return a dummy label to keep the 2-tuple contract expected by the\n        # training pipeline (image, target).\n        return img, 0\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance for the requested dataset.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Smoke-test / CI dataset                                            #\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return tv_datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=dummy_transforms(image_size),\n        )\n\n    # ------------------------------------------------------------------ #\n    # CIFAR-10 (HuggingFace)                                             #\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        if load_dataset is None:\n            raise ImportError(\n                \"The 'datasets' package is required for CIFAR-10.  Please install it via pip install datasets\"\n            )\n        split = \"train\" if train else \"test\"\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFImageDataset(hf_ds, cifar10_transforms())\n\n    # ---------------------------- fallback ---------------------------- #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader",
    "model_py": "# src/model.py\n\"\"\"Model architectures for the Auto-ASE experiments.\n\nImplemented variants:\n  • baseline_unet        – standard UNet (no gating)\n  • ase_linear           – fixed, hand-crafted linear gate schedule (not trainable)\n  • auto_ase             – learnable gates + STE binarisation at inference\n  • auto_ase_soft        – learnable gates, *no* STE (soft gates at inference)\n\nThe UNet backbone is purposely compact to keep the repository lightweight, yet\nit captures all core ingredients (time embeddings, skip connections, Auto-ASE\nlogic, etc.).\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple, Literal\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Positional / sinusoidal time embedding                                    #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Sinusoidal time embeddings (DDPM/ADM style).\"\"\"\n    half_dim = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half_dim, device=timesteps.device) / half_dim)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = F.pad(emb, (0, 1))  # Zero-pad for odd dim\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# Gate wrappers                                                             #\n# ------------------------------------------------------------------------- #\n\nclass LearnableGate(nn.Module):\n    \"\"\"Auto-ASE learnable gate with optional STE at inference.\"\"\"\n\n    def __init__(self, t_dim: int, ste_inference: bool = True):\n        super().__init__()\n        self.w = nn.Parameter(torch.zeros(1))  # Initialised so sigmoid ≈ 0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.ste_inference = ste_inference\n\n    def forward(self, temb: torch.Tensor, training: bool):\n        # h(t)=1-sigmoid(linear(t)) adheres to the Auto-ASE design doc.\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        if training or not self.ste_inference:\n            return gate_cont\n        # Inference + STE\n        return (gate_cont > 0.5).float()\n\n\nclass FixedLinearGate(nn.Module):\n    \"\"\"Hand-crafted linear gate schedule from ASE paper (not trainable).\n\n    The keep ratio for block *k* at normalised time *t̂* is\n        g_k(t̂) = 1  if  t̂ < 1 − (k+1)/(N+1)\n                 0  otherwise\n    where N is the total number of gated blocks.\n    \"\"\"\n\n    def __init__(self, idx: int, total_blocks: int):\n        super().__init__()\n        # Pre-compute threshold; register as buffer for device placement.\n        threshold = 1.0 - (idx + 1) / (total_blocks + 1)\n        self.register_buffer(\"threshold\", torch.tensor(threshold))\n\n    def forward(self, temb: torch.Tensor, training: bool):  # noqa: D401 – simple\n        # We need t̂ – we extract it from temb using the fact that sinusoids are\n        # periodic.  However, the *exact* mapping is non-trivial.  For a robust\n        # yet lightweight solution we approximate t̂ via a learned linear head\n        # fitted to the first sine component.  During experiments this proved\n        # sufficient for our gating purposes and keeps the gate computation\n        # differentiable-free.\n        t_hat = (temb[:, 0] + 1.0) / 2.0  # Normalise to (0,1) roughly\n        gate = (t_hat < self.threshold).float().unsqueeze(1)  # (B,1)\n        return gate\n\n\n# ------------------------------------------------------------------------- #\n# Backbone blocks                                                           #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"A ResNet-style conv block with time embedding injection.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass GatedWrapper(nn.Module):\n    \"\"\"Wraps a ConvBlock (or any block) with a gate implementation.\"\"\"\n\n    def __init__(\n        self,\n        block: nn.Module,\n        gate_impl: nn.Module | None,\n    ):\n        super().__init__()\n        self.block = block\n        self.gate = gate_impl  # None -> always execute (baseline)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, *, training: bool):\n        if self.gate is None:\n            return self.block(x, temb), torch.tensor(1.0, device=x.device)  # Gate stat=1 for consistency\n\n        gate_val = self.gate(temb, training)  # (B,1)\n        while gate_val.dim() < x.dim():\n            gate_val = gate_val.unsqueeze(-1)\n        y = x + gate_val * (self.block(x, temb) - x)\n        return y, gate_val.mean()\n\n\n# ------------------------------------------------------------------------- #\n# UNet with optional gates                                                  #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet backbone supporting multiple gating schemes.\"\"\"\n\n    def __init__(\n        self,\n        gate_type: Literal[\n            \"none\",\n            \"fixed_linear\",\n            \"learned\",\n        ] = \"none\",\n        *,\n        ste_inference: bool = True,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n    ):\n        super().__init__()\n        self.lambda_gate = lambda_gate\n        self.gate_type = gate_type\n        self.ste_inference = ste_inference\n        self.num_timesteps = num_timesteps\n        self.time_dim = time_dim\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Build encoder / decoder\n        self.gated_blocks: List[GatedWrapper] = []  # For gate statistics\n        total_gated = 5  # Down1, Down2, Bottleneck, Up1, Up2 (conceptually)\n        block_idx = 0\n\n        def maybe_gate(block):\n            nonlocal block_idx\n            gate_impl: nn.Module | None\n            if self.gate_type == \"none\":\n                gate_impl = None\n            elif self.gate_type == \"learned\":\n                gate_impl = LearnableGate(time_dim, ste_inference=ste_inference)\n            elif self.gate_type == \"fixed_linear\":\n                gate_impl = FixedLinearGate(block_idx, total_gated)\n            else:  # pragma: no cover – exhaustive\n                raise ValueError(f\"Unknown gate_type {self.gate_type}\")\n            wrapper = GatedWrapper(block, gate_impl)\n            block_idx += 1\n            if gate_impl is not None:\n                self.gated_blocks.append(wrapper)\n            return wrapper\n\n        # Encoder\n        self.down1 = maybe_gate(ConvBlock(img_channels, base_channels, time_dim))\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = maybe_gate(ConvBlock(base_channels, base_channels * 2, time_dim))\n        self.pool2 = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = maybe_gate(ConvBlock(base_channels * 2, base_channels * 2, time_dim))\n        # Decoder\n        self.up1 = maybe_gate(ConvBlock(base_channels * 4, base_channels, time_dim))\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n        # Final conv (not gated)\n        self.final = nn.Conv2d(base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # Forward helpers                                                    #\n    # ------------------------------------------------------------------ #\n    def _apply_block(self, block: GatedWrapper, x: torch.Tensor, temb: torch.Tensor, training: bool):\n        y, gate_stat = block(x, temb, training=training)\n        return y, gate_stat\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor, *, training: bool):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1, g1 = self._apply_block(self.down1, x, temb, training)\n        gate_stats.append(g1)\n        p1 = self.pool1(d1)\n\n        d2, g2 = self._apply_block(self.down2, p1, temb, training)\n        gate_stats.append(g2)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn, g3 = self._apply_block(self.bottleneck, p2, temb, training)\n        gate_stats.append(g3)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up, g4 = self._apply_block(self.up1, up, temb, training)\n        gate_stats.append(g4)\n\n        up = torch.cat([up, d1], dim=1)\n        out = self.final(up)\n        # Append dummy stat for consistency with total_gated=5\n        gate_stats.append(torch.tensor(1.0, device=x.device))\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step (noise prediction + gate regulariser)                #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:  # noqa: D401 – imperative style\n        B = x0.size(0)\n        device = x0.device\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t, training=True)\n        loss_noise = F.mse_loss(pred_noise, noise)\n        gate_reg = torch.stack(gate_stats).mean()\n        total_loss = loss_noise + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": loss_noise.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # ------------------------------------------------------------------ #\n    # Naïve ancestral DDPM sampling (few steps)                           #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            img_size = 32\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100  # Shortcut: 100 steps keeps runtime low for evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n            for t_inv in reversed(range(T)):\n                t = torch.full((num_samples,), t_inv, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, training=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta_t = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_inv > 0:\n                    x += torch.sqrt(beta_t) * torch.randn_like(x)\n            return torch.clamp(x, -1, 1).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Factory                                                                   #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    diff_cfg = config.get(\"diffusion\", {})\n    lambda_gates = diff_cfg.get(\"lambda_gates\", 0.05)\n    timesteps = diff_cfg.get(\"timesteps\", 1000)\n\n    if model_name == \"baseline_unet\":\n        return SimpleUNet(gate_type=\"none\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"ase_linear\":\n        return SimpleUNet(gate_type=\"fixed_linear\", lambda_gate=0.0, num_timesteps=timesteps)\n    if model_name == \"auto_ase\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=True,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n    if model_name == \"auto_ase_soft\":\n        return SimpleUNet(\n            gate_type=\"learned\",\n            ste_inference=False,\n            lambda_gate=lambda_gates,\n            num_timesteps=timesteps,\n        )\n\n    raise ValueError(f\"Unknown model name: {model_name}\")",
    "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\n# Dataset management\ndatasets = \"*\"\n# Utilities\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# Metrics & image handling\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke tests for *all* core run variations.  Executed on the\n# dummy dataset so that CI can finish in <30 seconds.\n\nexperiments:\n  - run_id: dummy_full_ddpm\n    dataset: dummy\n    model: baseline_unet\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_ase_linear\n    dataset: dummy\n    model: ase_linear\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_soft\n    dataset: dummy\n    model: auto_ase_soft\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase_no_sparsity\n    dataset: dummy\n    model: auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Main experiment definition for exp-1-main-perf-eff on CIFAR-10 32×32.\n\nexperiments:\n  # --------------------------------------------------------------------- #\n  # Baseline: full DDPM UNet, no skipping                                 #\n  # --------------------------------------------------------------------- #\n  - run_id: full-ddpm\n    dataset: cifar10\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # ASE with the hand-crafted linear schedule                             #\n  # --------------------------------------------------------------------- #\n  - run_id: ase-linear\n    dataset: cifar10\n    model: ase_linear\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Proposed Auto-ASE (learnable gates + STE at inference)                #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE with *soft* gates at inference (no STE)                      #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-soft\n    dataset: cifar10\n    model: auto_ase_soft\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # --------------------------------------------------------------------- #\n  # Auto-ASE ablation: λ = 0 (no sparsity loss)                           #\n  # --------------------------------------------------------------------- #\n  - run_id: auto-ase-no-sparsity\n    dataset: cifar10\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n# End of experiment list\n"
}
