
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/` directory, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "ρBYOL and most contrastive/self-distillation methods pick two random temporal crops from the same clip and force their representations to match equally, no matter whether the crops are 1 frame or 3 s apart. On rapidly changing rodent behaviour this causes:\n1) Positive pairs that are too far apart in time to depict the same action, blurring fine-grained motion cues.\n2) Weak ordering of the latent space with respect to temporal distance, which hurts downstream behaviour segmentation and rare-action recognition.\nA fix that does not require new architectures is to make the objective aware of the temporal distance between the two views.",
    "Methods": "Time-Weighted BYOL (TW-BYOL)\nMinimal change: multiply BYOL’s mean-squared error loss between the online and target projections by an exponential weight that decays with frame distance Δt between the two augmented views.\nL_TW = w(Δt) * || q( f_θ( v_i ) ) – stop_grad( f_ϕ( v_j ) ) ||² ,  \nwith w(Δt)=exp(-Δt / τ) and a single hyper-parameter τ (e.g. 30 frames).\nTheoretical motivation: For small Δt the two views are likely to be the same action, so the model should align them strongly; for large Δt alignment is weakened, allowing the network to separate different actions and preserve temporal information. The weighting acts as a soft regulariser imposing temporal locality without introducing extra negatives or additional networks.",
    "Experimental Setup": "Dataset: MABe22 mice-triplet videos (video modality only).\nTraining: reproduce original ρBYOL recipe (SlowFast-8×8 backbone, same augmentations) and TW-BYOL (identical but with the weighted loss).\nHyper-parameter sweep: τ ∈ {15, 30, 60} frames.\nEvaluation: linear probe F1 on the 8 hidden downstream mice tasks; report mean and per-task scores.\nBaseline: official ρBYOL numbers from MABe22 leaderboard.\nCompute: 4×A100 GPUs, 50 epochs (same as baseline).",
    "Experimental Code": "# only loss change shown\nimport torch, math\n\ndef time_weighted_byol_loss(p_online, z_target, frame_dist, tau=30.):\n    \"\"\"p_online and z_target: (B,D) tensors; frame_dist: (B,) ints\"\"\"\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\n    loss = (p_online - z_target.detach()).pow(2).sum(dim=1)  # (B,)\n    weighted_loss = (weight * loss).mean()\n    return weighted_loss\n\n# inside training loop\np = projector_online(backbone_online(view_i))      # (B,D)\nz = projector_target(backbone_target(view_j))      # (B,D)\nframe_dist = torch.abs(frame_idx_i - frame_idx_j)  # (B,)\nloss = time_weighted_byol_loss(p, z, frame_dist, tau=30)",
    "Expected Result": "Across three random seeds the mean F1 on the mice tasks is expected to rise by ~2-3 points (e.g. ρBYOL 68.5 → TW-BYOL 71.0). Improvements should be largest on tasks requiring discrimination of short, bursty actions (e.g. grooming vs sniffing) while remaining neutral on slowly varying experimental condition classification.",
    "Expected Conclusion": "A single, two-line weighting term makes the self-distillation objective respect temporal locality, sharpening representations for fast rodent behaviours. Because no extra negatives, memory, or architectural changes are introduced, the method keeps BYOL’s simplicity and training speed while delivering measurable accuracy gains on fine-grained behaviour annotation tasks."
}

# Experimental Design
## Experiment Strategy
Global Goal
Prove that the proposed Time-Weighted BYOL (TW-BYOL) yields temporally better-ordered, more behaviour-discriminative and equally efficient video representations than existing self-supervised alternatives, while remaining robust to hyper-parameter choices and generalising across rodent datasets.

1. Core Hypotheses to Validate
   H1 – Performance: TW-BYOL improves downstream behaviour recognition (overall F1, rare-action recall, few-shot transfer).
   H2 – Temporal Awareness: embeddings respect temporal proximity (distance in latent space grows with frame gap).
   H3 – Efficiency: training speed, GPU memory and wall-clock cost stay within ±5 % of ρBYOL.
   H4 – Robustness: gains hold under different τ values, random seeds, crop strategies and limited labelled data.
   H5 – Generalisation: improvements transfer to unseen rodents/tasks and to a second behaviour dataset.

2. Comparative Framework
   a. Baseline: reproduced ρBYOL recipe.
   b. State-of-the-art self-supervised video baselines: MoCo-v3, SimCLR-v2, TimeContrast.
   c. Supervised upper bound: same backbone trained with full labels (for context only).
   d. Ablations:
      • No weighting (ρBYOL loss) – “Uniform”.
      • Hard cut-off weighting – “Binary”.
      • Alternative decays (linear, inverse square) to test the importance of exponential form.
      • τ sweep (15, 30, 60 frames).

3. Experimental Angles
   3.1 Quantitative Performance
       • Linear-probe F1 per task and averaged (primary metric).
       • k-NN accuracy (label-free evaluation of representation quality).
       • Rare-action recall (top-20 % least frequent labels).
   3.2 Temporal Locality Analysis
       • Spearman correlation ρ between embedding distance and frame gap Δt.
       • Temporal retrieval: mean reciprocal rank when querying a frame for its 5 nearest temporal neighbours.
   3.3 Efficiency Metrics
       • GPU hours per pre-training run.
       • Samples / sec and peak VRAM.
   3.4 Qualitative
       • t-SNE / UMAP plots coloured by action and by timestamp.
       • Video retrieval demos.
   3.5 Robustness & Generalisation
       • Sensitivity curves over τ and crop policies.
       • Subset-of-data training (25 %, 50 % of unlabelled video) to test data efficiency.
       • Cross-dataset transfer: pre-train on MABe22, evaluate on a second rodent-behaviour set (e.g., RatSI).

4. Validation Criteria for Success
   Pass if ALL are met:
   • +2 F1 absolute (≥ p<0.05, paired t-test over 3 seeds) versus ρBYOL on mean of 8 tasks.
   • At least 6/8 tasks individually improve or remain equal.
   • Embedding–time correlation improves by ≥10 % over ρBYOL.
   • GPU hours increase ≤5 %.
   • Variance of F1 across seeds not higher than ρBYOL.
   • Improvements persist (≥75 % retained) when τ∈[15,60] or when only 50 % of unlabelled video is available.

5. Experimental Protocol
   • Hardware: up to 4×A100 80 GB per run; mixed-precision training; identical data-loading pipeline for all methods.
   • Controlled compute: fix batch size, epochs (50), optimiser and augmentation suite; record seeds.
   • Run each configuration 3× for statistics.
   • Hyper-parameter grid executed with identical wall-clock budget; schedule runs via SLURM to exploit 2 TB RAM node.
   • Evaluation code placed in a separate repo; blind-test labels kept hidden until final metrics are logged to ensure fairness.

6. Multi-Perspective Demonstration Strategy
   a. Start with baseline vs TW-BYOL to establish headline gains.
   b. Add ablation study to attribute gains specifically to exponential weighting and to choice of τ.
   c. Compare against external SOTA to position method in field.
   d. Present temporal locality analyses to back mechanistic claim.
   e. Provide efficiency table to show “no free lunch” avoided.
   f. Show robustness curves and cross-dataset transfer to argue for broad applicability.

This unified strategy will be executed for every subsequent experiment, ensuring that each study supplies comparable evidence along performance, temporal fidelity, efficiency, robustness and generalisation axes while sharing compute budgets and evaluation protocols across the research programme.

# Generated Base Code Files
{"evaluate_py": "\"\"\"\nevaluate.py \u2013 Aggregate \u0026 compare results of the run variations.\nReads all sub-directories in --results-dir that contain results.json, compiles\ncomparison tables \u0026 figures and writes them to stdout + images/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_TOPIC_FINAL_LOSS = \"final_loss\"\n\n################################################################################\n# -----------------------------  utilities  -----------------------------------#\n################################################################################\n\ndef collect_results(results_dir: Path) -\u003e List[Dict]:\n    records = []\n    for run_dir in results_dir.iterdir():\n        file = run_dir / \"results.json\"\n        if file.exists():\n            with open(file, \"r\", encoding=\"utf-8\") as fp:\n                records.append(json.load(fp))\n    return records\n\n################################################################################\n# --------------------------  figure helpers  ---------------------------------#\n################################################################################\n\ndef plot_final_loss(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(data=df, x=\"run_id\", y=\"final_val_loss\", hue=\"algorithm\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Loss\")\n    ax.set_title(\"Final Validation Loss Across Experiments\")\n\n    # annotate each bar\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=8)\n\n    plt.tight_layout()\n    fname = f\"{FIG_TOPIC_FINAL_LOSS}.pdf\"\n    images_dir = out_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n    plt.savefig(images_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef main(results_dir: Path):\n    records = collect_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n\n    df = pd.DataFrame(records)\n    # ---------------------------------------------------------------- figures\n    fig_files = []\n    fig_files.append(plot_final_loss(df, results_dir))\n\n    # --------------------------------------------------------- stdout outputs\n    comparison = {\n        \"num_runs\": len(records),\n        \"best_final_val_loss\": df[\"final_val_loss\"].min(),\n        \"worst_final_val_loss\": df[\"final_val_loss\"].max(),\n        \"figure_files\": fig_files,\n    }\n\n    print(\"\\n===== Cross-Run Comparison Summary =====\")\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(Path(args.results_dir))", "full_experiment_yaml": "# Full experiment configuration \u2013 place only PLACEHOLDERs here.\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: |\n      # PLACEHOLDER: Replace with actual experiment description\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      root: DATASET_PATH_PLACEHOLDER\n      params: {}\n    model:\n      type: MODEL_PLACEHOLDER\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: ALGORITHM_PLACEHOLDER  # e.g., BYOL, TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 50  # PLACEHOLDER: override if needed\n      batch_size: 64\n      learning_rate: 1e-3\n  # Add additional experiment blocks as needed\n", "main_py": "\"\"\"\nmain.py \u2013 Experiment orchestrator.\nReads smoke_test.yaml or full_experiment.yaml, spawns src/train.py sequentially\nfor each run variation, captures logs, and finally launches src/evaluate.py.\n\"\"\"\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n################################################################################\n# -----------------------------  log helpers  ---------------------------------#\n################################################################################\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run *cmd* while tee-ing stdout / stderr to the given files + parent console.\"\"\"\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as out_fp, open(stderr_path, \"w\", encoding=\"utf-8\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                sys.stdout.write(stdout_line)\n                out_fp.write(stdout_line)\n            if stderr_line:\n                sys.stderr.write(stderr_line)\n                err_fp.write(stderr_line)\n            if stdout_line == \"\" and stderr_line == \"\" and process.poll() is not None:\n                break\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n################################################################################\n# -----------------------------  orchestrator  --------------------------------#\n################################################################################\n\ndef load_yaml(path: Path) -\u003e Dict:\n    with open(path, \"r\", encoding=\"utf-8\") as fp:\n        return yaml.safe_load(fp)\n\n\ndef create_temp_run_config(run_cfg: Dict) -\u003e Path:\n    \"\"\"Write *run_cfg* to a NamedTemporaryFile and return its path.\"\"\"\n    tmp = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\")\n    json.dump(run_cfg, tmp)\n    tmp.flush()\n    return Path(tmp.name)\n\n\ndef main(args):\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    exp_cfg = load_yaml(cfg_path)\n\n    runs = exp_cfg.get(\"experiments\", [])\n    if not runs:\n        print(f\"No experiments defined in {cfg_path}\")\n        sys.exit(1)\n\n    for run in runs:\n        run_id = run[\"run_id\"]\n        print(f\"\\n=== Launching run: {run_id} ===\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        temp_cfg_path = create_temp_run_config(run)\n        cmd = [\n            sys.executable, \"-m\", \"src.train\",\n            \"--run-config\", str(temp_cfg_path),\n            \"--results-dir\", str(results_dir),\n        ]\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # ------------------------------------------------- post-hoc evaluation\n    print(\"\\n===== All runs finished, starting evaluation =====\")\n    eval_cmd = [\n        sys.executable, \"-m\", \"src.evaluate\",\n        \"--results-dir\", str(results_dir),\n    ]\n    tee_subprocess(\n        eval_cmd,\n        results_dir / \"evaluation_stdout.log\",\n        results_dir / \"evaluation_stderr.log\",\n    )\n\n\n################################################################################\n# --------------------------------  CLI  --------------------------------------#\n################################################################################\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run full experimental pipeline.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results are stored.\")\n\n    args_parsed = parser.parse_args()\n    main(args_parsed)", "model_py": "\"\"\"\nmodel.py \u2013 Model architectures \u0026 BYOL utilities common to all experiments.\nThis file contains the COMPLETE implementation of BYOL \u0026 TW-BYOL algorithms\nexcept for dataset-specific modules.\n\"\"\"\nfrom typing import Tuple\nimport copy\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models as tv_models\n\n################################################################################\n# --------------------------   projection heads   -----------------------------#\n################################################################################\n\nclass MLPHead(nn.Module):\n    \"\"\"2-layer MLP projection/prediction head for BYOL.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 4096, out_dim: int = 256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n################################################################################\n# ------------------------------  backbones  ----------------------------------#\n################################################################################\n\n\ndef build_backbone_and_heads(model_cfg: dict):\n    \"\"\"Returns (backbone, projector, predictor) all as nn.Module.\n\n    backbone is WITHOUT final classification layer.\n    \"\"\"\n    model_type = model_cfg[\"type\"].lower()\n    if model_type == \"resnet18\":\n        backbone = tv_models.resnet18(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"resnet50\":\n        backbone = tv_models.resnet50(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"model_placeholder\":  # PLACEHOLDER for specialised video backbone\n        # PLACEHOLDER: insert actual backbone creation logic\n        raise NotImplementedError(\"MODEL_PLACEHOLDER needs to be replaced with real model.\")\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n    # projector \u0026 predictor\n    proj_hidden = model_cfg.get(\"proj_hidden_dim\", 4096)\n    proj_out = model_cfg.get(\"proj_output_dim\", 256)\n    predictor_hidden = model_cfg.get(\"predictor_hidden_dim\", 4096)\n\n    projector = MLPHead(feat_dim, proj_hidden, proj_out)\n    predictor = MLPHead(proj_out, predictor_hidden, proj_out)\n    return backbone, projector, predictor\n\n\n################################################################################\n# -------------------------------  BYOL  --------------------------------------#\n################################################################################\n\nclass BYOL(nn.Module):\n    \"\"\"Minimal BYOL implementation supporting TW-BYOL loss computation.\"\"\"\n\n    def __init__(self, backbone: nn.Module, projector: nn.Module, predictor: nn.Module,\n                 moving_average_decay: float = 0.996):\n        super().__init__()\n        self.online_backbone = backbone\n        self.online_projector = projector\n        self.predictor = predictor\n\n        # create target encoder as EMA copy\n        self.target_backbone = copy.deepcopy(backbone)\n        self.target_projector = copy.deepcopy(projector)\n        for p in self.target_backbone.parameters():\n            p.requires_grad = False\n        for p in self.target_projector.parameters():\n            p.requires_grad = False\n\n        self.moving_average_decay = moving_average_decay\n\n    @torch.no_grad()\n    def update_target_network(self):\n        \"\"\"EMA update of target network.\"\"\"\n        self._update_moving_average(self.online_backbone, self.target_backbone)\n        self._update_moving_average(self.online_projector, self.target_projector)\n\n    @torch.no_grad()\n    def _update_moving_average(self, online: nn.Module, target: nn.Module):\n        for p_o, p_t in zip(online.parameters(), target.parameters()):\n            p_t.data = p_t.data * self.moving_average_decay + p_o.data * (1.0 - self.moving_average_decay)\n\n    def forward(self, view1, view2):\n        # Online network on view1\n        o1 = self.online_backbone(view1)\n        p1 = self.online_projector(o1)\n        p_online = self.predictor(p1)\n\n        # Target network (no grad) on view2\n        with torch.no_grad():\n            t2 = self.target_backbone(view2)\n            z_target = self.target_projector(t2).detach()\n        return p_online, z_target\n\n\n################################################################################\n# ----------------------------  loss functions  -------------------------------#\n################################################################################\n\ndef byol_loss(p_online: torch.Tensor, z_target: torch.Tensor) -\u003e torch.Tensor:\n    return F.mse_loss(p_online, z_target)\n\n\ndef time_weighted_byol_loss(p_online: torch.Tensor, z_target: torch.Tensor,\n                            frame_dist: torch.Tensor, tau: float = 30.0) -\u003e torch.Tensor:\n    \"\"\"Time-weighted BYOL loss as described in the methodology.\"\"\"\n    if frame_dist is None:\n        raise RuntimeError(\"frame_dist tensor is required for TW-BYOL.\")\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)\n    per_sample_loss = (p_online - z_target).pow(2).sum(dim=1)\n    return (weight * per_sample_loss).mean()\n", "preprocess_py": "\"\"\"\npreprocess.py \u2013 Common dataset loading \u0026 preprocessing utilities.\nAll dataset-specific logic is FORBIDDEN in this foundation layer and therefore\nplaced behind explicit placeholders.\n\"\"\"\nfrom typing import Dict\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\n\n# =============================================================================\n# Placeholders that WILL be replaced in later stages\n# =============================================================================\nclass DatasetPlaceholder(Dataset):\n    \"\"\"PLACEHOLDER: Replace with actual dataset implementation.\n\n    The dataset must return a dict with keys:\n        - \u0027view1\u0027: Tensor\n        - \u0027view2\u0027: Tensor\n        - \u0027frame_dist\u0027: Tensor or int (optional, required for TW-BYOL)\n    \"\"\"\n\n    def __init__(self, root: Path, split: str, transform=None, **kwargs):\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.transform = transform\n        self.data = []  # PLACEHOLDER: populate with actual data indices\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # PLACEHOLDER: implement real loading \u0026 augmentation\n        dummy = torch.randn(3, 224, 224)\n        if self.transform:\n            dummy = self.transform(dummy)\n        sample = {\n            \"view1\": dummy,\n            \"view2\": dummy.clone(),\n            \"frame_dist\": torch.tensor(0),\n        }\n        return sample\n\n\n# =============================================================================\n# Public API\n# =============================================================================\n\ndef get_transforms(train: bool = True, cfg: Dict = None):\n    cfg = cfg or {}\n    if train:\n        # basic augmentation pipeline (can be overridden)\n        return T.Compose([\n            T.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n        ])\n    else:\n        return T.Compose([\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n        ])\n\n\ndef get_dataset(cfg: Dict, split: str):\n    \"\"\"Factory that returns correct dataset instance.\n\n    cfg: The \u0027dataset\u0027 section of the run configuration.\n    split: \u0027train\u0027, \u0027val\u0027, or \u0027test\u0027\n    \"\"\"\n    name = cfg[\"name\"].lower()\n    root = Path(cfg.get(\"root\", \"DATASET_ROOT_PLACEHOLDER\"))  # PLACEHOLDER path\n    params = cfg.get(\"params\", {})\n    transform = get_transforms(train=(split == \"train\"), cfg=params.get(\"transforms\"))\n\n    if name == \"dataset_placeholder\":\n        return DatasetPlaceholder(root, split, transform=transform, **params)\n    else:\n        raise ValueError(\n            f\"Dataset \u0027{name}\u0027 not recognised. \"\n            \"# PLACEHOLDER: register dataset in preprocess.get_dataset().\"\n        )", "pyproject_toml": "[project]\nname = \"tw_byol_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common foundation for BYOL / TW-BYOL experimental variations\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntorchvision = \"\u003e=0.15.0\"\npyyaml = \"^6.0\"\ntqdm = \"^4.66.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\nscikit-learn = \"^1.3.0\"\npandas = \"^2.1.0\"\n", "smoke_test_yaml": "# Smoke test configuration \u2013 intentionally small for CI / GitHub Actions\nexperiments:\n  - run_id: smoke_baseline\n    description: |\n      Smoke test baseline BYOL using placeholder dataset \u0026 ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: replace with actual dataset name\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n\n  - run_id: smoke_tw_byol\n    description: |\n      Smoke test TW-BYOL (tau=30) using placeholder dataset \u0026 ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n", "train_py": "\"\"\"\ntrain.py \u2013 Train a single self-supervised run variation (BYOL / TW-BYOL, etc.)\nThe script is launched ONLY by src/main.py. It therefore assumes that all CLI\narguments originate from main.py and are validated there.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport time\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom src import preprocess as pp\nfrom src import model as models\n\n################################################################################\n# ------------------------------   helpers   ----------------------------------#\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -\u003e torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n################################################################################\n# ------------------------------   training   ---------------------------------#\n################################################################################\n\ndef byol_step(batch: Dict[str, torch.Tensor], learner, optimizer, scaler, config):\n    \"\"\"One optimisation step for BYOL/TW-BYOL.\n\n    Args\n    ----\n    batch : Dict \u2013 must have keys \u0027view1\u0027, \u0027view2\u0027, \u0027frame_dist\u0027 (frame_dist optional)\n    learner : models.BYOL \u2013 model wrapper that returns p_online \u0026 z_target\n    optimizer : torch Optimizer\n    scaler : GradScaler or None\n    config : dict \u2013 algorithm section of YAML\n    \"\"\"\n    view1 = batch[\"view1\"].to(get_device(), non_blocking=True)\n    view2 = batch[\"view2\"].to(get_device(), non_blocking=True)\n    frame_dist = batch.get(\"frame_dist\")  # may be None for ordinary BYOL\n    if frame_dist is not None:\n        frame_dist = frame_dist.to(get_device(), non_blocking=True)\n\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.cuda.amp.autocast(enabled=config.get(\"mixed_precision\", True)):\n        p_online, z_target = learner(view1, view2)\n        if config[\"type\"].lower() == \"tw-byol\":\n            tau = config[\"params\"].get(\"tau\", 30.0)\n            loss = models.time_weighted_byol_loss(\n                p_online, z_target, frame_dist=frame_dist, tau=tau\n            )\n        else:  # ordinary BYOL\n            loss = models.byol_loss(p_online, z_target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    learner.update_target_network()\n    return loss.item()\n\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef run_training(cfg: Dict, results_dir: Path):\n    description = cfg.get(\"description\", \"No description provided.\")\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------ paths\n    run_dir = results_dir / run_id\n    images_dir = run_dir / \"images\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------- device\n    device = get_device()\n\n    # --------------------------------------------------------- dataset / dataloader\n    dataset_cfg = cfg[\"dataset\"]\n    train_ds = pp.get_dataset(dataset_cfg, split=\"train\")\n    val_ds = pp.get_dataset(dataset_cfg, split=\"val\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        shuffle=True,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg[\"training\"].get(\"val_batch_size\", cfg[\"training\"][\"batch_size\"]),\n        shuffle=False,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n\n    # ------------------------------------------------------------- model / opt\n    model_cfg = cfg[\"model\"]\n    algorithm_cfg = cfg[\"algorithm\"]\n\n    online_backbone, projector, predictor = models.build_backbone_and_heads(model_cfg)\n    learner = models.BYOL(\n        backbone=online_backbone,\n        projector=projector,\n        predictor=predictor,\n        moving_average_decay=algorithm_cfg.get(\"ema_decay\", 0.996),\n    ).to(device)\n\n    optimizer = optim.Adam(\n        learner.parameters(), lr=cfg[\"training\"][\"learning_rate\"], weight_decay=1e-6\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=algorithm_cfg.get(\"mixed_precision\", True))\n\n    # ------------------------------------------------------------- training loop\n    epochs = cfg[\"training\"][\"epochs\"]\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"time_sec\": [],\n    }\n\n    best_val_loss = float(\"inf\")\n    start_time_total = time.time()\n    for epoch in range(1, epochs + 1):\n        learner.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            loss_val = byol_step(batch, learner, optimizer, scaler, algorithm_cfg)\n            train_losses.append(loss_val)\n            pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n        # ---------------- validation (BYOL self-supervised loss on val set)\n        learner.eval()\n        with torch.no_grad():\n            val_losses = []\n            for batch in val_loader:\n                view1 = batch[\"view1\"].to(device, non_blocking=True)\n                view2 = batch[\"view2\"].to(device, non_blocking=True)\n                frame_dist = batch.get(\"frame_dist\")\n                if frame_dist is not None:\n                    frame_dist = frame_dist.to(device, non_blocking=True)\n\n                with torch.cuda.amp.autocast(enabled=algorithm_cfg.get(\"mixed_precision\", True)):\n                    p_online, z_target = learner(view1, view2)\n                    if algorithm_cfg[\"type\"].lower() == \"tw-byol\":\n                        tau = algorithm_cfg[\"params\"].get(\"tau\", 30.0)\n                        val_loss_val = models.time_weighted_byol_loss(\n                            p_online, z_target, frame_dist=frame_dist, tau=tau\n                        ).item()\n                    else:\n                        val_loss_val = models.byol_loss(p_online, z_target).item()\n                val_losses.append(val_loss_val)\n\n        mean_train_loss = float(np.mean(train_losses))\n        mean_val_loss = float(np.mean(val_losses))\n        epoch_time = time.time() - start_time_total\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(mean_train_loss)\n        history[\"val_loss\"].append(mean_val_loss)\n        history[\"time_sec\"].append(epoch_time)\n\n        # Save best model checkpoint\n        if mean_val_loss \u003c best_val_loss:\n            best_val_loss = mean_val_loss\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save({\"epoch\": epoch, \"state_dict\": learner.state_dict()}, ckpt_path)\n\n        # Epoch-level JSON logging (append-safe)\n        with open(run_dir / \"epoch_metrics.jsonl\", \"a\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps({\n                \"epoch\": epoch,\n                \"train_loss\": mean_train_loss,\n                \"val_loss\": mean_val_loss,\n                \"time_sec\": epoch_time,\n            }) + \"\\n\")\n\n    total_time = time.time() - start_time_total\n\n    # --------------------------------------------------------- save figures\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set(style=\"whitegrid\")\n\n    # Training \u0026 validation loss curve\n    plt.figure(figsize=(8, 4))\n    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss \u2013 {run_id}\")\n    # Annotate final value\n    plt.annotate(f\"{history[\u0027val_loss\u0027][-1]:.4f}\",\n                 xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n                 xytext=(5, -10), textcoords=\u0027offset points\u0027)\n    plt.legend()\n    plt.tight_layout()\n    fig_name = f\"training_loss_{run_id}.pdf\"\n    plt.savefig(images_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------------------------------------------------- final results\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm\": algorithm_cfg[\"type\"],\n        \"dataset\": dataset_cfg[\"name\"],\n        \"model\": model_cfg[\"type\"],\n        \"epochs\": epochs,\n        \"best_val_loss\": best_val_loss,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"total_time_sec\": total_time,\n        \"figure_files\": [fig_name],\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # ----------------------------------------------------- stdout requirements\n    print(\"\\n===== Experiment Description =====\")\n    print(description)\n    print(\"===== Numerical Results (JSON) =====\")\n    print(json.dumps(results))\n\n\n################################################################################\n# ------------------------------   CLI   --------------------------------------#\n################################################################################\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True,\n                        help=\"Path to JSON or YAML file with a SINGLE run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where outputs will be written.\")\n    return parser.parse_args()\n\n\ndef load_run_config(path: str) -\u003e Dict:\n    path = Path(path)\n    if path.suffix in {\".yaml\", \".yml\"}:\n        import yaml\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = yaml.safe_load(fp)\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = json.load(fp)\n    return cfg\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = load_run_config(args.run_config)\n    run_training(cfg, Path(args.results_dir))"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": true,
    "base_code_issue": ""
}
