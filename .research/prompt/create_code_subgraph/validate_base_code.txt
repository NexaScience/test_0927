
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/` directory, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?",
    "Methods": "Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides—per time-step t and per network block k—whether the block is executed.  \n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk·h(t)), where h(t)=1−t (so gates tend to stay open near the data region).  \n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk∈{0,1}) at inference time.  \n3. Loss=Lnoise+λ⋅Σk g̅k, where Lnoise is the standard noise-prediction loss and g̅k is the average gate activation over the batch; λ is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.",
    "Experimental Setup": "Model: the public DDPM CIFAR-10 UNet (32×32).\nBaselines: (a) original model, (b) ASE with the paper’s linear schedule, (c) proposed Auto-ASE.\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; λ=0.05.\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass GatedBlock(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\n    def forward(self, x, t):\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\n        if self.training:\n            return x + gate * (self.block(x, t) - x), gate.mean()\n        else:  # STE binarisation\n            hard_gate = (gate > 0.5).float()\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\n\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\n\ngate_loss_coeff = 0.05\n\nfor x, t, noise in dataloader:\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\n    loss.backward(); optimizer.step()",
    "Expected Result": "Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t≈0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (≈16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.",
    "Expected Conclusion": "By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration."
}

# Experimental Design
## Experiment Strategy
Goal: Build a single, rigorous experimental play-book that can be applied to every study of Auto-ASE, proving that the learnable gating schedule (a) preserves or improves sample quality, (b) reduces sampling cost, (c) is robust across models, datasets and solvers, and (d) removes the need for manual tuning.

1. Key hypotheses to validate
   H1  Performance: Auto-ASE delivers the same or better generative quality than the original network and the hand-crafted ASE schedule.
   H2  Efficiency: Auto-ASE cuts wall–clock sampling time, FLOPs and GPU energy by at least 20 % versus the full model and beats the best fixed schedule.
   H3  Robustness & Generalisation: The learned schedule works (without re-tuning) on multiple datasets, resolutions, network backbones and numerical solvers.
   H4  Simplicity: Training overhead and parameter growth are negligible (<0.5 % extra parameters, <5 % extra training time).

2. Universal comparison set
   a. Baselines
      • Full model (no skipping)
      • Original ASE with its published dropping rule
   b. State-of-the-art accelerators that keep the backbone intact (e.g. DDIM, DPM-Solver++, PNDM) to show trade-off curves
   c. Ablations of Auto-ASE
      • No sparsity loss (λ = 0)
      • Shared vs individual gates
      • Different gate shapes h(t)
      • Soft-gating at inference (no STE)

3. Evaluation axes (applied in every experiment)
   Quantitative quality: FID, KID, IS (for images) or task-specific metrics
   Qualitative quality: curated sample grids + human Turing test where feasible
   Efficiency:  (i) average number of executed blocks, (ii) total UNet/Transformer calls, (iii) wall-clock latency, (iv) TFLOPs, (v) peak & average GPU memory, (vi) Joule/1000 images measured by nvidia-smi
   Robustness: metric distributions across 3 random seeds and across 3 λ values
   Schedule insight: plots of g_k(t) heat-maps & histogram of closed gates

4. Experimental matrix (re-used each time)
   Tier-1  In-domain sanity: original public UNet × CIFAR-10 × DDPM solver (50 steps)
   Tier-2  Cross-architecture: replace UNet by DiT-XL/2 or ADM-KD while keeping CIFAR-10
   Tier-3  Cross-data & resolution: ImageNet-64, LSUN-256, Stable-Diffusion 512×512 latent UNet; keep the same Auto-ASE hyper-parameters
   Tier-4  Cross-solver: DDIM 25 steps, DPM-Solver++ 15 steps, PLMS 50 steps
   Tier-5  Stress tests: (i) halve/ double λ, (ii) prune 70 % of blocks, (iii) deploy on corrupted noise schedules

5. Success criteria (must be met on every Tier-1 & Tier-2 run, and on ≥75 % of Tier-3/4/5 runs)
   • ΔFID ≤ +0.5 (or KID ≤ +0.002) relative to full model
   • ≥20 % speed-up vs full model; ≥5 % extra speed-up vs best fixed ASE schedule
   • <0.5 % parameter growth; <5 % extra training time
   • For robustness tiers: variance of ΔFID across seeds ≤ 0.8 and no catastrophic failure (FID < ×1.5 of baseline)

6. Measurement protocol
   • All runs on a single NVIDIA A100-80GB, PyTorch compiled with CUDA 11.8 + cuDNN 8.
   • Disable CPU-GPU overlaps to obtain deterministic timing; average over 1 k samples after 50 warm-ups.
   • Log with Weights & Biases to expose full metrics, curves and gate heat-maps.

7. Reporting template (identical for all papers/sections)
   Table 1  Quality & efficiency metrics for Baselines vs Auto-ASE
   Figure 1 Trade-off curve: FID vs wall-clock time
   Figure 2 Gate activation heat-map g_k(t)
   Table 2 Ablation results
   Appendix: energy profile & hardware counters

By adhering to this uniform strategy, each experiment will systematically attack the four key hypotheses with controlled baselines, rich metrics and clear success thresholds, allowing reviewers to judge Auto-ASE’s effectiveness from multiple, reproducible perspectives.

# Generated Base Code Files
{"evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -\u003e List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -\u003e pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate \u0026 compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# PLACEHOLDER: This template will be populated in the next step with actual\n# datasets, models and hyper-parameters. The structure MUST remain identical\n# so that src.main can parse it without changes.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER-baseline\n    dataset: DATASET_PLACEHOLDER\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: DATASET_PLACEHOLDER-auto_ase\n    dataset: DATASET_PLACEHOLDER\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # Additional ablations / variants can be appended here following the same key names.\n", "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {\u0027 \u0027.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a \u0027run_id\u0027 field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run \u0027{run_id}\u0027 ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run \u0027{run_id}\u0027 completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain \u0027experiments\u0027 list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "# src/model.py\n\"\"\"Model architecture implementations.\nIncludes baseline UNet-style model plus Auto-ASE variant with learnable gates.\nThe gating logic is FULLY implemented here; swapping datasets or changing the\nunderlying block structure can be done without touching the base logic.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Time embedding helpers (positional)\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -\u003e torch.Tensor:\n    \"\"\"From OpenAI\u0027s ADM code: create sinusoidal embeddings.\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:  # zero pad\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\n# ------------------------------------------------------------------------- #\n# Gating mechanism (Auto-ASE core)\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an arbitrary nn.Module block with a learnable gate following Auto-ASE.\n\n    During training the gate is continuous (sigmoid).  During inference the gate\n    is binarised via the straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int, h_function: str = \"linear\"):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # gate logit parameter\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.h_function = h_function\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        # Compute gate scalar g_k(t) per sample in batch\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)  # (B,1)\n        gate = gate_cont if train else (gate_cont \u003e 0.5).float()  # STE at inference\n\n        # Reshape for broadcasting over feature maps\n        while gate.dim() \u003c x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()  # use continuous gate stat for loss\n\n\n# ------------------------------------------------------------------------- #\n# Simple UNet-like backbone (CIFAR-10 compatible, kept intentionally small)\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.activation = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.activation(self.conv1(x))\n        # Add time embedding\n        temb_broadcast = self.emb_proj(temb)[:, :, None, None]\n        h = h + temb_broadcast\n        h = self.activation(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet with optional gating wrappers based on Auto-ASE.\"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels, gated)\n        self.down2 = self._make_block(base_channels, base_channels * 2, gated)\n        self.pool = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2, gated)\n        # Decoder\n        self.up1 = self._make_block(base_channels * 4, base_channels, gated)\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n\n        # Output layer\n        self.out_conv = nn.Conv2d(base_channels, img_channels, 1)\n\n    def _make_block(self, in_ch: int, out_ch: int, gated: bool):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    # ------------------------------------------------------------------ #\n    # Diffusion-specific helpers                                         #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gated_stats: List[torch.Tensor] = []\n\n        def apply(block, *args):\n            if isinstance(block, GatedBlock):\n                y, g_stat = block(*args, train=train)\n                gated_stats.append(g_stat)\n                return y\n            else:\n                return block(*args)\n\n        # Encoder\n        d1 = apply(self.down1, x, temb)\n        p1 = self.pool(d1)\n        d2 = apply(self.down2, p1, temb)\n        p2 = self.pool(d2)\n\n        # Bottleneck\n        bn = apply(self.bottleneck, p2, temb)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = apply(self.up1, up, temb)\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gated_stats\n\n    # ------------------------ Training interface ---------------------- #\n    def training_step(self, x0: torch.Tensor) -\u003e dict:\n        \"\"\"Implements standard DDPM noise-prediction loss + gate sparsity.\"\"\"\n        device = x0.device\n        batch_size = x0.size(0)\n        config_T = 1000\n        t = torch.randint(0, config_T, (batch_size,), device=device)\n        betas = torch.linspace(1e-4, 0.02, config_T, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": noise_loss.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # --------------------- Simple ancestral sampling ------------------- #\n    def generate(self, num_samples: int, device: torch.device) -\u003e torch.Tensor:\n        \"\"\"Very basic DDPM sampling loop (for evaluation) \u2013 not optimised.\"\"\"\n        self.eval()\n        with torch.no_grad():\n            img_size = 32  # assume square for simplicity \u2013 can be changed later\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_ in reversed(range(T)):\n                t = torch.full((num_samples,), t_, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, train=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_ \u003e 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta) * noise\n            x = torch.clamp(x, -1.0, 1.0)\n            return x.cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Model factory                                                             #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -\u003e nn.Module:\n    model_name = config.get(\"model\")\n    lambda_gates = config.get(\"diffusion\", {}).get(\"lambda_gates\", 0.05)\n    if model_name in {\"dummy_baseline\", \"baseline_unet\"}:\n        return SimpleUNet(gated=False, lambda_gate=0.0)\n    elif model_name in {\"dummy_auto_ase\", \"auto_ase\"}:\n        return SimpleUNet(gated=True, lambda_gate=lambda_gates)\n\n    # ------------------------- PLACEHOLDER -------------------------------- #\n    # Insert additional architectures (DiT, ADM-KD, Stable-Diffusion UNet etc.) here\n\n    raise ValueError(f\"Unknown model name: {model_name}\")\n", "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with dataset placeholders.\nThe logic here is COMPLETE for the built-in \"dummy\" dataset used during smoke\ntests.  For real experiments, simply extend the `get_dataset` function with\nactual dataset-specific loading code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# ------------------------------------------------------------------------- #\n# Config-driven helpers\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms based on config.\n\n    For image datasets we support optional resizing and normalisation.\n    \"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize))\n    tfms.append(transforms.ToTensor())\n\n    # Normalisation (ImageNet stats by default)\n    if config.get(\"data\", {}).get(\"normalize\", True):\n        mean = config.get(\"data\", {}).get(\"mean\", [0.485, 0.456, 0.406])\n        std = config.get(\"data\", {}).get(\"std\", [0.229, 0.224, 0.225])\n        tfms.append(transforms.Normalize(mean, std))\n\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory (with placeholders for extension)                          #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\n\n    Built-in:\n        \u2022 \"dummy\"  \u2013 torchvision.datasets.FakeData with tiny size (used for CI / smoke tests)\n\n    PLACEHOLDER: Extend this function with actual dataset logic, e.g. CIFAR-10,\n    ImageNet-64, LSUN, etc.  Keep the interface unchanged so the rest of the\n    pipeline remains intact.\n    \"\"\"\n\n    if name == \"dummy\":\n        # A tiny fake dataset with 1-channel or 3-channel images depending on config.\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        dataset = datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n        return dataset\n\n    # ------------------------- PLACEHOLDER ------------------------------ #\n    # Insert real dataset paths / download logic here. For example:\n    # if name == \"cifar10\":\n    #     root = Path(config[\"data\"][\"root\"])\n    #     return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    raise NotImplementedError(f\"Dataset \u0027{name}\u0027 is not implemented yet.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -\u003e Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) \u003e 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n", "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \"\u003e=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntorchvision = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# For FID computation\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# This configuration runs two tiny experiments on a dummy dataset to make sure\n# the entire pipeline executes correctly. It is deliberately lightweight so it\n# can be executed in \u003c30 seconds on CPU-only CI.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: dummy\n    model: dummy_baseline\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: dummy_auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n", "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -\u003e None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -\u003e None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -\u003e Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser \u0026 schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add \u0027torchmetrics\u0027 to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches \u003e= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics \u0026 figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss \u2013 {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history[\u0027train_loss\u0027][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -\u003e argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Requirement 1 not fully met: base code does not implement model checkpoint saving/loading (train.py never saves the trained nn.Module and there is no load logic).  Minor related gaps: pyproject.toml is missing the pandas dependency used in src/evaluate.py.  All other criteria are satisfied, but absence of checkpoint I/O violates the Complete Core Logic Implementation requirement."
}
