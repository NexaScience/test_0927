
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/` directory, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "In BOIL the whole learning curve is compressed into a single scalar through a sigmoid‐weighted average of the rewards.  This scalar ignores instability of the curve.  In Deep RL it is common that a hyper-parameter setting reaches a high reward but with large oscillations – such settings are often unreliable at test time.  Because the GP sees no penalty for instability it may keep sampling such noisy settings, wasting evaluations and slowing convergence.",
    "Methods": "Stability–Aware Curve Compression (SACC).\nModification to BOIL: after computing the sigmoid–weighted average m(curve) we subtract a stability penalty proportional to the standard deviation of the last K% of the curve.\n    score = m(curve) – λ · std(curve[ -K: ])\nOnly three extra lines are needed inside apply_one_transform_logistic.  λ ≥ 0 is an additional scalar hyper-parameter that is learned together with the sigmoid midpoint and growth by maximising the GP log-marginal likelihood (just append λ to the vector that is optimised).\nMotivation:   std(curve_tail) is an inexpensive proxy for reliability; subtracting it keeps the objective one-dimensional so BOIL’s GP, acquisition function and data-augmentation remain untouched.",
    "Experimental Setup": "Task: tune learning rate and target-network update period of a DQN agent on CartPole-v0.\nBaselines: (1) Original BOIL, (2) BOIL+SACC (ours).\nBudget: 25 BO iterations, 5 random initial points.\nK: last 10 % of episodes, λ initialised to 1.0 with bounds [0,5].\nMetrics:\n  • Best validation reward after 25 evaluations.\n  • Number of evaluations required to reach an average reward ≥ 195.\n  • Post-training stability: std of reward over 20 extra evaluation episodes.\nSame random seeds are used for both methods.",
    "Experimental Code": "import numpy as np\n\ndef transform_logistic_sacc(curve, midpoint, growth, max_ep, lam=1.0, tail_frac=0.1):\n    # original sigmoid weighting\n    x_scaled = np.linspace(-6, 6, len(curve))\n    weights = 1/(1+np.exp(-growth*(x_scaled-midpoint)))\n    m = np.mean(curve*weights)\n    # stability penalty (std of last K% of curve)\n    k = max(1, int(len(curve)*tail_frac))\n    stability = np.std(curve[-k:])\n    return m - lam*stability\n\n# drop-in replacement inside BOIL\n# in apply_one_transform_logistic simply call transform_logistic_sacc with lam learned from GP\n",
    "Expected Result": "Across 10 independent runs the proposed BOIL+SACC is expected to:\n  • Reach the success threshold (avg reward 195) after ≈12 evaluations versus ≈17 for BOIL.\n  • Achieve ~5-10 % higher best-of-run reward.\n  • Produce policies whose evaluation-phase reward std is roughly 30 % lower than those from vanilla BOIL, showing better stability.",
    "Expected Conclusion": "Penalising late-phase reward variability with one extra term guides BOIL away from hyper-parameters that merely spike in performance, concentrating budget on genuinely robust settings.  The change touches only the curve-compression function, keeps BOIL’s remaining machinery intact, adds one learnable scalar, and empirically yields faster and more reliable hyper-parameter optimisation."
}

# Experimental Design
## Experiment Strategy
Overall Objective
Design a single, modular validation protocol that can be reused on every experiment (Cart-Pole → Atari → MuJoCo, small → large HP spaces, low → high reward noise) to prove that Stability-Aware Curve Compression (SACC) brings (1) faster convergence, (2) higher ultimate performance, (3) greater policy reliability, (4) negligible overhead, and (5) good generalization across tasks and search regimes.

I. Core Hypotheses To Validate
1. Sample-efficiency: BOIL+SACC requires fewer BO evaluations to reach a target performance.
2. Performance ceiling: BOIL+SACC attains a higher best-of-run return than baselines.
3. Stability / robustness:
   a. Training stability – learning curves show less oscillation.
   b. Evaluation stability – final policy reward std is lower.
4. Computational cost: wall-clock and GPU hours are not significantly higher than vanilla BOIL.
5. Generalization: the λ learned by GP adapts automatically to a variety of reward scales and noise levels without retuning.

II. Comparison Matrix (applied in every experiment)
A. Baselines
   • Vanilla BOIL (identical surrogate, no penalty)
   • BOIL with human-set λ (constant, no learning) – ablation
   • Alternative curve compressors (e.g., simple last-N averaging, BOIL-MAX) – sanity check
   • External state-of-the-art HPO: ASHA, TPE – competitive bar
B. Ablations / Sensitivity
   1. Vary tail fraction K and observe effect.
   2. Optimizer without λ in GP vector (λ fixed to 0) – isolates impact of learning λ.
C. Stress Settings
   • High-variance environment (stochastic CartPole, randomized seeds)
   • Large search space (add optimizer momentum, epsilon, etc.)

III. Evaluation Angles & Metrics (recorded for every run)
1. Quantitative
   a. Best validation reward vs #evaluations curve (primary) – Area-Under-Curve
   b. Time-to-threshold (first hit of task-specific success)
   c. Final policy test reward mean ± std over 30 episodes
   d. Std of last K% training rewards (same K for fairness)
   e. CPU/GPU time & memory footprint (profiling hooks)
2. Qualitative
   a. Plot learning curves of representative runs (median, 25/75 percentile shading)
   b. Acquisition trajectories – how λ evolves, sample dispersion
3. Statistical Validation
   • 10 independent seeds per setting
   • Report mean, 95% CI; use paired t-tests or Wilcoxon on matched seeds
   • Success criterion: BOIL+SACC beats every baseline on at least 3/4 primary metrics with p<0.05.

IV. Experimental Procedure (identical template)
Step 1: Fix task-specific success threshold & search space.
Step 2: Generate identical initial random design for all methods.
Step 3: Run BO for B iterations (budget fixed across methods) logging full learning curve at each eval.
Step 4: After BO terminates, retrain best hyper-params for T extra episodes, collect evaluation stats.
Step 5: Aggregate across seeds, compute metrics, statistical tests, produce plots & cost table.

V. Resource & Reproducibility Controls
• All runs limited to 1×A100; concurrency chosen so peak VRAM ≤80 GB and RAM ≤2 TB.
• Deterministic CuDNN + fixed numpy/PyTorch seeds stored.
• Codebase uses the same call-paths; SACC flag toggles extra 3-line penalty.
• Auto-logging: JSON + TensorBoard + csv for downstream analysis scripts.

VI. Success Criteria for the Whole Study
The method will be declared effective if, on a diverse benchmark suite (≥3 tasks, ≥2 noise regimes), it consistently:
1. Reduces evaluations-to-threshold by ≥20 % on average.
2. Improves best-of-run reward by ≥5 % on ≥70 % of tasks.
3. Cuts evaluation-phase reward std by ≥25 %.
4. Adds <2 % overhead in wall-clock time.
5. Shows no catastrophic regressions relative to any baseline.

This unified strategy ensures every forthcoming experiment follows a consistent, statistically sound, and multi-angle protocol, providing compelling evidence for SACC’s benefits while remaining practical within the available computational environment.

# Generated Base Code Files
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from multiple experiment variations, produces comparative\nfigures and prints summary statistics in structured JSON.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate \u0026 compare experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory containing variation sub-directories.\")\n    return parser.parse_args()\n\n\ndef load_results(results_dir: str) -\u003e List[Dict[str, Any]]:\n    runs = []\n    for run_id in sorted(os.listdir(results_dir)):\n        res_file = os.path.join(results_dir, run_id, \"results.json\")\n        if os.path.isfile(res_file):\n            with open(res_file, \"r\") as f:\n                runs.append(json.load(f))\n    return runs\n\n\ndef aggregate_metrics(runs: List[Dict[str, Any]]):\n    summary = {}\n    for run in runs:\n        summary[run[\"run_id\"]] = {\n            \"best_score\": run[\"best_score\"],\n            \"time_to_threshold\": run[\"time_to_threshold\"],\n        }\n    return summary\n\n\ndef plot_best_scores(runs: List[Dict[str, Any]], out_dir: str):\n    os.makedirs(os.path.join(out_dir, \"images\"), exist_ok=True)\n    run_ids = [r[\"run_id\"] for r in runs]\n    best_scores = [r[\"best_score\"] for r in runs]\n    plt.figure(figsize=(6, 4))\n    ax = sns.barplot(x=run_ids, y=best_scores)\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Best Compressed Score\")\n    ax.set_title(\"Best Score Comparison across Variations\")\n    for idx, val in enumerate(best_scores):\n        ax.text(idx, val + 0.01, f\"{val:.2f}\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(out_dir, \"images\", \"best_score_comparison.pdf\"), bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    args = parse_args()\n    runs = load_results(args.results_dir)\n    if not runs:\n        print(\"No result files found \u2013 nothing to evaluate.\")\n        return\n\n    summary = aggregate_metrics(runs)\n    # --------- Figures ---------\n    plot_best_scores(runs, args.results_dir)\n\n    print(json.dumps({\"comparison\": summary}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "experiments:\n  # PLACEHOLDER: Baseline experiment configuration \u2013 replace DATASET_PLACEHOLDER / MODEL_PLACEHOLDER\n  - run_id: BASELINE_PLACEHOLDER\n    description: |\n      PLACEHOLDER description for baseline experiment (vanilla BOIL) \u2013 will be\n      replaced in derived-specific configuration.\n    dataset: DATASET_PLACEHOLDER  # PLACEHOLDER: replace with concrete dataset config\n    model: MODEL_PLACEHOLDER      # PLACEHOLDER: replace with concrete model config\n    algorithm:\n      type: boil\n      use_sacc: false\n      lambda: 0.0\n      random_initial_points: 5\n      total_evaluations: 25\n      tail_frac: 0.1\n      success_threshold: SPECIFIC_CONFIG_PLACEHOLDER\n    search_space: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # PLACEHOLDER: Proposed SACC experiment configuration\n  - run_id: PROPOSED_PLACEHOLDER\n    description: |\n      PLACEHOLDER description for proposed BOIL+SACC experiment \u2013 to be filled\n      with dataset-specific parameters.\n    dataset: DATASET_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    algorithm:\n      type: boil\n      use_sacc: true\n      lambda: 1.0\n      learn_lambda: true\n      random_initial_points: 5\n      total_evaluations: 25\n      tail_frac: 0.1\n      success_threshold: SPECIFIC_CONFIG_PLACEHOLDER\n    search_space: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # Additional ablations / baselines can be appended here following the same schema.\n\n# Note: This is a template \u2013 will be fully populated in the derive-specific step.\n# Only the placeholders in UPPER_SNAKE_CASE should be replaced; the rest of the\n# structure must remain unchanged so that the core logic continues to work.\n\n\n\n", "main_py": "\"\"\"src/main.py\nExperiment orchestrator: reads a config-file listing all variations, launches\ntrain.py sequentially, collects logs, and finally invokes evaluate.py.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport yaml\nfrom pathlib import Path\nimport shutil\nimport time\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Experiment orchestrator.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run variations defined in smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run variations defined in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where all outputs will be saved.\")\n    return parser.parse_args()\n\n\ndef tee_subprocess(cmd, stdout_path: Path, stderr_path: Path):\n    \"\"\"Runs *cmd* and simultaneously writes stdout/stderr to file and console.\"\"\"\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n    with open(stdout_path, \"w\") as f_out, open(stderr_path, \"w\") as f_err:\n        # Non-blocking read loop\n        while True:\n            out_line = proc.stdout.readline()\n            err_line = proc.stderr.readline()\n            if out_line:\n                sys.stdout.write(out_line)\n                f_out.write(out_line)\n            if err_line:\n                sys.stderr.write(err_line)\n                f_err.write(err_line)\n            if not out_line and not err_line and proc.poll() is not None:\n                break\n    return proc.returncode\n\n\ndef main():\n    args = parse_args()\n    root = Path(__file__).resolve().parent.parent  # project root\n    config_path = (\n        root / \"config\" / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    )\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    experiments = config[\"experiments\"]\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Copy config for provenance\n    shutil.copy(config_path, results_dir / config_path.name)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_cfg_path = results_dir / f\"{run_id}_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        stdout_path = results_dir / run_id / \"stdout.log\"\n        stderr_path = results_dir / run_id / \"stderr.log\"\n        stdout_path.parent.mkdir(parents=True, exist_ok=True)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"===== Launching {run_id} =====\")\n        sys.stdout.flush()\n        rc = tee_subprocess(cmd, stdout_path, stderr_path)\n        if rc != 0:\n            print(f\"Experiment {run_id} failed with return-code {rc}\")\n            sys.exit(rc)\n        print(f\"===== Completed {run_id} =====\\n\")\n\n    # -------------------- Post-hoc evaluation -----------------------\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ]\n    subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel definitions and curve-compression utilities.\n\"\"\"\nfrom __future__ import annotations\nimport torch\nfrom torch import nn\nfrom typing import Dict, Any, List\nimport numpy as np\n\n# ================================================================\n# Simple classifier used for dummy smoke tests\n# ================================================================\nclass BaseClassifier(nn.Module):\n    def __init__(self, input_dim: int = 10, hidden_dim: int = 64, num_classes: int = 2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ================================================================\n# Model factory with placeholders for future extensions\n# ================================================================\n\ndef get_model(model_cfg: Dict[str, Any]) -\u003e nn.Module:\n    name = model_cfg.get(\"name\", \"dummy_classifier\")\n    if name == \"dummy_classifier\":\n        return BaseClassifier(\n            input_dim=int(model_cfg.get(\"input_dim\", 10)),\n            hidden_dim=int(model_cfg.get(\"hidden_dim\", 64)),\n            num_classes=int(model_cfg.get(\"num_classes\", 2)),\n        )\n    else:\n        # PLACEHOLDER: Will be replaced with specific model construction logic\n        raise NotImplementedError(f\"Model \u0027{name}\u0027 not implemented in common foundation.\")\n\n\n# ================================================================\n# Curve compression utilities (BOIL + optional SACC)\n# ================================================================\n\ndef sigmoid_weighted_average(curve: List[float], midpoint: float = 0.0, growth: float = 1.0) -\u003e float:\n    \"\"\"Applies sigmoid weighting across the curve and returns weighted mean.\"\"\"\n    n = len(curve)\n    x_scaled = np.linspace(-6, 6, n)\n    weights = 1.0 / (1.0 + np.exp(-growth * (x_scaled - midpoint)))\n    weights = weights / weights.sum()  # normalise so \u2211w =1\n    return float(np.sum(np.array(curve) * weights))\n\n\ndef sacc_compressed_score(\n    curve: List[float],\n    midpoint: float,\n    growth: float,\n    lam: float = 1.0,\n    tail_frac: float = 0.1,\n) -\u003e float:\n    base = sigmoid_weighted_average(curve, midpoint, growth)\n    k = max(1, int(len(curve) * tail_frac))\n    stability_penalty = np.std(curve[-k:])\n    return float(base - lam * stability_penalty)\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data loading / preprocessing utilities with dataset placeholders.\n\"\"\"\nfrom __future__ import annotations\nimport random\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom typing import Tuple, Dict, Any\n\n# ================================================================\n# Seed control \u2013 deterministic behaviour across experiments\n# ================================================================\n\ndef set_global_seeds(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# ================================================================\n# Dataset loading \u2013 with placeholders for future replacements\n# ================================================================\n\ndef _build_dummy_dataset(n_samples: int = 512, input_dim: int = 10, n_classes: int = 2):\n    x = torch.randn(n_samples, input_dim)\n    y = torch.randint(0, n_classes, (n_samples,))\n    return TensorDataset(x, y)\n\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any]) -\u003e Tuple[DataLoader, DataLoader]:\n    \"\"\"Returns train \u0026 validation dataloaders.\n\n    PLACEHOLDER: Will be replaced with task-specific dataset logic in later steps.\n    \"\"\"\n    name = dataset_cfg.get(\"name\", \"dummy\")\n    batch_size = int(dataset_cfg.get(\"batch_size\", 32))\n\n    if name == \"dummy\":\n        ds = _build_dummy_dataset()\n        train_size = int(0.8 * len(ds))\n        val_size = len(ds) - train_size\n        train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n        return train_loader, val_loader\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(f\"Dataset \u0027{name}\u0027 not yet implemented in common foundation.\")\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"boil-sacc-framework\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for BOIL+SACC experiments\"\nrequires-python = \"\u003e=3.10\"\nauthors = [\n    { name = \"Research Automation\", email = \"research@example.com\" }\n]\nlicense = { text = \"Apache-2.0\" }\n\n[project.dependencies]\ntorch = \"*\"\nnumpy = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npyyaml = \"*\"\ntqdm = \"*\"\nscikit-learn = \"*\"\nscipy = \"*\"\n# gpytorch \u0026 botorch are included for future, heavier-duty experiments\nbotorch = \"*\"\ngpytorch = \"*\"\n", "smoke_test_yaml": "experiments:\n  # Lightweight baseline without stability penalty\n  - run_id: baseline_dummy\n    description: |\n      Smoke-test baseline: vanilla BOIL logistic compression (\u03bb=0) on dummy dataset.\n    seed: 123\n    dataset:\n      name: dummy\n      batch_size: 32\n    model:\n      name: dummy_classifier\n      input_dim: 10\n      num_classes: 2\n    algorithm:\n      type: boil\n      use_sacc: false\n      lambda: 0.0\n      random_initial_points: 1\n      total_evaluations: 3\n      tail_frac: 0.1\n      success_threshold: 0.8\n    search_space:\n      learning_rate: {min: 1e-4, max: 5e-3}\n\n  # Smoke-test proposed SACC variant (\u03bb learned)\n  - run_id: sacc_dummy\n    description: |\n      Smoke-test SACC: BOIL + Stability-Aware Curve Compression on dummy dataset.\n    seed: 456\n    dataset:\n      name: dummy\n      batch_size: 32\n    model:\n      name: dummy_classifier\n      input_dim: 10\n      num_classes: 2\n    algorithm:\n      type: boil\n      use_sacc: true\n      lambda: 1.0  # initial value\n      learn_lambda: true\n      random_initial_points: 1\n      total_evaluations: 3\n      tail_frac: 0.1\n      success_threshold: 0.8\n    search_space:\n      learning_rate: {min: 1e-4, max: 5e-3}\n", "train_py": "\"\"\"src/train.py\nRuns a single experiment variation defined by a YAML config file.  Implements\nBOIL with optional SACC curve\u2013compression, saves metrics and figures.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nimport random\nimport yaml\nimport math\nfrom typing import Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom scipy.stats import norm  # For EI\nfrom scipy.optimize import minimize\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nfrom src.preprocess import get_dataloaders, set_global_seeds\nfrom src.model import (\n    get_model,\n    sigmoid_weighted_average,\n    sacc_compressed_score,\n)\nimport matplotlib\nmatplotlib.use(\"Agg\")  # Headless\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root results directory provided by orchestrator.\")\n    parser.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this variation (matches config entry).\")\n    return parser.parse_args()\n\n\ndef expected_improvement(\n    X_candidates: np.ndarray,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    model: GaussianProcessRegressor,\n    xi: float = 0.01,\n):\n    \"\"\"Computes EI for a set of candidate hyper-parameters.\"\"\"\n    mu, sigma = model.predict(X_candidates, return_std=True)\n    mu = mu.ravel()\n    sigma = sigma.ravel()\n    y_best = y_train.max()\n    with np.errstate(divide=\"warn\"):\n        imp = mu - y_best - xi\n        Z = np.zeros_like(mu)\n        mask = sigma \u003e 0\n        Z[mask] = imp[mask] / sigma[mask]\n        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n        ei[sigma == 0.0] = 0.0\n    return ei\n\n\ndef transform_curve(\n    curve: List[float],\n    use_sacc: bool,\n    midpoint: float,\n    growth: float,\n    lam: float,\n    tail_frac: float,\n):\n    if use_sacc:\n        return sacc_compressed_score(curve, midpoint, growth, lam, tail_frac)\n    else:\n        return sigmoid_weighted_average(curve, midpoint, growth)\n\n\ndef optimise_transform_hyperparams(\n    curves: List[List[float]],\n    X_params: np.ndarray,\n    use_sacc: bool,\n    tail_frac: float,\n    initial: np.ndarray,\n    bounds: List[tuple],\n):\n    \"\"\"Learns midpoint, growth (and \u03bb if SACC) by maximising GP log-marginal likelihood.\"\"\"\n\n    def objective(params):\n        midpoint, growth, lam = params\n        y = np.array([\n            transform_curve(c, use_sacc, midpoint, growth, lam, tail_frac) for c in curves\n        ])\n        # Fit GP (tiny kernel to keep this fast)\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(X_params.shape[1]), length_scale_bounds=(1e-2, 1e3))\n        gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=2)\n        gp.fit(X_params, y)\n        lml = gp.log_marginal_likelihood_value_\n        return -lml  # Minimise negative log-likelihood\n\n    res = minimize(objective, initial, bounds=bounds, method=\"L-BFGS-B\")\n    return res.x  # best parameters\n\n\ndef train_single_model(\n    hparams: Dict[str, Any],\n    data_cfg: Dict[str, Any],\n    model_cfg: Dict[str, Any],\n    training_cfg: Dict[str, Any],\n    device: torch.device,\n):\n    \"\"\"Given a hyper-parameter dict, trains the model and returns validation curve.\"\"\"\n    # Build data\n    train_loader, val_loader = get_dataloaders(data_cfg)\n\n    # Build model\n    model_cfg = model_cfg.copy()\n    model_cfg.update(hparams)  # allow structural HPs (e.g., hidden size) to be tuned\n    model = get_model(model_cfg).to(device)\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"])\n\n    epochs = training_cfg.get(\"epochs\", 3)\n    val_metric_curve = []\n    model.train()\n    for epoch in range(epochs):\n        for (x, y) in train_loader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n        # ---- validation ----\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for (xv, yv) in val_loader:\n                xv, yv = xv.to(device), yv.to(device)\n                pred = model(xv).argmax(dim=1)\n                correct += (pred == yv).sum().item()\n                total += yv.size(0)\n        acc = correct / total if total else 0.0\n        val_metric_curve.append(acc)\n        model.train()\n    return val_metric_curve\n\n\ndef main():\n    args = parse_args()\n    # ------------------------------------------------------------------\n    with open(args.config, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    run_id = args.run_id\n\n    # --------------------- Prepare result directories -----------------\n    run_dir = os.path.join(args.results_dir, run_id)\n    images_dir = os.path.join(run_dir, \"images\")\n    os.makedirs(images_dir, exist_ok=True)\n\n    # ----------------------- Reproducibility --------------------------\n    seed = cfg.get(\"seed\", 42)\n    set_global_seeds(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------- Standard output description -----------------\n    description = cfg.get(\"description\", \"No description provided.\")\n    print(\"===== Experiment Description =====\")\n    print(description)\n    print(\"==================================\")\n    sys.stdout.flush()\n\n    # ------------------ Algorithmic configuration --------------------\n    algo_cfg = cfg[\"algorithm\"]\n    total_evals = int(algo_cfg.get(\"total_evaluations\", 25))\n    random_init = int(algo_cfg.get(\"random_initial_points\", 5))\n    tail_frac = float(algo_cfg.get(\"tail_frac\", 0.1))\n    use_sacc = bool(algo_cfg.get(\"use_sacc\", False))\n    learn_lambda = bool(algo_cfg.get(\"learn_lambda\", False))\n\n    # Search space definition (simple flat numeric ranges)\n    search_space = cfg[\"search_space\"]  # dict name -\u003e {\"min\":float, \"max\":float}\n    param_names = list(search_space.keys())\n    dim = len(param_names)\n\n    def sample_random(n: int = 1):\n        out = []\n        for _ in range(n):\n            cand = [\n                random.uniform(search_space[p][\"min\"], search_space[p][\"max\"]) for p in param_names\n            ]\n            out.append(cand)\n        return np.array(out)\n\n    # Containers\n    X_evaluated: List[List[float]] = []\n    curves: List[List[float]] = []\n    y_scores: List[float] = []\n    all_evals: List[Dict[str, Any]] = []\n\n    # Initial transform hyper-parameters\n    midpoint, growth, lam = 0.0, 1.0, float(algo_cfg.get(\"lambda\", 0.0))\n    # Bounds for optimiser\n    transform_bounds = [(-6, 6), (1e-2, 6), (0.0, 5.0)]  # midpoint, growth, \u03bb\n\n    # Success threshold for time-to-threshold metric\n    success_threshold = algo_cfg.get(\"success_threshold\", None)\n    time_to_threshold = None\n\n    # ----------------------- BO main loop ----------------------------\n    for eval_idx in range(total_evals):\n        start_time = time.time()\n        if eval_idx \u003c random_init or len(y_scores) \u003c 2:\n            x_next = sample_random(1)[0]\n        else:\n            # Fit GP to existing data\n            X_np = np.array(X_evaluated)\n            y_np = np.array(y_scores)\n            kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(dim), length_scale_bounds=(1e-3, 1e3))\n            gp = GaussianProcessRegressor(\n                kernel=kernel,\n                alpha=1e-6,\n                normalize_y=True,\n                n_restarts_optimizer=5,\n            )\n            gp.fit(X_np, y_np)\n\n            # Optimise transform hyper-params if required\n            if learn_lambda and len(curves) \u003e= 2:\n                midpoint, growth, lam = optimise_transform_hyperparams(\n                    curves,\n                    X_np,\n                    use_sacc,\n                    tail_frac,\n                    np.array([midpoint, growth, lam]),\n                    transform_bounds,\n                )\n\n            # Acquisition \u2013 Expected Improvement on 1,000 random samples\n            X_cand = sample_random(1000)\n            ei = expected_improvement(X_cand, X_np, y_np, gp)\n            best_idx = int(np.argmax(ei))\n            x_next = X_cand[best_idx]\n\n        # Build hyper-param dict for training call\n        hparams = {param_names[i]: float(x_next[i]) for i in range(dim)}\n        # Mandatory learning rate param for our dummy trainer\n        if \"learning_rate\" not in hparams:\n            hparams[\"learning_rate\"] = 1e-3\n\n        # ---- Run a training instance \u0026 get learning curve ----\n        curve = train_single_model(\n            hparams,\n            data_cfg=cfg[\"dataset\"],\n            model_cfg=cfg[\"model\"],\n            training_cfg=cfg.get(\"training\", {}),\n            device=device,\n        )\n\n        score = transform_curve(curve, use_sacc, midpoint, growth, lam, tail_frac)\n\n        # ------------------------ bookkeeping ------------------------\n        X_evaluated.append(list(x_next))\n        curves.append(curve)\n        y_scores.append(score)\n\n        if success_threshold is not None and score \u003e= success_threshold and time_to_threshold is None:\n            time_to_threshold = eval_idx + 1\n\n        all_evals.append(\n            {\n                \"index\": eval_idx,\n                \"hyperparameters\": hparams,\n                \"curve\": curve,\n                \"compressed_score\": score,\n                \"duration_sec\": time.time() - start_time,\n            }\n        )\n\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"eval_index\": eval_idx,\n                    \"score\": score,\n                    \"midpoint\": midpoint,\n                    \"growth\": growth,\n                    \"lambda\": lam,\n                }\n            )\n        )\n        sys.stdout.flush()\n\n    # ---------------------- Final reporting -------------------------\n    best_idx = int(np.argmax(y_scores))\n    best_score = float(y_scores[best_idx])\n    best_hparams = all_evals[best_idx][\"hyperparameters\"]\n\n    # Save results.json\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm_cfg\": algo_cfg,\n        \"search_space\": search_space,\n        \"transform_params\": {\n            \"midpoint\": midpoint,\n            \"growth\": growth,\n            \"lambda\": lam,\n        },\n        \"evaluations\": all_evals,\n        \"best_index\": best_idx,\n        \"best_score\": best_score,\n        \"best_hyperparameters\": best_hparams,\n        \"time_to_threshold\": time_to_threshold,\n    }\n    with open(os.path.join(run_dir, \"results.json\"), \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # ---------------------------- Figures ---------------------------\n    # 1. Score vs evaluation index\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=list(range(len(y_scores))), y=y_scores, marker=\"o\")\n    plt.title(f\"Compressed Score Progression \u2013 {run_id}\")\n    plt.xlabel(\"Evaluation #\")\n    plt.ylabel(\"Compressed Score\")\n    # Annotate best\n    plt.annotate(f\"best={best_score:.3f}\",(best_idx, best_score),textcoords=\"data\", xytext=(5,5),\n                 textcoords_offset=\u0027offset points\u0027, arrowprops=dict(arrowstyle=\"-\u003e\"))\n    plt.tight_layout()\n    fig_path = os.path.join(images_dir, f\"score_progression_{run_id}.pdf\")\n    plt.savefig(fig_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # 2. Learning curve of best model\n    best_curve = curves[best_idx]\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=list(range(len(best_curve))), y=best_curve, marker=\"o\")\n    plt.title(f\"Validation Metric per Epoch \u2013 best run ({run_id})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(images_dir, f\"learning_curve_{run_id}.pdf\"), bbox_inches=\"tight\")\n    plt.close()\n\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"best_score\": best_score}))\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Core requirement 1 is not fully satisfied: (a) train.py never saves model checkpoints with torch.save nor does it expose any loading utility, so model persistence is missing; (b) result-figure code in train.py calls plt.annotate with an invalid keyword argument 'textcoords_offset', which will raise a TypeError at run time, breaking figure generation. Because complete core logic and publication-ready figure generation are required, the current foundation is incomplete."
}
