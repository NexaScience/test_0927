
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['SD2.1-FD-5', 'SD2.1-FCR(λ=0.10)-FD-10', 'SD2.1-FCR(λ=0.10)+DPMSolver-FD-10', 'OpenJourney-FD-5', 'OpenJourney-FCR(λ=0.10)-FD-10']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Encoder-caching methods (e.g. Faster Diffusion) still lose image quality when the stride between “key” time-steps becomes large (≤10–12 steps are usually safe, larger strides cause blur / artifacts). The root cause is that encoder features, although relatively stable, are not perfectly time invariant. How can we make the encoder features even more consistent so that we can safely reuse them for many more skipped steps and obtain larger speed-ups?",
    "Methods": "Feature Consistency Regularization (FCR)\n1. During (re)training or a short fine-tuning session, add an auxiliary loss that explicitly encourages the encoder feature map to be invariant over time:\n   L_FCR = λ · E_{t,Δ} [ ‖ Enc(x_t) − Enc(x_{t−Δ}) ‖^2 ]\n   where t ~ Uniform(Δ , T), Δ ~ Uniform(1 , Δ_max).\n2. The total loss becomes  L_total = L_denoise + L_FCR.\n3. No network architecture change is required; only an extra forward pass of the encoder on a second, more-noisy latent x_{t−Δ} drawn within the same mini-batch.\n\nTheoretical motivation: If the encoder learns to output nearly identical representations for nearby (or even moderately distant) noise levels, then at inference we can safely reuse a cached encoder feature over more steps (larger stride) without harming the decoder’s conditioning quality. This directly addresses the stride-vs-quality trade-off with a single, simple regularizer.",
    "Experimental Setup": "Model: the open-source Stable Diffusion v1.5 UNet.\nData: 50 k randomly selected LAION-Aesthetics captions & images (10 k training steps are sufficient for a proof-of-concept).\nBaselines:\n  a) Original model + Faster Diffusion sampling with stride 5 (FD-5).\n  b) Original model + Faster Diffusion sampling with stride 10 (FD-10).\nProposed:\n  c) Model fine-tuned with FCR (λ=0.1, Δ_max=10) + Faster Diffusion sampling with stride 10 (FCR-FD-10).\nMetrics: FID (↓) on 30 k MS-COCO validation prompts, CLIPScore (↑), and wall-clock sampling time per image on one RTX-4090 GPU.\nExpectation: (c) keeps FID & CLIPScore close to (a) while matching the speed of (b).",
    "Experimental Code": "# core training snippet (PyTorch)\nimport torch, torch.nn.functional as F\n\nlambda_fcr = 0.1\nD = unet                      # loaded Stable-Diffusion UNet\nenc = lambda feats: feats[\"encoder_hidden_states\"]  # assumes encoder features are returned in dict\n\nfor batch in dataloader:\n    imgs, text = batch\n    z0 = autoencoder.encode(imgs)            # latent\n    t = torch.randint(1, T, (len(z0),), device=z0.device)\n    eps = torch.randn_like(z0)\n    zt = q_sample(z0, t, eps)                # standard diffusion marche\n\n    # main denoising loss\n    out, feats_t = D(zt, t, return_dict=True)\n    loss_main = F.mse_loss(out, eps)\n\n    # second time-step for FCR\n    delta = torch.randint(1, 11, (len(z0),), device=z0.device)\n    t2 = torch.clamp(t - delta, min=1)\n    zt2 = q_sample(z0, t2, eps)\n    _, feats_t2 = D(zt2, t2, return_dict=True, encoder_only=True)  # small extra cost\n\n    loss_fcr = (enc(feats_t) - enc(feats_t2)).pow(2).mean()\n\n    loss = loss_main + lambda_fcr * loss_fcr\n    loss.backward()\n    optimizer.step(); optimizer.zero_grad()",
    "Expected Result": "• FD-5 (baseline):   FID ≈ 6.3,   time ≈ 0.28 s/img.\n• FD-10 (baseline):  FID ≈ 7.9,   time ≈ 0.18 s/img.  (quality drops)\n• FCR-FD-10:         FID ≈ 6.5,   time ≈ 0.18 s/img.\n\nThus the proposed method recovers almost all lost quality while preserving the larger speed-up from the doubled stride.",
    "Expected Conclusion": "A single, easily-implemented regularization term that aligns encoder features across time makes cached-encoder sampling robust to much larger strides. This yields substantial extra acceleration (≈1.5× over already-accelerated FD-5) at virtually no inference-time cost and only a brief fine-tuning cost. The idea is generic and can be plugged into any UNet-based diffusion model or used jointly with other acceleration or distillation techniques."
}

# Experimental Design
- Strategy: Global Experimental Strategy for Validating Feature Consistency Regularization (FCR)

1. Objectives to Validate
   1.1 Quality-vs-Speed Trade-off: Does FCR maintain image quality while enabling larger sampling strides?  
   1.2 Computational Efficiency: How much wall-clock speed-up, FLOP reduction and GPU-memory saving are achieved at inference?  
   1.3 Robustness: Does quality remain stable across random seeds, prompt difficulty, higher resolutions and perturbed noise schedules?  
   1.4 Generalization: Does the benefit transfer to (i) different diffusion backbones, (ii) different domains (text-to-image, in-painting, depth-to-image) and (iii) different datasets?  
   1.5 Mechanistic Validation: Does FCR actually reduce temporal variance in encoder features and is this reduction predictive of downstream quality?  
   1.6 Cost of Adoption: What is the additional training cost and is the inference overhead negligible?

2. Comparison Grid Used in All Experiments
   Baselines (held fixed across studies):  
   • Original model with conservative stride (FD-5) → «quality upper-bound».  
   • Original model with large stride (FD-10/15/20) → «speed upper-bound».  
   • State-of-the-art accelerators (e.g. DPMSolver++, Latent-Distillation) at matched step counts.  
   Ablations:  
   • FCR λ ∈ {0, 0.05, 0.1, 0.2}.  
   • Δ_max ∈ {5, 10, 20}.  
   • FCR applied only to early / mid / late UNet blocks.  
   • One-pass vs two-pass encoder caching.  
   • Stride sweep: 5, 10, 15, 20.  
   Combined Methods: FCR + DPMSolver, FCR + Distillation.

3. Experimental Angles (each experiment will pick a subset, but the philosophy stays the same)
   A. Quantitative Quality: FID, CLIPScore, Inception Score, and prompt-conditional precision / recall over ≥30k COCO prompts with 3 seeds.  
   B. Human Preference: 1k A/B pairs rated on MTurk; significance @95 % CI.  
   C. Computational Cost: (i) Wall-clock time per image on one A100, (ii) #UNet forward passes, (iii) peak GPU RAM, (iv) added fine-tuning time.  
   D. Feature Analysis: Variance(Enc(x_t)–Enc(x_{t-Δ})) plotted against Δ for all compared models; Pearson correlation of this variance with FID drop.  
   E. Stress & Robustness: (i) 1024² images, (ii) out-of-distribution prompts (abstract, medical), (iii) noisy schedules with ε-perturbations, (iv) adversarial latent noise.  
   F. Cross-Model Transfer: repeat core benchmarks on SD-v2.1, OpenJourney, and a latent video UNet.

4. Multi-Perspective Demonstration Plan
   Phase-0: Reproduce paper numbers on SD-v1.5 (sanity check).  
   Phase-1 (Controlled): Full ablation grid on SD-v1.5 @512²; collect all metrics A–D.  
   Phase-2 (Generalization): Repeat best λ, Δ_max settings on two new backbones & two new datasets (ImageNet, Flickr30k); collect metrics A, C, D.  
   Phase-3 (Combination): Integrate FCR with DPMSolver++ and latent-distillation; benchmark quality & cost (metrics A, C).  
   Phase-4 (Robustness): Run stress tests (metrics A, B, C).  
   Phase-5 (Human Study): Crowd-source preference on a mixture of output from Phases 1-4.

5. Success Criteria (must satisfy all to claim win)
   • Quality Preservation: For stride 10, ΔFID ≤ +5 % over FD-5 AND ΔCLIPScore ≥ –2 %.  
   • Speed-up: ≥1.4 × real-time speed-up vs FD-5 at equal quality, with ≤2 % extra VRAM.  
   • Robustness: Quality drop under stress tests ≤ original model’s drop.  
   • Generalization: All three backbones pass Quality & Speed criteria.  
   • Mechanistic Proof: Encoder-feature variance reduced by ≥30 %, with R² ≥0.6 when regressing variance against FID difference.  
   • Training Overhead: Extra fine-tune compute ≤3 % of full model pre-training compute.

6. Practical Considerations
   • Hardware: every timing number reported on a single NVIDIA A100-80 GB; batch size 1 & 8.  
   • Reproducibility: 3 random seeds per setting; publish code, hyper-params, and evaluation scripts.  
   • Statistical Tests: Paired t-test or bootstrap; report 95 % CI for all key metrics.  
   • Stopping Rule: If any candidate fails Quality Preservation, it is excluded from later phases.

This unified strategy ensures that every subsequent experiment, regardless of model or dataset, is anchored to the same validation axes, comparison baselines, measurement tools, and success thresholds, enabling a coherent narrative of FCR’s effectiveness from empirical performance down to mechanistic evidence.

# Current Experiment (to generate code for)
- Experiment ID: exp-2-generalisation-robustness
- Description: Objective / hypothesis: Test whether the benefits of FCR transfer to newer backbones and remain stable under resolution up-scaling, schedule perturbations, and adversarial latent noise. Also quantify the incremental gain when combined with DPMSolver++.  

Models:  
• Stable-Diffusion-v2.1 UNet (SD2.1).  
• OpenJourney checkpoint (public SD-v1 derivative heavily fine-tuned on artistic data).  
• DPMSolver++ sampler (20 steps) in one variation.  

Datasets:  
• Fine-tune: same 50 k LAION-Aesthetics for SD2.1; 40 k ArtBench-10 for OpenJourney to avoid forgetting (class-balanced).  
• Evaluation:  
  – Text-to-image: 30 k Flickr30k captions (diverse, human-written).  
  – Class-conditional: 50 k ImageNet-1k val labels converted to prompts ("a photo of a {class}").  

Pre-processing: identical resizing; additionally generate a 1024² set by super-res rescaling prompts.  

Splits / seeds: 3 random seeds; early-stop on moving FID same as exp-1.  

Robustness protocol:  
• Resolution: run each variation at 512² and 1024².  
• Noise schedule perturbation: add ε∼N(0,0.05) to each σ_t during sampling.  
• Adversarial noise: run Fast-Gradient-Sign on latent z0 (ε=0.1) for 10 steps; measure FID.  

Evaluation metrics:  
Primary – FID, CLIPScore.  
Secondary – image-pair human preference (A/B on 1 k pairs using Crowd-MTurk), feature variance σ²_Δ, RMSE of CLIP embedding drift, speed/VRAM/FLOPs.  
Pairwise statistical test: two-sided paired t-test (α=0.05).  

Hyper-parameter sensitivity: only λ swept in exp-1; here fixed λ=0.10 but we sweep stride {5,10,15} in a micro-study on SD2.1-FCR to confirm stability (reported in appendix).  

Mechanistic check: Pearson R between σ²_Δ and FID across all robustness scenarios; expect R²≥0.6.  

Computation tracking: We log `torch.cuda.memory_summary()`, FLOPs via fvcore, wall-time with NVTX ranges; exported as CSV per run.  

Example robustness code (noise schedule perturbation):
```python
def noisy_sampler(pipe,prompt,steps,eps_std=0.05):
    latents=pipe.prepare_latents(prompt)
    for i,t in enumerate(timesteps):
        sigma = scheduler.sigmas[i]*(1+torch.randn(1,device='cuda')*eps_std)
        latents = pipe.unet(latents, sigma)  # simplified
    return pipe.decode(latents)
```

Resource budget: ~60 GPU-hours per variation (fine-tune + multi-scenario eval) ⇒ 300 GPU-hours, still below 2 days on one A100.  

Success criteria for exp-2: (i) Every FCR variation matches FD-5 quality within +5 % FID at both resolutions while giving ≥1.4 × speed-up; (ii) quality drop under adversarial noise ≤ drop of corresponding baseline; (iii) positive transfer confirmed on both backbones.
- Run Variations: ['SD2.1-FD-5', 'SD2.1-FCR(λ=0.10)-FD-10', 'SD2.1-FCR(λ=0.10)+DPMSolver-FD-10', 'OpenJourney-FD-5', 'OpenJourney-FCR(λ=0.10)-FD-10']

# Base Code
{'train_py': '"""src/train.py\nRuns a single experiment variation specified by --run-id in a YAML config.\nAll experiment artefacts are written to <results-dir>/<run-id>/.\nThe script prints a single JSON line with the final metrics to stdout so that\nmain.py can parse it. Stdout/stderr are also captured by main.py and mirrored\ninto log files.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport yaml\n\nfrom . import preprocess as pp\nfrom . import model as mdl\n\n################################################################################\n# Helper functions                                                               \n################################################################################\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef setup_device() -> torch.device:\n    return torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n################################################################################\n# Core training / validation utilities                                          \n################################################################################\n\ndef _train_one_epoch(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    max_norm: float | None = None,\n) -> float:\n    model.train()\n    running_loss: List[float] = []\n    pbar = tqdm(dataloader, desc=f"Train E{epoch}", leave=False)\n    for batch in pbar:\n        optimizer.zero_grad(set_to_none=True)\n        batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n        out = model.training_step(batch_on_device)\n        loss: torch.Tensor = out["loss"]\n        loss.backward()\n        if max_norm is not None:\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n        optimizer.step()\n        running_loss.append(loss.item())\n        pbar.set_postfix(loss=f"{loss.item():.4f}")\n    return float(sum(running_loss) / len(running_loss))\n\n\ndef _validate(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    device: torch.device,\n    epoch: int,\n) -> float:\n    model.eval()\n    val_losses: List[float] = []\n    pbar = tqdm(dataloader, desc=f"Val   E{epoch}", leave=False)\n    with torch.no_grad():\n        for batch in pbar:\n            batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n            out = model.validation_step(batch_on_device)\n            loss: torch.Tensor = out["val_loss"]\n            val_losses.append(loss.item())\n            pbar.set_postfix(val_loss=f"{loss.item():.4f}")\n    return float(sum(val_losses) / len(val_losses))\n\n################################################################################\n# Inference‐time utilities                                                      \n################################################################################\n\ndef _measure_inference_time(model: mdl.BaseExperimentModel, prompts: List[str], device: torch.device) -> float:\n    """Measures the average wall‐clock inference latency per sample.\n    For diffusion models this calls the generate method; for toy models this\n    performs a single forward pass.\n    """\n    model.eval()\n    with torch.no_grad():\n        start = time.time()\n        _ = model.generate(prompts) if hasattr(model, "generate") else model(torch.randn(1, 3, 32, 32, device=device))\n        torch.cuda.synchronize() if device.type == "cuda" else None\n        end = time.time()\n    return float(end - start)\n\n################################################################################\n# Main entry point                                                              \n################################################################################\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description="Run a single experiment variation")\n    parser.add_argument("--run-id", required=True, help="Name of the run variation as defined in the YAML config")\n    parser.add_argument("--config-file", required=True, help="Path to YAML experiment config")\n    parser.add_argument("--results-dir", required=True, help="Directory where results will be stored")\n    parser.add_argument("--seed", type=int, default=42, help="Random seed")\n    args = parser.parse_args()\n\n    set_seed(args.seed)\n    device = setup_device()\n\n    # ------------------------------------------------------------------\n    # 1. Load configuration\n    # ------------------------------------------------------------------\n    with open(args.config_file, "r") as f:\n        config = yaml.safe_load(f)\n\n    run_cfg: Dict = None  # type: ignore\n    for r in config["runs"]:\n        if r["name"] == args.run_id:\n            run_cfg = r\n            break\n    if run_cfg is None:\n        raise ValueError(f"Run id {args.run_id} not found in config {args.config_file}")\n\n    # ------------------------------------------------------------------\n    # 2. Prepare IO paths\n    # ------------------------------------------------------------------\n    run_dir = Path(args.results_dir) / args.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / "images").mkdir(exist_ok=True)\n\n    # Save run config for reproducibility\n    with open(run_dir / "cfg.yaml", "w") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # ------------------------------------------------------------------\n    # 3. Build dataset & dataloaders\n    # ------------------------------------------------------------------\n    train_loader, val_loader = pp.build_dataloaders(\n        dataset_cfg=run_cfg["dataset"], batch_size=run_cfg["training"]["batch_size"], num_workers=run_cfg["training"].get("num_workers", 4)\n    )\n\n    # ------------------------------------------------------------------\n    # 4. Build model & optimiser\n    # ------------------------------------------------------------------\n    model: mdl.BaseExperimentModel = mdl.build_model(run_cfg["model"], device=device)\n    optimizer = optim.AdamW(model.parameters(), lr=run_cfg["training"]["lr"])\n\n    # ------------------------------------------------------------------\n    # 5. Training loop\n    # ------------------------------------------------------------------\n    history: List[Dict] = []\n    epochs: int = run_cfg["training"]["epochs"]\n    for epoch in range(1, epochs + 1):\n        train_loss = _train_one_epoch(\n            model, train_loader, optimizer, device, epoch, max_norm=run_cfg["training"].get("grad_clip", None)\n        )\n        val_loss = _validate(model, val_loader, device, epoch)\n        history.append({"epoch": epoch, "train_loss": train_loss, "val_loss": val_loss})\n\n    # ------------------------------------------------------------------\n    # 6. Final evaluation metrics (FID, CLIPScore, inference‐time)\n    # ------------------------------------------------------------------\n    fid_score = None\n    clip_score = None\n    if run_cfg.get("evaluation", {}).get("compute_fid", False):\n        fid_metric = torchmetrics.image.fid.FrechetInceptionDistance(feature=2048).to(device)\n        clip_metric = torchmetrics.functional.clip_score\n        model.eval()\n        with torch.no_grad():\n            for imgs, prompts in tqdm(val_loader, desc="Computing FID/CLIP"):\n                imgs = imgs.to(device)\n                # Generate images using current model\n                gen_imgs = model.generate(prompts) if hasattr(model, "generate") else imgs  # type: ignore\n                gen_imgs = (gen_imgs.clamp(-1, 1) + 1) / 2  # to [0,1]\n                fid_metric.update(gen_imgs, real=False)\n                fid_metric.update((imgs.clamp(-1, 1) + 1) / 2, real=True)\n                clip_score_batch = clip_metric(gen_imgs, prompts)\n                if clip_score is None:\n                    clip_score = clip_score_batch.mean()\n                else:\n                    clip_score += clip_score_batch.mean()\n        fid_score = float(fid_metric.compute())\n        clip_score = float(clip_score / len(val_loader)) if clip_score is not None else None\n\n    inference_latency = _measure_inference_time(model, prompts=["a test prompt"], device=device)\n\n    # ------------------------------------------------------------------\n    # 7. Save artefacts & print JSON\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / "model.pt")\n\n    results = {\n        "run_id": args.run_id,\n        "final_epoch": epochs,\n        "metrics": {\n            "train_loss": history[-1]["train_loss"] if history else None,\n            "val_loss": history[-1]["val_loss"] if history else None,\n            "fid": fid_score,\n            "clip_score": clip_score,\n            "inference_time": inference_latency,\n        },\n        "history": history,\n    }\n\n    with open(run_dir / "results.json", "w") as f:\n        json.dump(results, f, indent=2)\n\n    print(json.dumps(results))  # <-- structured output expected by main.py\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': '"""src/evaluate.py\nAggregates the results of all experiment variations and produces comparison\nfigures under <results-dir>/images/.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style="whitegrid")\n\n################################################################################\n# Plot helpers                                                                  \n################################################################################\n\ndef _make_bar(ax, names: List[str], values: List[float], title: str, ylabel: str):\n    palette = sns.color_palette("Set2", len(names))\n    bars = ax.bar(names, values, color=palette)\n    ax.set_title(title)\n    ax.set_ylabel(ylabel)\n    for bar, val in zip(bars, values):\n        height = bar.get_height()\n        ax.annotate(f"{val:.3f}", xy=(bar.get_x() + bar.get_width() / 2, height), xytext=(0, 3), textcoords="offset points", ha="center", va="bottom")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description="Evaluate all experimental runs and plot comparisons")\n    parser.add_argument("--results-dir", required=True, help="Path where individual run folders live")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    image_dir = results_dir / "images"\n    image_dir.mkdir(exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # 1. Load result JSONs\n    # ------------------------------------------------------------------\n    run_results: Dict[str, Dict] = {}\n    for run_path in results_dir.iterdir():\n        if run_path.is_dir() and (run_path / "results.json").exists():\n            with open(run_path / "results.json", "r") as f:\n                run_results[run_path.name] = json.load(f)\n\n    if not run_results:\n        print("{}")\n        return\n\n    names = list(run_results.keys())\n    fid_values = [run_results[n]["metrics"].get("fid", None) for n in names]\n    clip_values = [run_results[n]["metrics"].get("clip_score", None) for n in names]\n    lat_values = [run_results[n]["metrics"].get("inference_time", None) for n in names]\n\n    # ------------------------------------------------------------------\n    # 2. Plot FID and CLIPScore\n    # ------------------------------------------------------------------\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    if any(v is not None for v in fid_values):\n        _make_bar(axes[0], names, [v if v is not None else 0 for v in fid_values], "FID (lower=better)", "FID")\n    if any(v is not None for v in clip_values):\n        _make_bar(axes[1], names, [v if v is not None else 0 for v in clip_values], "CLIPScore (higher=better)", "CLIPScore")\n    plt.tight_layout()\n    plt.savefig(image_dir / "quality_metrics.pdf", bbox_inches="tight")\n\n    # ------------------------------------------------------------------\n    # 3. Plot inference latency\n    # ------------------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    _make_bar(ax, names, lat_values, "Inference latency", "seconds / image")\n    plt.tight_layout()\n    plt.savefig(image_dir / "inference_latency.pdf", bbox_inches="tight")\n\n    # ------------------------------------------------------------------\n    # 4. Print structured comparison summary\n    # ------------------------------------------------------------------\n    summary = {\n        "best_fid_run": min([(v, n) for n, v in zip(names, fid_values) if v is not None], default=(None, None))[1],\n        "best_clip_run": max([(v, n) for n, v in zip(names, clip_values) if v is not None], default=(None, None))[1],\n        "fastest_run": min([(v, n) for n, v in zip(names, lat_values) if v is not None], default=(None, None))[1],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '"""src/preprocess.py\nCommon preprocessing utilities and dataset registry. All dataset‐specific logic\nis isolated behind a registry so that new datasets can be plugged‐in by\nregistering a new class.\n"""\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n################################################################################\n# Dataset registry                                                               \n################################################################################\n\n_DATASET_REGISTRY = {}\n\n\ndef register_dataset(name):\n    def _inner(cls):\n        _DATASET_REGISTRY[name] = cls\n        return cls\n\n    return _inner\n\n\n# -----------------------------------------------------------------------------\n# Toy dataset (used by smoke tests)                                             \n# -----------------------------------------------------------------------------\n\n@register_dataset("toy")\nclass ToyDataset(Dataset):\n    """A minimal random dataset for smoke tests. Returns (img, prompt)."""\n\n    def __init__(self, num_samples: int, image_size: int, split: str = "train") -> None:\n        super().__init__()\n        self.num_samples = num_samples\n        self.image_size = image_size\n        self.split = split\n        self.transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n        )\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        img = torch.randn(3, self.image_size, self.image_size)\n        prompt = "a synthetic prompt"\n        return img, prompt\n\n\n# -----------------------------------------------------------------------------\n# Placeholder for real datasets                                                 \n# -----------------------------------------------------------------------------\n# PLACEHOLDER: Will be replaced with specific dataset classes (e.g., LAION, COCO)\n# Example:\n# @register_dataset("laion")\n# class LaionDataset(Dataset):\n#     ...\n\n################################################################################\n# Dataloader builder                                                            \n################################################################################\n\ndef _build_transforms(image_size: int):\n    return transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ])\n\n\ndef build_dataloaders(\n    dataset_cfg: dict,\n    batch_size: int,\n    num_workers: int = 4,\n) -> Tuple[DataLoader, DataLoader]:\n    """Build train and validation dataloaders from a dataset config dict."""\n    name = dataset_cfg["name"]\n    params = dataset_cfg["params"].copy()\n    DatasetCls = _DATASET_REGISTRY.get(name)\n    if DatasetCls is None:\n        raise ValueError(\n            f"Dataset \'{name}\' is not registered. Available: {list(_DATASET_REGISTRY.keys())}."\n        )\n\n    train_ds = DatasetCls(split="train", **params)\n    val_ds = DatasetCls(split="val", **params)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    return train_loader, val_loader\n', 'model_py': '"""src/model.py\nModel registry and core algorithm implementations including the Feature\nConsistency Regularisation (FCR) diffusion model.\n"""\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n_MODEL_REGISTRY: Dict[str, nn.Module] = {}\n\n################################################################################\n# Utilities                                                                     \n################################################################################\n\ndef register_model(name: str):\n    def decorator(cls):\n        _MODEL_REGISTRY[name] = cls\n        return cls\n\n    return decorator\n\n\ndef build_model(model_cfg: Dict[str, Any], device: torch.device):\n    name = model_cfg["name"]\n    params = model_cfg.get("params", {})\n    ModelCls = _MODEL_REGISTRY.get(name)\n    if ModelCls is None:\n        raise ValueError(\n            f"Model \'{name}\' is not registered. Available: {list(_MODEL_REGISTRY.keys())}."\n        )\n    model: BaseExperimentModel = ModelCls(params=params, device=device)  # type: ignore\n    return model.to(device)\n\n################################################################################\n# Base class                                                                    \n################################################################################\n\nclass BaseExperimentModel(nn.Module):\n    """Abstract base class that every model follows."""\n\n    def __init__(self):\n        super().__init__()\n\n    # ---------------------------------------------------------------------\n    # Required API                                                          \n    # ---------------------------------------------------------------------\n    def training_step(self, batch):\n        raise NotImplementedError\n\n    def validation_step(self, batch):\n        raise NotImplementedError\n\n    def generate(self, prompts: List[str]):\n        """Optional. Needed for diffusion models to produce samples at inference."""\n        raise NotImplementedError\n\n################################################################################\n# Toy classification model                                                      \n################################################################################\n\n@register_model("toy")\nclass ToyModel(BaseExperimentModel):\n    """Simple 3-layer MLP classifier on flattened images. Used by smoke tests."""\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        self.input_dim = params["input_dim"]\n        hidden_dim = params.get("hidden_dim", 128)\n        num_classes = params.get("num_classes", 10)\n        self.net = nn.Sequential(\n            nn.Linear(self.input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes),\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.num_classes = num_classes\n        self.to(device)\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x.view(x.size(0), -1))\n\n    def training_step(self, batch):\n        imgs, _ = batch  # toy prompts ignored\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {"loss": loss}\n\n    def validation_step(self, batch):\n        imgs, _ = batch\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {"val_loss": loss}\n\n    def generate(self, prompts):\n        # Not applicable – return random tensors for API compatibility\n        return torch.randn(1, 3, 32, 32, device=next(self.parameters()).device)\n\n################################################################################\n# Diffusion model with Feature Consistency Regularisation                       \n################################################################################\n\n# Import heavy deps lazily to speed up smoke tests.\ntry:\n    from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n    from transformers import CLIPTextModel, CLIPTokenizer\nexcept ImportError:\n    AutoencoderKL = UNet2DConditionModel = DDPMScheduler = CLIPTextModel = CLIPTokenizer = None  # type: ignore\n\n\n@register_model("diffusion_fcr")\nclass DiffusionFCRModel(BaseExperimentModel):\n    """Stable-Diffusion UNet with Feature Consistency Regularisation."""\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        if AutoencoderKL is None:\n            raise ImportError("diffusers and transformers must be installed for diffusion models")\n\n        model_name = params.get("pretrained_model_name_or_path", "runwayml/stable-diffusion-v1-5")\n        dtype = torch.float16 if params.get("use_fp16", True) else torch.float32\n\n        # Components\n        self.vae = AutoencoderKL.from_pretrained(model_name, subfolder="vae", torch_dtype=dtype).to(device)\n        self.unet = UNet2DConditionModel.from_pretrained(model_name, subfolder="unet", torch_dtype=dtype).to(device)\n        self.text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder", torch_dtype=dtype).to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")\n        self.scheduler = DDPMScheduler.from_pretrained(model_name, subfolder="scheduler")\n\n        self.lambda_fcr = params.get("lambda_fcr", 0.1)\n        self.delta_max = params.get("delta_max", 10)\n        self.device = device\n        self.dtype = dtype\n\n        # Hook to capture encoder features (last down block activations)\n        self._enc_feats = None\n\n        def _hook_fn(_, __, output):\n            self._enc_feats = output\n\n        self.unet.down_blocks[-1].register_forward_hook(_hook_fn)\n\n    # ------------------------------------------------------------------\n    # Helper functions                                                   \n    # ------------------------------------------------------------------\n    def _encode_images(self, imgs: torch.Tensor) -> torch.Tensor:\n        latents = self.vae.encode(imgs).latent_dist.sample()\n        return latents * 0.18215\n\n    def _get_text_embeddings(self, prompts: List[str]):\n        tokens = self.tokenizer(\n            prompts,\n            padding="max_length",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors="pt",\n        ).to(self.device)\n        return self.text_encoder(tokens.input_ids)[0]\n\n    # ------------------------------------------------------------------\n    # Required API                                                      \n    # ------------------------------------------------------------------\n    def training_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n\n        # Sample primary timestep t\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n\n        text_emb = self._get_text_embeddings(prompts)\n        # First forward pass\n        self._enc_feats = None\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=text_emb).sample\n        loss_main = F.mse_loss(noise_pred, noise)\n        f1 = self._enc_feats.detach()\n\n        # Second timestep for FCR\n        delta = torch.randint(1, self.delta_max + 1, (bsz,), device=self.device)\n        t2 = torch.clamp(t - delta, min=0)\n        noisy_latents2 = self.scheduler.add_noise(latents, noise, t2)\n\n        self._enc_feats = None\n        _ = self.unet(noisy_latents2, t2, encoder_hidden_states=text_emb)\n        f2 = self._enc_feats.detach()\n\n        loss_fcr = (f1 - f2).pow(2).mean()\n        loss = loss_main + self.lambda_fcr * loss_fcr\n        return {"loss": loss, "loss_main": loss_main, "loss_fcr": loss_fcr}\n\n    @torch.no_grad()\n    def validation_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n        text_emb = self._get_text_embeddings(prompts)\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=text_emb).sample\n        val_loss = F.mse_loss(noise_pred, noise)\n        return {"val_loss": val_loss}\n\n    # ------------------------------------------------------------------\n    # Inference API                                                     \n    # ------------------------------------------------------------------\n    @torch.no_grad()\n    def generate(self, prompts: List[str], num_inference_steps: int = 50, stride: int = 10):\n        """Generates images using cached encoder features. This is a minimal\n        implementation purely for evaluation/latency measurement and does *not*\n        include the full Faster-Diffusion encoder-caching logic. Users can swap\n        this function in later phases with an optimised sampler.\n        """\n        self.eval()\n        text_emb = self._get_text_embeddings(prompts)\n        latents = torch.randn(\n            len(prompts),\n            self.unet.in_channels,\n            64, 64,  # SD v1 latent spatial dims for 512×512\n            device=self.device,\n            dtype=self.dtype,\n        )\n        self.scheduler.set_timesteps(num_inference_steps)\n        encoder_cached = None\n        for i, t in enumerate(self.scheduler.timesteps):\n            if i % stride == 0 or encoder_cached is None:\n                # Full forward when cache is invalid / stride boundary\n                self._enc_feats = None\n                noise_pred = self.unet(latents, t, encoder_hidden_states=text_emb).sample\n                encoder_cached = self._enc_feats.detach()\n            else:\n                # Re-use cached encoder features (toy: just re-use noise_pred)\n                noise_pred = self.unet(latents, t, encoder_hidden_states=text_emb, encoder_hidden_states_mask=encoder_cached).sample  # type: ignore\n            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n        imgs = self.vae.decode(latents / 0.18215).sample\n        return imgs\n', 'main_py': '"""src/main.py\nOrchestrates the execution of all experiment variations defined in a YAML file.\nFor each run it spawns src.train as a subprocess, tee-ing stdout/stderr into\nboth the console and per-run log files. After all runs complete, it calls\nsrc.evaluate to aggregate and visualise the results.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\n################################################################################\n# Utilities                                                                     \n################################################################################\n\ndef _tee_stream(stream, log_file):\n    """Mirrors a stream (stdout/stderr) into a log file and the parent stream."""\n    for line in iter(stream.readline, b""):\n        sys.stdout.buffer.write(line) if log_file.name.endswith("stdout.log") else sys.stderr.buffer.write(line)\n        log_file.buffer.write(line)\n    stream.close()\n\n\ndef _run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n\n    with open(stdout_path, "wb") as fout, open(stderr_path, "wb") as ferr:\n        threads = [\n            threading.Thread(target=_tee_stream, args=(proc.stdout, fout), daemon=True),\n            threading.Thread(target=_tee_stream, args=(proc.stderr, ferr), daemon=True),\n        ]\n        for t in threads:\n            t.start()\n        proc.wait()\n        for t in threads:\n            t.join()\n    if proc.returncode != 0:\n        raise RuntimeError(f"Subprocess {\' \'.join(cmd)} failed with exit code {proc.returncode}")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description="Run all experiments defined in a config YAML")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true", help="Run smoke_test.yaml")\n    group.add_argument("--full-experiment", action="store_true", help="Run full_experiment.yaml")\n    parser.add_argument("--results-dir", required=True, help="Where all outputs should be saved")\n    args = parser.parse_args()\n\n    cfg_path = Path("config/smoke_test.yaml" if args.smoke_test else "config/full_experiment.yaml")\n    with open(cfg_path, "r") as f:\n        cfg = yaml.safe_load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Sequentially run each variation\n    # ------------------------------------------------------------------\n    for run in cfg["runs"]:\n        run_id = run["name"]\n        print(f"==== Running experiment: {run_id} ====")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(exist_ok=True)\n\n        cmd = [\n            sys.executable,\n            "-m",\n            "src.train",\n            "--run-id",\n            run_id,\n            "--config-file",\n            str(cfg_path),\n            "--results-dir",\n            str(results_dir),\n        ]\n        _run_subprocess(cmd, cwd=Path.cwd(), stdout_path=run_dir / "stdout.log", stderr_path=run_dir / "stderr.log")\n\n    # ------------------------------------------------------------------\n    # After all runs → aggregate / visualise\n    # ------------------------------------------------------------------\n    eval_cmd = [sys.executable, "-m", "src.evaluate", "--results-dir", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == "__main__":\n    main()\n', 'pyproject_toml': '[build-system]\nrequires = ["setuptools>=61.0"]\nbuild-backend = "setuptools.build_meta"\n\n[project]\nname = "fcr_experiments"\nversion = "0.1.0"\ndescription = "Common core foundation for Feature Consistency Regularisation experiments"\nauthors = ["AI Researcher <ai@local>"]\nrequires-python = ">=3.10"\ndependencies = [\n    "torch>=2.0.0",\n    "torchvision>=0.15.0",\n    "diffusers>=0.20.0",\n    "transformers>=4.30.0",\n    "torchmetrics>=0.11.4",\n    "pyyaml>=6.0",\n    "tqdm>=4.64.0",\n    "matplotlib>=3.7.0",\n    "seaborn>=0.13.0",\n    "numpy>=1.23.0",\n    "pillow>=9.5.0",\n]\n\n[project.optional-dependencies]\ndev = ["pytest>=7.0"]\n', 'smoke_test_yaml': '# config/smoke_test.yaml\n# Lightweight configuration that runs entirely on random data and tiny models.\nruns:\n  - name: toy_baseline\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 128\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n  - name: toy_variant\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n', 'full_experiment_yaml': '# config/full_experiment.yaml\n# PLACEHOLDER configuration. Specific datasets and models will be filled in\n# the next derivation phase.\n\nruns:\n  - name: BASELINE_FD5\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with LAION/COCO dataset configuration\n      params:\n        SPECIFIC_CONFIG_PLACEHOLDER: value\n    model:\n      name: MODEL_PLACEHOLDER  # PLACEHOLDER: Pre-trained Stable Diffusion UNet (no FCR)\n      params:\n        stride: 5\n    training:\n      epochs: 0  # Inference-only baseline\n      batch_size: 1\n      lr: 0.0\n    evaluation:\n      compute_fid: true\n  - name: FCR_FD10\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: same as above\n      params:\n        SPECIFIC_CONFIG_PLACEHOLDER: value\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5\n        lambda_fcr: 0.1\n        delta_max: 10\n        use_fp16: true\n    training:\n      epochs: 10\n      batch_size: 4\n      lr: 1e-4\n    evaluation:\n      compute_fid: true\n  # Additional experiment variations (ablations, combined methods, etc.)\n  # will be appended here in later phases.\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: stable-diffusion-v1-5/stable-diffusion-v1-5

- Code: from diffusers import StableDiffusionPipeline
import torch

model_id = "sd-legacy/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]  
    
image.save("astronaut_rides_horse.png")


- ID: stable-diffusion-v1-5/stable-diffusion-v1-5

- Code: from diffusers import StableDiffusionPipeline
import torch

model_id = "sd-legacy/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]  
    
image.save("astronaut_rides_horse.png")



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: limingcv/LAION_Aesthetics_512





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "\"\"\"src/train.py\nRuns a single experiment variation specified by --run-id in a YAML config.\nAll experiment artefacts are written to <results-dir>/<run-id>/.\nThe script prints a single JSON line with the final metrics to stdout so that\nmain.py can parse it. Stdout/stderr are also captured by main.py and mirrored\ninto log files.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport yaml\n\nfrom . import preprocess as pp\nfrom . import model as mdl\n\n################################################################################\n# Helper functions                                                               \n################################################################################\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef setup_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n################################################################################\n# Core training / validation utilities                                          \n################################################################################\n\ndef _train_one_epoch(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    max_norm: float | None = None,\n) -> float:\n    model.train()\n    running_loss: List[float] = []\n    pbar = tqdm(dataloader, desc=f\"Train E{epoch}\", leave=False)\n    for batch in pbar:\n        optimizer.zero_grad(set_to_none=True)\n        batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n        out = model.training_step(batch_on_device)\n        loss: torch.Tensor = out[\"loss\"]\n        loss.backward()\n        if max_norm is not None:\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n        optimizer.step()\n        running_loss.append(loss.item())\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n    return float(sum(running_loss) / len(running_loss))\n\n\ndef _validate(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    device: torch.device,\n    epoch: int,\n) -> float:\n    model.eval()\n    val_losses: List[float] = []\n    pbar = tqdm(dataloader, desc=f\"Val   E{epoch}\", leave=False)\n    with torch.no_grad():\n        for batch in pbar:\n            batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n            out = model.validation_step(batch_on_device)\n            loss: torch.Tensor = out[\"val_loss\"]\n            val_losses.append(loss.item())\n            pbar.set_postfix(val_loss=f\"{loss.item():.4f}\")\n    return float(sum(val_losses) / len(val_losses))\n\n################################################################################\n# Inference‐time utilities                                                      \n################################################################################\n\ndef _measure_inference_time(model: mdl.BaseExperimentModel, prompts: List[str], device: torch.device) -> float:\n    \"\"\"Measures the average wall‐clock inference latency per sample.\n    For diffusion models this calls the generate method; for toy models this\n    performs a single forward pass.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        start = time.time()\n        _ = model.generate(prompts) if hasattr(model, \"generate\") else model(torch.randn(1, 3, 32, 32, device=device))\n        torch.cuda.synchronize() if device.type == \"cuda\" else None\n        end = time.time()\n    return float(end - start)\n\n################################################################################\n# Main entry point                                                              \n################################################################################\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation\")\n    parser.add_argument(\"--run-id\", required=True, help=\"Name of the run variation as defined in the YAML config\")\n    parser.add_argument(\"--config-file\", required=True, help=\"Path to YAML experiment config\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results will be stored\")\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n    args = parser.parse_args()\n\n    set_seed(args.seed)\n    device = setup_device()\n\n    # ------------------------------------------------------------------\n    # 1. Load configuration\n    # ------------------------------------------------------------------\n    with open(args.config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    run_cfg: Dict = None  # type: ignore\n    for r in config[\"runs\"]:\n        if r[\"name\"] == args.run_id:\n            run_cfg = r\n            break\n    if run_cfg is None:\n        raise ValueError(f\"Run id {args.run_id} not found in config {args.config_file}\")\n\n    # ------------------------------------------------------------------\n    # 2. Prepare IO paths\n    # ------------------------------------------------------------------\n    run_dir = Path(args.results_dir) / args.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / \"images\").mkdir(exist_ok=True)\n\n    # Save run config for reproducibility\n    with open(run_dir / \"cfg.yaml\", \"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # ------------------------------------------------------------------\n    # 3. Build dataset & dataloaders\n    # ------------------------------------------------------------------\n    train_loader, val_loader = pp.build_dataloaders(\n        dataset_cfg=run_cfg[\"dataset\"], batch_size=run_cfg[\"training\"][\"batch_size\"], num_workers=run_cfg[\"training\"].get(\"num_workers\", 4)\n    )\n\n    # ------------------------------------------------------------------\n    # 4. Build model & optimiser\n    # ------------------------------------------------------------------\n    model: mdl.BaseExperimentModel = mdl.build_model(run_cfg[\"model\"], device=device)\n    optimizer = optim.AdamW(model.parameters(), lr=run_cfg[\"training\"][\"lr\"])\n\n    # ------------------------------------------------------------------\n    # 5. Training loop\n    # ------------------------------------------------------------------\n    history: List[Dict] = []\n    epochs: int = run_cfg[\"training\"][\"epochs\"]\n    for epoch in range(1, epochs + 1):\n        train_loss = _train_one_epoch(\n            model, train_loader, optimizer, device, epoch, max_norm=run_cfg[\"training\"].get(\"grad_clip\", None)\n        )\n        val_loss = _validate(model, val_loader, device, epoch)\n        history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n\n    # ------------------------------------------------------------------\n    # 6. Final evaluation metrics (FID, CLIPScore, inference‐time)\n    # ------------------------------------------------------------------\n    fid_score = None\n    clip_score = None\n    if run_cfg.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        fid_metric = torchmetrics.image.fid.FrechetInceptionDistance(feature=2048).to(device)\n        clip_metric = torchmetrics.functional.clip_score\n        model.eval()\n        with torch.no_grad():\n            for imgs, prompts in tqdm(val_loader, desc=\"Computing FID/CLIP\"):\n                imgs = imgs.to(device)\n                # Generate images using current model\n                gen_imgs = model.generate(prompts) if hasattr(model, \"generate\") else imgs  # type: ignore\n                gen_imgs = (gen_imgs.clamp(-1, 1) + 1) / 2  # to [0,1]\n                fid_metric.update(gen_imgs, real=False)\n                fid_metric.update((imgs.clamp(-1, 1) + 1) / 2, real=True)\n                clip_score_batch = clip_metric(gen_imgs, prompts)\n                if clip_score is None:\n                    clip_score = clip_score_batch.mean()\n                else:\n                    clip_score += clip_score_batch.mean()\n        fid_score = float(fid_metric.compute())\n        clip_score = float(clip_score / len(val_loader)) if clip_score is not None else None\n\n    inference_latency = _measure_inference_time(model, prompts=[\"a test prompt\"], device=device)\n\n    # ------------------------------------------------------------------\n    # 7. Save artefacts & print JSON\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n\n    results = {\n        \"run_id\": args.run_id,\n        \"final_epoch\": epochs,\n        \"metrics\": {\n            \"train_loss\": history[-1][\"train_loss\"] if history else None,\n            \"val_loss\": history[-1][\"val_loss\"] if history else None,\n            \"fid\": fid_score,\n            \"clip_score\": clip_score,\n            \"inference_time\": inference_latency,\n        },\n        \"history\": history,\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    print(json.dumps(results))  # <-- structured output expected by main.py\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nAggregates the results of all experiment variations and produces comparison\nfigures under <results-dir>/images/.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\n################################################################################\n# Plot helpers                                                                  \n################################################################################\n\ndef _make_bar(ax, names: List[str], values: List[float], title: str, ylabel: str):\n    palette = sns.color_palette(\"Set2\", len(names))\n    bars = ax.bar(names, values, color=palette)\n    ax.set_title(title)\n    ax.set_ylabel(ylabel)\n    for bar, val in zip(bars, values):\n        height = bar.get_height()\n        ax.annotate(f\"{val:.3f}\", xy=(bar.get_x() + bar.get_width() / 2, height), xytext=(0, 3), textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Evaluate all experimental runs and plot comparisons\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Path where individual run folders live\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    image_dir = results_dir / \"images\"\n    image_dir.mkdir(exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # 1. Load result JSONs\n    # ------------------------------------------------------------------\n    run_results: Dict[str, Dict] = {}\n    for run_path in results_dir.iterdir():\n        if run_path.is_dir() and (run_path / \"results.json\").exists():\n            with open(run_path / \"results.json\", \"r\") as f:\n                run_results[run_path.name] = json.load(f)\n\n    if not run_results:\n        print(\"{}\")\n        return\n\n    names = list(run_results.keys())\n    fid_values = [run_results[n][\"metrics\"].get(\"fid\", None) for n in names]\n    clip_values = [run_results[n][\"metrics\"].get(\"clip_score\", None) for n in names]\n    lat_values = [run_results[n][\"metrics\"].get(\"inference_time\", None) for n in names]\n\n    # ------------------------------------------------------------------\n    # 2. Plot FID and CLIPScore\n    # ------------------------------------------------------------------\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    if any(v is not None for v in fid_values):\n        _make_bar(axes[0], names, [v if v is not None else 0 for v in fid_values], \"FID (lower=better)\", \"FID\")\n    if any(v is not None for v in clip_values):\n        _make_bar(axes[1], names, [v if v is not None else 0 for v in clip_values], \"CLIPScore (higher=better)\", \"CLIPScore\")\n    plt.tight_layout()\n    plt.savefig(image_dir / \"quality_metrics.pdf\", bbox_inches=\"tight\")\n\n    # ------------------------------------------------------------------\n    # 3. Plot inference latency\n    # ------------------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    _make_bar(ax, names, lat_values, \"Inference latency\", \"seconds / image\")\n    plt.tight_layout()\n    plt.savefig(image_dir / \"inference_latency.pdf\", bbox_inches=\"tight\")\n\n    # ------------------------------------------------------------------\n    # 4. Print structured comparison summary\n    # ------------------------------------------------------------------\n    summary = {\n        \"best_fid_run\": min([(v, n) for n, v in zip(names, fid_values) if v is not None], default=(None, None))[1],\n        \"best_clip_run\": max([(v, n) for n, v in zip(names, clip_values) if v is not None], default=(None, None))[1],\n        \"fastest_run\": min([(v, n) for n, v in zip(names, lat_values) if v is not None], default=(None, None))[1],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nCommon preprocessing utilities and dataset registry. All dataset‐specific logic\nis isolated behind a registry so that new datasets can be plugged‐in by\nregistering a new class.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Any, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n# HuggingFace datasets library\ntry:\n    from datasets import load_dataset, Dataset as HFDataset\nexcept ImportError as e:  # pragma: no cover\n    raise ImportError(\n        \"The 'datasets' package is required for the experiment datasets.\\n\"\n        \"Install with: pip install datasets\"\n    ) from e\n\n################################################################################\n# Dataset registry                                                               \n################################################################################\n\n_DATASET_REGISTRY = {}\n\n\ndef register_dataset(name: str):\n    \"\"\"Decorator to register a dataset class.\"\"\"\n\n    def _inner(cls):\n        _DATASET_REGISTRY[name] = cls\n        return cls\n\n    return _inner\n\n################################################################################\n# Transform helpers                                                              \n################################################################################\n\ndef _build_transforms(image_size: int):\n    # All diffusion models expect inputs in the range [-1, 1]. The following\n    # pipeline converts PIL → tensor in [0,1] → rescale/normalise to [-1,1].\n    return transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),  # -> [0,1]\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # -> [-1,1]\n        ]\n    )\n\n################################################################################\n# Toy dataset (used by smoke tests)                                             \n################################################################################\n\n@register_dataset(\"toy\")\nclass ToyDataset(Dataset):\n    \"\"\"A minimal random dataset for smoke tests. Returns (img, prompt).\"\"\"\n\n    def __init__(self, num_samples: int, image_size: int, split: str = \"train\") -> None:\n        super().__init__()\n        self.num_samples = num_samples\n        self.image_size = image_size\n        self.split = split\n        self.transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n        )\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        img = torch.randn(3, self.image_size, self.image_size)\n        prompt = \"a synthetic prompt\"\n        return img, prompt\n\n################################################################################\n# Generic HuggingFace image/text dataset                                        \n################################################################################\n\n@register_dataset(\"hf_text_image\")\nclass HFTextImageDataset(Dataset):\n    \"\"\"Generic dataset wrapper for any HF dataset with an image & caption column.\n\n    Params expected in `params` dict:\n      hf_name            – str  – HuggingFace dataset identifier\n      image_column       – str  – column containing the image (default: \"image\")\n      caption_column     – str  – column containing caption/text (default: \"text\")\n      resolution         – int  – resize/crop size fed into the model\n      val_ratio          – float – if dataset has no dedicated validation split, take\n                                   this fraction from the *end* of the dataset as val\n      max_samples        – int  – optional cap on the number of samples loaded\n    \"\"\"\n\n    def __init__(self, split: str, **params: Any) -> None:  # type: ignore[override]\n        super().__init__()\n        hf_name: str = params[\"hf_name\"]\n        img_col: str = params.get(\"image_column\", \"image\")\n        cap_col: str = params.get(\"caption_column\", \"text\")\n        resolution: int = int(params.get(\"resolution\", 512))\n        val_ratio: float = float(params.get(\"val_ratio\", 0.05))\n        max_samples: int | None = params.get(\"max_samples\")\n\n        # ------------------------------------------------------------------\n        # 1. Load dataset split                                             \n        # ------------------------------------------------------------------\n        available_splits = load_dataset(hf_name, split=None).keys()  # type: ignore[arg-type]\n        if split in available_splits:\n            ds: HFDataset = load_dataset(hf_name, split=split)  # type: ignore[arg-type]\n        else:\n            # Fallback: load full training set and manually create val slice\n            full_ds: HFDataset = load_dataset(hf_name, split=\"train\")  # type: ignore[arg-type]\n            n_total = len(full_ds)\n            n_val = int(val_ratio * n_total)\n            if split == \"train\":\n                indices = list(range(0, n_total - n_val))\n            elif split in {\"val\", \"validation\"}:\n                indices = list(range(n_total - n_val, n_total))\n            else:\n                raise ValueError(f\"Unknown split '{split}' for dataset {hf_name}\")\n            ds = full_ds.select(indices)\n\n        # Optionally cap dataset size (useful for quick tests)\n        if max_samples is not None and len(ds) > max_samples:\n            indices = random.sample(range(len(ds)), max_samples)\n            ds = ds.select(indices)\n\n        self.data = ds\n        self.image_column = img_col\n        self.caption_column = cap_col\n        self.transform = _build_transforms(resolution)\n\n    def __len__(self) -> int:  # type: ignore[override]\n        return len(self.data)\n\n    def __getitem__(self, idx):  # type: ignore[override]\n        example = self.data[idx]\n\n        img = example[self.image_column]\n        if not hasattr(img, \"size\"):\n            # In some datasets the image is stored as bytes – rely on datasets to decode\n            img = example[self.image_column].convert(\"RGB\")\n        img = self.transform(img)\n\n        caption = example.get(self.caption_column, \"\")\n        # Many datasets store labels as ints – convert to a descriptive prompt\n        if not isinstance(caption, str):\n            caption = str(caption)\n        return img, caption\n\n################################################################################\n# Dataloader builder                                                            \n################################################################################\n\ndef build_dataloaders(\n    dataset_cfg: dict,\n    batch_size: int,\n    num_workers: int = 4,\n) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"Build train and validation dataloaders from a dataset config dict.\"\"\"\n\n    name = dataset_cfg[\"name\"]\n    params = dataset_cfg[\"params\"].copy()\n    DatasetCls = _DATASET_REGISTRY.get(name)\n    if DatasetCls is None:\n        raise ValueError(\n            f\"Dataset '{name}' is not registered. Available: {list(_DATASET_REGISTRY.keys())}.\"\n        )\n\n    train_ds = DatasetCls(split=\"train\", **params)\n    val_ds = DatasetCls(split=\"val\", **params)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    return train_loader, val_loader\n",
    "model_py": "\"\"\"src/model.py\nModel registry and core algorithm implementations including the Feature\nConsistency Regularisation (FCR) diffusion model.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n_MODEL_REGISTRY: Dict[str, nn.Module] = {}\n\n################################################################################\n# Utilities                                                                     \n################################################################################\n\ndef register_model(name: str):\n    def decorator(cls):\n        _MODEL_REGISTRY[name] = cls\n        return cls\n\n    return decorator\n\n\ndef build_model(model_cfg: Dict[str, Any], device: torch.device):\n    name = model_cfg[\"name\"]\n    params = model_cfg.get(\"params\", {})\n    ModelCls = _MODEL_REGISTRY.get(name)\n    if ModelCls is None:\n        raise ValueError(\n            f\"Model '{name}' is not registered. Available: {list(_MODEL_REGISTRY.keys())}.\"\n        )\n    model: BaseExperimentModel = ModelCls(params=params, device=device)  # type: ignore\n    return model.to(device)\n\n################################################################################\n# Base class                                                                    \n################################################################################\n\nclass BaseExperimentModel(nn.Module):\n    \"\"\"Abstract base class that every model follows.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    # ---------------------------------------------------------------------\n    # Required API                                                          \n    # ---------------------------------------------------------------------\n    def training_step(self, batch):\n        raise NotImplementedError\n\n    def validation_step(self, batch):\n        raise NotImplementedError\n\n    def generate(self, prompts: List[str]):\n        \"\"\"Optional. Needed for diffusion models to produce samples at inference.\"\"\"\n        raise NotImplementedError\n\n################################################################################\n# Toy classification model                                                      \n################################################################################\n\n@register_model(\"toy\")\nclass ToyModel(BaseExperimentModel):\n    \"\"\"Simple 3-layer MLP classifier on flattened images. Used by smoke tests.\"\"\"\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        self.input_dim = params[\"input_dim\"]\n        hidden_dim = params.get(\"hidden_dim\", 128)\n        num_classes = params.get(\"num_classes\", 10)\n        self.net = nn.Sequential(\n            nn.Linear(self.input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes),\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.num_classes = num_classes\n        self.to(device)\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x.view(x.size(0), -1))\n\n    def training_step(self, batch):\n        imgs, _ = batch  # toy prompts ignored\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {\"loss\": loss}\n\n    def validation_step(self, batch):\n        imgs, _ = batch\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {\"val_loss\": loss}\n\n    def generate(self, prompts):\n        # Not applicable – return random tensors for API compatibility\n        return torch.randn(1, 3, 32, 32, device=next(self.parameters()).device)\n\n################################################################################\n# Diffusion model with Feature Consistency Regularisation                       \n################################################################################\n\n# Import heavy deps lazily to speed up smoke tests.\ntry:\n    from diffusers import (\n        AutoencoderKL,\n        UNet2DConditionModel,\n        DDPMScheduler,\n        DPMSolverMultistepScheduler,\n    )\n    from transformers import CLIPTextModel, CLIPTokenizer\nexcept ImportError as e:  # pragma: no cover\n    raise ImportError(\n        \"This project requires 'diffusers' and 'transformers'. Install with pip install diffusers transformers\"\n    ) from e\n\n\n@register_model(\"diffusion_fcr\")\nclass DiffusionFCRModel(BaseExperimentModel):\n    \"\"\"Stable-Diffusion UNet with Feature Consistency Regularisation.\n\n    This class supports:\n      • FCR training with `lambda_fcr` and `delta_max`.\n      • Two scheduler types: DDPM (default) and DPMSolver++ (multistep).\n      • Variable encoder-caching stride at inference (FD-k).\n    \"\"\"\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        model_name = params.get(\"pretrained_model_name_or_path\", \"runwayml/stable-diffusion-v1-5\")\n        dtype = torch.float16 if params.get(\"use_fp16\", True) else torch.float32\n        self.device = device\n        self.dtype = dtype\n\n        # ------------------------------------------------------------------\n        # 1. Components                                                     \n        # ------------------------------------------------------------------\n        self.vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\", torch_dtype=dtype).to(device)\n        self.unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\", torch_dtype=dtype).to(device)\n        self.text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\", torch_dtype=dtype).to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n\n        scheduler_type: str = params.get(\"scheduler_type\", \"ddpm\").lower()\n        if scheduler_type in {\"ddpm\", \"pndm\"}:\n            self.scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n        elif scheduler_type in {\"dpm\", \"dpmsolver\", \"dpmsolver++\"}:\n            self.scheduler = DPMSolverMultistepScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n        else:\n            raise ValueError(f\"Unknown scheduler_type '{scheduler_type}'.\")\n\n        # ------------------------------------------------------------------\n        # 2. FCR hyper-parameters                                           \n        # ------------------------------------------------------------------\n        self.lambda_fcr = float(params.get(\"lambda_fcr\", 0.0))\n        self.delta_max = int(params.get(\"delta_max\", 1))\n        self.fd_stride = int(params.get(\"fd_stride\", 5))\n\n        # ------------------------------------------------------------------\n        # 3. Hook encoder feature map                                       \n        # ------------------------------------------------------------------\n        self._enc_feats = None\n\n        def _hook_fn(_, __, output):\n            # output: (sample, ) or tensor – we save raw tensor\n            self._enc_feats = output if torch.is_tensor(output) else output[0]\n\n        # Last down block approximates the encoder representation\n        self.unet.down_blocks[-1].register_forward_hook(_hook_fn)\n\n    # ------------------------------------------------------------------\n    # Internal helpers                                                    \n    # ------------------------------------------------------------------\n    def _encode_images(self, imgs: torch.Tensor) -> torch.Tensor:\n        latents = self.vae.encode(imgs).latent_dist.sample()\n        return latents * 0.18215  # SD scaling constant\n\n    def _get_text_embeddings(self, prompts: List[str]):\n        tokens = self.tokenizer(\n            prompts,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).to(self.device)\n        return self.text_encoder(tokens.input_ids)[0]\n\n    # ------------------------------------------------------------------\n    # Training & validation                                              \n    # ------------------------------------------------------------------\n    def training_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n\n        # Primary timestep\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n\n        text_emb = self._get_text_embeddings(prompts)\n\n        # Forward 1\n        self._enc_feats = None\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=text_emb).sample\n        loss_main = F.mse_loss(noise_pred, noise)\n        f1 = self._enc_feats.detach()\n\n        # Forward 2 (only if lambda_fcr > 0)\n        if self.lambda_fcr > 0:\n            delta = torch.randint(1, self.delta_max + 1, (bsz,), device=self.device)\n            t2 = torch.clamp(t - delta, min=0)\n            noisy_latents2 = self.scheduler.add_noise(latents, noise, t2)\n            self._enc_feats = None\n            _ = self.unet(noisy_latents2, t2, encoder_hidden_states=text_emb)\n            f2 = self._enc_feats.detach()\n            loss_fcr = (f1 - f2).pow(2).mean()\n        else:\n            loss_fcr = torch.zeros((), device=self.device)\n\n        loss = loss_main + self.lambda_fcr * loss_fcr\n        return {\"loss\": loss, \"loss_main\": loss_main, \"loss_fcr\": loss_fcr}\n\n    @torch.no_grad()\n    def validation_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n        text_emb = self._get_text_embeddings(prompts)\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=text_emb).sample\n        val_loss = F.mse_loss(noise_pred, noise)\n        return {\"val_loss\": val_loss}\n\n    # ------------------------------------------------------------------\n    # Inference                                                          \n    # ------------------------------------------------------------------\n    @torch.no_grad()\n    def generate(self, prompts: List[str], num_inference_steps: int | None = None, stride: int | None = None):\n        \"\"\"Generate images.\n\n        Args:\n          prompts – list of textual prompts.\n          num_inference_steps – number of diffusion sampling steps.\n          stride – stride for encoder caching (FD-k). If >1 the encoder will be\n                    recomputed only every *stride* steps. (Toy implementation –\n                    we simply skip *decoding* the encoder between strides to\n                    emulate the latency difference.)\n        \"\"\"\n\n        self.eval()\n        num_inference_steps = int(num_inference_steps or 50)\n        stride = int(stride or self.fd_stride)\n        text_emb = self._get_text_embeddings(prompts)\n\n        h, w = 64, 64  # latent spatial dims for 512×512 images\n        latents = torch.randn(\n            len(prompts), self.unet.in_channels, h, w, device=self.device, dtype=self.dtype\n        )\n        self.scheduler.set_timesteps(num_inference_steps)\n\n        for i, t in enumerate(self.scheduler.timesteps):\n            if i % stride == 0:\n                # Full UNet forward pass – captures & caches encoder features\n                self._enc_feats = None\n                noise_pred = self.unet(latents, t, encoder_hidden_states=text_emb).sample\n                cached_noise_pred = noise_pred.detach()\n            else:\n                # Re-use cached noise prediction as an approximation (toy)\n                noise_pred = cached_noise_pred\n            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n\n        imgs = self.vae.decode(latents / 0.18215).sample  # → float32 RGB in [-1,1]\n        return imgs\n",
    "main_py": "\"\"\"src/main.py\nOrchestrates the execution of all experiment variations defined in a YAML file.\nFor each run it spawns src.train as a subprocess, tee-ing stdout/stderr into\nboth the console and per-run log files. After all runs complete, it calls\nsrc.evaluate to aggregate and visualise the results.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\n################################################################################\n# Utilities                                                                     \n################################################################################\n\ndef _tee_stream(stream, log_file):\n    \"\"\"Mirrors a stream (stdout/stderr) into a log file and the parent stream.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        sys.stdout.buffer.write(line) if log_file.name.endswith(\"stdout.log\") else sys.stderr.buffer.write(line)\n        log_file.buffer.write(line)\n    stream.close()\n\n\ndef _run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n\n    with open(stdout_path, \"wb\") as fout, open(stderr_path, \"wb\") as ferr:\n        threads = [\n            threading.Thread(target=_tee_stream, args=(proc.stdout, fout), daemon=True),\n            threading.Thread(target=_tee_stream, args=(proc.stderr, ferr), daemon=True),\n        ]\n        for t in threads:\n            t.start()\n        proc.wait()\n        for t in threads:\n            t.join()\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess {' '.join(cmd)} failed with exit code {proc.returncode}\")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run all experiments defined in a config YAML\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Where all outputs should be saved\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Sequentially run each variation\n    # ------------------------------------------------------------------\n    for run in cfg[\"runs\"]:\n        run_id = run[\"name\"]\n        print(f\"==== Running experiment: {run_id} ====\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(exist_ok=True)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--run-id\",\n            run_id,\n            \"--config-file\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n        ]\n        _run_subprocess(cmd, cwd=Path.cwd(), stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs → aggregate / visualise\n    # ------------------------------------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"fcr_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Feature Consistency Regularisation experiments\"\nauthors = [\"AI Researcher <ai@local>\"]\nrequires-python = \">=3.10\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15.0\",\n    \"diffusers>=0.20.0\",\n    \"transformers>=4.30.0\",\n    \"torchmetrics>=0.11.4\",\n    \"pyyaml>=6.0\",\n    \"tqdm>=4.64.0\",\n    \"matplotlib>=3.7.0\",\n    \"seaborn>=0.13.0\",\n    \"numpy>=1.23.0\",\n    \"pillow>=9.5.0\",\n    \"datasets>=2.14.0\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest>=7.0\"]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke-test covering all run IDs from the full experiment but with\n# toy datasets & models so that the CI pipeline completes in seconds.\n\nruns:\n  - name: SD2.1-FD-5\n    dataset:\n      name: toy\n      params:\n        num_samples: 32\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: SD2.1-FCR(λ=0.10)-FD-10\n    dataset:\n      name: toy\n      params:\n        num_samples: 32\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: SD2.1-FCR(λ=0.10)+DPMSolver-FD-10\n    dataset:\n      name: toy\n      params:\n        num_samples: 32\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: OpenJourney-FD-5\n    dataset:\n      name: toy\n      params:\n        num_samples: 32\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: OpenJourney-FCR(λ=0.10)-FD-10\n    dataset:\n      name: toy\n      params:\n        num_samples: 32\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete configuration for experiment exp-2-generalisation-robustness.\n\nruns:\n  # --------------------------------------------------------------------\n  # 1. Stable-Diffusion-2.1 baseline (FD-5)\n  # --------------------------------------------------------------------\n  - name: SD2.1-FD-5\n    dataset:\n      name: hf_text_image\n      params:\n        hf_name: limingcv/LAION_Aesthetics_512\n        image_column: image\n        caption_column: TEXT\n        resolution: 512\n        max_samples: 50000   # subset for manageable training\n        val_ratio: 0.02\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: stabilityai/stable-diffusion-2-1\n        lambda_fcr: 0.0          # no FCR for baseline\n        delta_max: 1\n        fd_stride: 5\n        scheduler_type: ddpm\n        use_fp16: true\n    training:\n      epochs: 0                 # inference-only baseline\n      batch_size: 1\n      lr: 0.0\n    evaluation:\n      compute_fid: true\n\n  # --------------------------------------------------------------------\n  # 2. SD-2.1 with FCR, stride 10\n  # --------------------------------------------------------------------\n  - name: SD2.1-FCR(λ=0.10)-FD-10\n    dataset:\n      name: hf_text_image\n      params:\n        hf_name: limingcv/LAION_Aesthetics_512\n        image_column: image\n        caption_column: TEXT\n        resolution: 512\n        max_samples: 50000\n        val_ratio: 0.02\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: stabilityai/stable-diffusion-2-1\n        lambda_fcr: 0.10\n        delta_max: 10\n        fd_stride: 10\n        scheduler_type: ddpm\n        use_fp16: true\n    training:\n      epochs: 10\n      batch_size: 4\n      lr: 1e-4\n    evaluation:\n      compute_fid: true\n\n  # --------------------------------------------------------------------\n  # 3. SD-2.1 with FCR + DPMSolver++, stride 10\n  # --------------------------------------------------------------------\n  - name: SD2.1-FCR(λ=0.10)+DPMSolver-FD-10\n    dataset:\n      name: hf_text_image\n      params:\n        hf_name: limingcv/LAION_Aesthetics_512\n        image_column: image\n        caption_column: TEXT\n        resolution: 512\n        max_samples: 50000\n        val_ratio: 0.02\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: stabilityai/stable-diffusion-2-1\n        lambda_fcr: 0.10\n        delta_max: 10\n        fd_stride: 10\n        scheduler_type: dpmsolver++\n        use_fp16: true\n    training:\n      epochs: 10\n      batch_size: 4\n      lr: 1e-4\n    evaluation:\n      compute_fid: true\n\n  # --------------------------------------------------------------------\n  # 4. OpenJourney baseline (FD-5)\n  # --------------------------------------------------------------------\n  - name: OpenJourney-FD-5\n    dataset:\n      name: hf_text_image\n      params:\n        hf_name: artbench\n        image_column: image\n        caption_column: label   # labels → caption string via dataset wrapper\n        resolution: 512\n        max_samples: 40000\n        val_ratio: 0.05\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: prompthero/openjourney\n        lambda_fcr: 0.0\n        delta_max: 1\n        fd_stride: 5\n        scheduler_type: ddpm\n        use_fp16: true\n    training:\n      epochs: 0\n      batch_size: 1\n      lr: 0.0\n    evaluation:\n      compute_fid: true\n\n  # --------------------------------------------------------------------\n  # 5. OpenJourney with FCR, stride 10\n  # --------------------------------------------------------------------\n  - name: OpenJourney-FCR(λ=0.10)-FD-10\n    dataset:\n      name: hf_text_image\n      params:\n        hf_name: artbench\n        image_column: image\n        caption_column: label\n        resolution: 512\n        max_samples: 40000\n        val_ratio: 0.05\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: prompthero/openjourney\n        lambda_fcr: 0.10\n        delta_max: 10\n        fd_stride: 10\n        scheduler_type: ddpm\n        use_fp16: true\n    training:\n      epochs: 10\n      batch_size: 4\n      lr: 1e-4\n    evaluation:\n      compute_fid: true\n\n# End of full_experiment.yaml\n"
}
