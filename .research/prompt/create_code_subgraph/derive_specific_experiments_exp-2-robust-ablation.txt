
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['ase-linear', 'auto-ase-lam0.02', 'auto-ase-lam0.05', 'auto-ase-lam0.10', 'auto-ase-70prune-corrupt']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?",
    "Methods": "Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides—per time-step t and per network block k—whether the block is executed.  \n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk·h(t)), where h(t)=1−t (so gates tend to stay open near the data region).  \n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk∈{0,1}) at inference time.  \n3. Loss=Lnoise+λ⋅Σk g̅k, where Lnoise is the standard noise-prediction loss and g̅k is the average gate activation over the batch; λ is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.",
    "Experimental Setup": "Model: the public DDPM CIFAR-10 UNet (32×32).\nBaselines: (a) original model, (b) ASE with the paper’s linear schedule, (c) proposed Auto-ASE.\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; λ=0.05.\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass GatedBlock(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\n    def forward(self, x, t):\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\n        if self.training:\n            return x + gate * (self.block(x, t) - x), gate.mean()\n        else:  # STE binarisation\n            hard_gate = (gate > 0.5).float()\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\n\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\n\ngate_loss_coeff = 0.05\n\nfor x, t, noise in dataloader:\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\n    loss.backward(); optimizer.step()",
    "Expected Result": "Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t≈0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (≈16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.",
    "Expected Conclusion": "By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration."
}

# Experimental Design
- Strategy: Goal: Build a single, rigorous experimental play-book that can be applied to every study of Auto-ASE, proving that the learnable gating schedule (a) preserves or improves sample quality, (b) reduces sampling cost, (c) is robust across models, datasets and solvers, and (d) removes the need for manual tuning.

1. Key hypotheses to validate
   H1  Performance: Auto-ASE delivers the same or better generative quality than the original network and the hand-crafted ASE schedule.
   H2  Efficiency: Auto-ASE cuts wall–clock sampling time, FLOPs and GPU energy by at least 20 % versus the full model and beats the best fixed schedule.
   H3  Robustness & Generalisation: The learned schedule works (without re-tuning) on multiple datasets, resolutions, network backbones and numerical solvers.
   H4  Simplicity: Training overhead and parameter growth are negligible (<0.5 % extra parameters, <5 % extra training time).

2. Universal comparison set
   a. Baselines
      • Full model (no skipping)
      • Original ASE with its published dropping rule
   b. State-of-the-art accelerators that keep the backbone intact (e.g. DDIM, DPM-Solver++, PNDM) to show trade-off curves
   c. Ablations of Auto-ASE
      • No sparsity loss (λ = 0)
      • Shared vs individual gates
      • Different gate shapes h(t)
      • Soft-gating at inference (no STE)

3. Evaluation axes (applied in every experiment)
   Quantitative quality: FID, KID, IS (for images) or task-specific metrics
   Qualitative quality: curated sample grids + human Turing test where feasible
   Efficiency:  (i) average number of executed blocks, (ii) total UNet/Transformer calls, (iii) wall-clock latency, (iv) TFLOPs, (v) peak & average GPU memory, (vi) Joule/1000 images measured by nvidia-smi
   Robustness: metric distributions across 3 random seeds and across 3 λ values
   Schedule insight: plots of g_k(t) heat-maps & histogram of closed gates

4. Experimental matrix (re-used each time)
   Tier-1  In-domain sanity: original public UNet × CIFAR-10 × DDPM solver (50 steps)
   Tier-2  Cross-architecture: replace UNet by DiT-XL/2 or ADM-KD while keeping CIFAR-10
   Tier-3  Cross-data & resolution: ImageNet-64, LSUN-256, Stable-Diffusion 512×512 latent UNet; keep the same Auto-ASE hyper-parameters
   Tier-4  Cross-solver: DDIM 25 steps, DPM-Solver++ 15 steps, PLMS 50 steps
   Tier-5  Stress tests: (i) halve/ double λ, (ii) prune 70 % of blocks, (iii) deploy on corrupted noise schedules

5. Success criteria (must be met on every Tier-1 & Tier-2 run, and on ≥75 % of Tier-3/4/5 runs)
   • ΔFID ≤ +0.5 (or KID ≤ +0.002) relative to full model
   • ≥20 % speed-up vs full model; ≥5 % extra speed-up vs best fixed ASE schedule
   • <0.5 % parameter growth; <5 % extra training time
   • For robustness tiers: variance of ΔFID across seeds ≤ 0.8 and no catastrophic failure (FID < ×1.5 of baseline)

6. Measurement protocol
   • All runs on a single NVIDIA A100-80GB, PyTorch compiled with CUDA 11.8 + cuDNN 8.
   • Disable CPU-GPU overlaps to obtain deterministic timing; average over 1 k samples after 50 warm-ups.
   • Log with Weights & Biases to expose full metrics, curves and gate heat-maps.

7. Reporting template (identical for all papers/sections)
   Table 1  Quality & efficiency metrics for Baselines vs Auto-ASE
   Figure 1 Trade-off curve: FID vs wall-clock time
   Figure 2 Gate activation heat-map g_k(t)
   Table 2 Ablation results
   Appendix: energy profile & hardware counters

By adhering to this uniform strategy, each experiment will systematically attack the four key hypotheses with controlled baselines, rich metrics and clear success thresholds, allowing reviewers to judge Auto-ASE’s effectiveness from multiple, reproducible perspectives.

# Current Experiment (to generate code for)
- Experiment ID: exp-2-robust-ablation
- Description: Objective / hypothesis: Stress-test H3 (robustness/generalisation) and H4 (simplicity) across data resolutions, solvers and extreme pruning.

Models:
 • DDPM UNet-32 (CIFAR-10)
 • ADM-KD UNet-64 (ImageNet-64)
 • Stable-Diffusion-v1.5 latent UNet-512 (LSUN-bedrooms 256×256 → 512 latent) – zero-shot schedule transfer, no re-training.

Datasets & preprocessing:
 1. CIFAR-10 (as exp-1)
 2. ImageNet-64 subset of 1.28 M imgs – center-crop-resize 64×64, scale [-1,1].
 3. LSUN-bedrooms 256×256 – center-crop-resize 512×512 latent space.
Splits: official train/val/test where available; else 95 %/5 % for LSUN train/val, test withheld 10 k.

Solvers:
 • DDIM-25 steps
 • DPM-Solver++(2M)-15 steps
 • PLMS-50 steps

Run variations (evaluated on ALL models/solvers):
 1. ase-linear – fixed schedule baseline (re-tuned per resolution)
 2. auto-ase-lam0.02 – milder sparsity
 3. auto-ase-lam0.05 – default
 4. auto-ase-lam0.10 – aggressive sparsity
 5. auto-ase-70prune-corrupt – force 70 % blocks closed and corrupt noise schedule (+10 % σ) to simulate OOD.

Training protocol:
 • Fine-tune gates for 0.5 epoch on each dataset (≤5 % overhead), same optimizer.
 • For transfer runs (Stable-Diffusion) reuse gates learned on ImageNet-64, no additional updates.

Metrics:
 Quality – FID/KID/IS (images ≤256), CLIP-score (512 latent) for SD.
 Robustness – variance over 3 seeds; ΔFID distribution across solvers.
 Efficiency – as in exp-1 plus GPU memory/time on larger models.
 Calibration – ECE of predicted noise vs true noise (checks schedule accuracy).

Analysis:
 • λ sensitivity curve (Fig. 3): %blocks vs FID.
 • OOD curve (Fig. 4): corrupt-σ vs FID.
 • FLOPs vs resolution table.

Success criteria (per strategy): all Tier-3/4 runs meet thresholds in ≥75 % cases.

Example code (solver swap):
```
for solver in [DDIM, DPMSolverPP, PLMS]:
    sampler = solver(model, skip_schedule=gates)
    imgs = sampler(num_steps)
```

Branch: feature/exp-2-robust-ablation
- Run Variations: ['ase-linear', 'auto-ase-lam0.02', 'auto-ase-lam0.05', 'auto-ase-lam0.10', 'auto-ase-70prune-corrupt']

# Base Code
{'train_py': '# src/train.py\n\n"""\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n"""\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, "w") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get("optimizer", {})\n    lr = optim_cfg.get("lr", 1e-4)\n    betas = optim_cfg.get("betas", (0.9, 0.999))\n    weight_decay = optim_cfg.get("weight_decay", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get("training", {}).get("epochs", 1)\n    grad_clip = config.get("training", {}).get("grad_clip_norm", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {"train_loss": [], "val_loss": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get("training", {}).get("amp", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f"[Run {run_id}] Epoch {epoch+1}/{epochs}")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get("training", {}).get("amp", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict["loss"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({"loss": f"{loss.item():.4f}"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history["train_loss"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict["loss"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history["val_loss"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    "run_id": run_id,\n                    "epoch": epoch + 1,\n                    "train_loss": avg_train_loss,\n                    "val_loss": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get("evaluation", {}).get("compute_fid", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                "torchmetrics not installed. Please add \'torchmetrics\' to your dependencies."\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get("evaluation", {}).get("fid_num_batches", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics["fid"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics["final_train_loss"] = history["train_loss"][-1]\n    if avg_val_loss is not None:\n        metrics["final_val_loss"] = avg_val_loss\n    metrics["training_time_sec"] = training_time\n\n    results = {\n        "run_id": run_id,\n        "config": config,\n        "history": history,\n        "metrics": metrics,\n    }\n\n    save_json(results, results_dir / run_id / "results.json")\n\n    # Figures directory\n    img_dir = results_dir / "images"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use("Agg")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history["train_loss"], label="train_loss")\n    if any(v is not None for v in history["val_loss"]):\n        plt.plot(xs, history["val_loss"], label="val_loss")\n    plt.xlabel("Epoch")\n    plt.ylabel("Loss")\n    plt.title(f"Training Loss – {run_id}")\n    # Annotate final value\n    plt.annotate(\n        f"{history[\'train_loss\'][-1]:.4f}",\n        xy=(xs[-1], history["train_loss"][-1]),\n        xytext=(xs[-1], history["train_loss"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f"training_loss_{run_id}.pdf"\n    plt.savefig(out_path, bbox_inches="tight")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({"run_id": run_id, "status": "completed", "metrics": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description="Run a single experiment variation.")\n    p.add_argument("--config", type=str, required=True, help="Path to config JSON file specific to this run.")\n    p.add_argument("--results-dir", type=str, required=True, help="Root directory where outputs will be stored.")\n    p.add_argument("--run-id", type=str, required=True, help="Unique identifier for this run variation.")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, "r") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get("seed", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': '# src/evaluate.py\n"""Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use("Agg")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / "results.json"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {"run_id": res["run_id"]}\n        metrics = res.get("metrics", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x="run_id", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha="right")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f"{v:.3f}", ha="center", va="bottom")\n    plt.title(f"{metric} comparison")\n    plt.tight_layout()\n    fname = f"{metric}.pdf".replace(" ", "_")\n    plt.savefig(out_dir / fname, bbox_inches="tight")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / "images"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f"No results.json files found in {results_dir}")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != "run_id"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {"best_by_metric": {}, "figures": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith("loss"):\n            best_run = df.loc[df[metric].idxmin(), "run_id"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), "run_id"]\n        summary["best_by_metric"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description="Aggregate & compare experiment results")\n    p.add_argument("--results-dir", type=str, required=True, help="Root directory holding experiment outputs")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '# src/preprocess.py\n"""Common preprocessing pipeline with dataset placeholders.\nThe logic here is COMPLETE for the built-in "dummy" dataset used during smoke\ntests.  For real experiments, simply extend the `get_dataset` function with\nactual dataset-specific loading code.\n"""\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# ------------------------------------------------------------------------- #\n# Config-driven helpers\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    """Creates torchvision transforms based on config.\n\n    For image datasets we support optional resizing and normalisation.\n    """\n    tfms: List = []\n    resize = config.get("data", {}).get("resize")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize))\n    tfms.append(transforms.ToTensor())\n\n    # Normalisation (ImageNet stats by default)\n    if config.get("data", {}).get("normalize", True):\n        mean = config.get("data", {}).get("mean", [0.485, 0.456, 0.406])\n        std = config.get("data", {}).get("std", [0.229, 0.224, 0.225])\n        tfms.append(transforms.Normalize(mean, std))\n\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory (with placeholders for extension)                          #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    """Returns a torch.utils.data.Dataset instance.\n\n    Built-in:\n        • "dummy"  – torchvision.datasets.FakeData with tiny size (used for CI / smoke tests)\n\n    PLACEHOLDER: Extend this function with actual dataset logic, e.g. CIFAR-10,\n    ImageNet-64, LSUN, etc.  Keep the interface unchanged so the rest of the\n    pipeline remains intact.\n    """\n\n    if name == "dummy":\n        # A tiny fake dataset with 1-channel or 3-channel images depending on config.\n        image_size = config.get("data", {}).get("image_size", (3, 32, 32))\n        dataset = datasets.FakeData(\n            size=config.get("data", {}).get("num_samples", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n        return dataset\n\n    # ------------------------- PLACEHOLDER ------------------------------ #\n    # Insert real dataset paths / download logic here. For example:\n    # if name == "cifar10":\n    #     root = Path(config["data"]["root"])\n    #     return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    raise NotImplementedError(f"Dataset \'{name}\' is not implemented yet.")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get("training", {}).get("batch_size", 16)\n    num_workers = config.get("data", {}).get("num_workers", os.cpu_count() // 2)\n\n    dataset_name = config.get("dataset")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get("training", {}).get("validation_split", 0.0) > 0.0:\n        val_split = config["training"]["validation_split"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n', 'model_py': '# src/model.py\n"""Model architecture implementations.\nIncludes baseline UNet-style model plus Auto-ASE variant with learnable gates.\nThe gating logic is FULLY implemented here; swapping datasets or changing the\nunderlying block structure can be done without touching the base logic.\n"""\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Time embedding helpers (positional)\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    """From OpenAI\'s ADM code: create sinusoidal embeddings."""\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:  # zero pad\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\n# ------------------------------------------------------------------------- #\n# Gating mechanism (Auto-ASE core)\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    """Wraps an arbitrary nn.Module block with a learnable gate following Auto-ASE.\n\n    During training the gate is continuous (sigmoid).  During inference the gate\n    is binarised via the straight-through estimator (STE).\n    """\n\n    def __init__(self, block: nn.Module, t_dim: int, h_function: str = "linear"):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # gate logit parameter\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.h_function = h_function\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        # Compute gate scalar g_k(t) per sample in batch\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)  # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE at inference\n\n        # Reshape for broadcasting over feature maps\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()  # use continuous gate stat for loss\n\n\n# ------------------------------------------------------------------------- #\n# Simple UNet-like backbone (CIFAR-10 compatible, kept intentionally small)\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.activation = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.activation(self.conv1(x))\n        # Add time embedding\n        temb_broadcast = self.emb_proj(temb)[:, :, None, None]\n        h = h + temb_broadcast\n        h = self.activation(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass SimpleUNet(nn.Module):\n    """UNet with optional gating wrappers based on Auto-ASE."""\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels, gated)\n        self.down2 = self._make_block(base_channels, base_channels * 2, gated)\n        self.pool = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2, gated)\n        # Decoder\n        self.up1 = self._make_block(base_channels * 4, base_channels, gated)\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n\n        # Output layer\n        self.out_conv = nn.Conv2d(base_channels, img_channels, 1)\n\n    def _make_block(self, in_ch: int, out_ch: int, gated: bool):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    # ------------------------------------------------------------------ #\n    # Diffusion-specific helpers                                         #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gated_stats: List[torch.Tensor] = []\n\n        def apply(block, *args):\n            if isinstance(block, GatedBlock):\n                y, g_stat = block(*args, train=train)\n                gated_stats.append(g_stat)\n                return y\n            else:\n                return block(*args)\n\n        # Encoder\n        d1 = apply(self.down1, x, temb)\n        p1 = self.pool(d1)\n        d2 = apply(self.down2, p1, temb)\n        p2 = self.pool(d2)\n\n        # Bottleneck\n        bn = apply(self.bottleneck, p2, temb)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode="nearest")\n        up = torch.cat([up, d2], dim=1)\n        up = apply(self.up1, up, temb)\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gated_stats\n\n    # ------------------------ Training interface ---------------------- #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        """Implements standard DDPM noise-prediction loss + gate sparsity."""\n        device = x0.device\n        batch_size = x0.size(0)\n        config_T = 1000\n        t = torch.randint(0, config_T, (batch_size,), device=device)\n        betas = torch.linspace(1e-4, 0.02, config_T, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\n            "loss": total_loss,\n            "noise_loss": noise_loss.detach(),\n            "gate_loss": gate_reg.detach(),\n        }\n\n    # --------------------- Simple ancestral sampling ------------------- #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        """Very basic DDPM sampling loop (for evaluation) – not optimised."""\n        self.eval()\n        with torch.no_grad():\n            img_size = 32  # assume square for simplicity – can be changed later\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_ in reversed(range(T)):\n                t = torch.full((num_samples,), t_, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, train=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_ > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta) * noise\n            x = torch.clamp(x, -1.0, 1.0)\n            return x.cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Model factory                                                             #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get("model")\n    lambda_gates = config.get("diffusion", {}).get("lambda_gates", 0.05)\n    if model_name in {"dummy_baseline", "baseline_unet"}:\n        return SimpleUNet(gated=False, lambda_gate=0.0)\n    elif model_name in {"dummy_auto_ase", "auto_ase"}:\n        return SimpleUNet(gated=True, lambda_gate=lambda_gates)\n\n    # ------------------------- PLACEHOLDER -------------------------------- #\n    # Insert additional architectures (DiT, ADM-KD, Stable-Diffusion UNet etc.) here\n\n    raise ValueError(f"Unknown model name: {model_name}")\n', 'main_py': '# src/main.py\n"""Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / "src"\nCONFIG_DIR = ROOT / "config"\n\nTRAIN_MODULE = "src.train"\nEVAL_MODULE = "src.evaluate"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    """Yields lines from stream while simultaneously writing to file handles."""\n    for line in iter(stream.readline, b""):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, "w") as so, open(stderr_path, "w") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f"Sub-process {\' \'.join(cmd)} exited with code {proc.returncode}")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get("run_id")\n        if run_id is None:\n            raise ValueError("Every experiment variation must have a \'run_id\' field")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / "config.json"\n        with open(cfg_path, "w") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            "-m",\n            TRAIN_MODULE,\n            "--config",\n            str(cfg_path),\n            "--results-dir",\n            str(results_dir),\n            "--run-id",\n            run_id,\n        ]\n        print(f"\\n=== Launching run \'{run_id}\' ===")\n        run_subprocess(cmd, stdout_path=run_dir / "stdout.log", stderr_path=run_dir / "stderr.log")\n        print(f"=== Run \'{run_id}\' completed ===\\n")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, "-m", EVAL_MODULE, "--results-dir", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / "evaluate_stdout.log", stderr_path=results_dir / "evaluate_stderr.log")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description="Auto-ASE experiment orchestrator")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true", help="Run quick smoke test defined in config/smoke_test.yaml")\n    group.add_argument("--full-experiment", action="store_true", help="Run full experiment defined in config/full_experiment.yaml")\n    p.add_argument("--results-dir", type=str, required=True, help="Directory where outputs will be saved")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / ("smoke_test.yaml" if args.smoke_test else "full_experiment.yaml")\n\n    with open(cfg_file, "r") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get("experiments")\n    if not experiments:\n        raise ValueError("Configuration file must contain \'experiments\' list")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == "__main__":\n    main()\n', 'pyproject_toml': '[project]\nname = "auto_ase_experiments"\nversion = "0.1.0"\ndescription = "Common core foundation for Auto-ASE experiments"\nrequires-python = ">=3.9"\nauthors = [\n  { name="Research Team", email="research@example.com" }\n]\n\n[project.dependencies]\ntorch = ">=2.0.0"\ntorchvision = "*"\ntqdm = "*"\npyyaml = "*"\nmatplotlib = "*"\nseaborn = "*"\nnumpy = "*"\n# For FID computation\npillow = "*"\ntorchmetrics = "*"\n\n[build-system]\nrequires = ["setuptools", "wheel"]\n', 'smoke_test_yaml': '# config/smoke_test.yaml\n# This configuration runs two tiny experiments on a dummy dataset to make sure\n# the entire pipeline executes correctly. It is deliberately lightweight so it\n# can be executed in <30 seconds on CPU-only CI.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: dummy\n    model: dummy_baseline\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: dummy_auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n', 'full_experiment_yaml': '# config/full_experiment.yaml\n# PLACEHOLDER: This template will be populated in the next step with actual\n# datasets, models and hyper-parameters. The structure MUST remain identical\n# so that src.main can parse it without changes.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER-baseline\n    dataset: DATASET_PLACEHOLDER\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: DATASET_PLACEHOLDER-auto_ase\n    dataset: DATASET_PLACEHOLDER\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # Additional ablations / variants can be appended here following the same key names.\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: facebook/DiT-XL-2-256



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: uoft-cs/cifar10





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline now fully implemented for real datasets.\nSupports:\n  • dummy            – small FakeData for CI / smoke tests.\n  • cifar10          – torchvision CIFAR-10 download.\n  • cifar10_hf       – HuggingFace version (uoft-cs/cifar10).\n  • imagenet64       – HuggingFace subset (huggan/imagenet-64-32k).\n  • lsun_bedroom     – HuggingFace LSUN-bedroom subset (huggan/lsun_bedroom).\n\nAll images are converted to tensors in the range [-1,1].  Additional datasets\ncan be added by extending `get_dataset` following the same pattern.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Optional: Hugging Face datasets (installed via pyproject)\ntry:\n    from datasets import load_dataset  # type: ignore\nexcept ImportError:  # pragma: no cover\n    load_dataset = None  # Will raise later if user requests HF dataset\n\nfrom PIL import Image\n\n# ------------------------------------------------------------------------- #\n# Transform helpers                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms that output tensors in [-1, 1].\"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize, interpolation=transforms.InterpolationMode.BILINEAR))\n    tfms.extend([\n        transforms.ToTensor(),  # → [0,1]\n        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # → [-1,1]\n    ])\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# HuggingFace → PyTorch bridge                                              #\n# ------------------------------------------------------------------------- #\n\nclass HFDatasetTorch(torch.utils.data.Dataset):\n    \"\"\"Lightweight wrapper around a HuggingFace dataset that yields (tensor, 0).\"\"\"\n\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        example = self.ds[idx]\n        img = example[\"image\"]\n        if not isinstance(img, Image.Image):\n            img = Image.fromarray(img)\n        img_t = self.transform(img)\n        return img_t, 0  # dummy label so that downstream uses batch[0]\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory                                                           #\n# ------------------------------------------------------------------------- #\n\ndef _require_hf(pkg_name: str):  # pragma: no cover\n    if load_dataset is None:\n        raise ImportError(\n            f\"datasets library not installed – needed for dataset '{pkg_name}'. \"\n            \"Please install with `pip install datasets` or add to dependencies.\"\n        )\n\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\"\"\"\n\n    name = name.lower()\n    split = \"train\" if train else \"test\"\n\n    # ------------------------------------------------------------------ #\n    # 1. Dummy (for CI)\n    # ------------------------------------------------------------------ #\n    if name == \"dummy\":\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        return datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n\n    # ------------------------------------------------------------------ #\n    # 2. CIFAR-10 (torchvision)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10\":\n        root = Path(config.get(\"data\", {}).get(\"root\", \"./data\"))\n        return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    # ------------------------------------------------------------------ #\n    # 3. CIFAR-10 (HuggingFace – uoft-cs/cifar10)\n    # ------------------------------------------------------------------ #\n    if name == \"cifar10_hf\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"uoft-cs/cifar10\", split=split)\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 4. ImageNet-64 subset (huggan/imagenet-64-32k)\n    # ------------------------------------------------------------------ #\n    if name == \"imagenet64\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/imagenet-64-32k\", split=\"train\") if train else load_dataset(\n            \"huggan/imagenet-64-32k\", split=\"validation\"\n        )\n        # Optional subsampling for quick iterations\n        subset_size = config.get(\"data\", {}).get(\"subset_size\")\n        if subset_size is not None and subset_size < len(hf_ds):\n            hf_ds = hf_ds.shuffle(seed=42).select(range(subset_size))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # 5. LSUN-bedroom (huggan/lsun_bedroom)\n    # ------------------------------------------------------------------ #\n    if name == \"lsun_bedroom\":\n        _require_hf(name)\n        hf_ds = load_dataset(\"huggan/lsun_bedroom\", split=\"train\")\n        if not train:\n            # Use last 10k images as a pseudo-test set\n            hf_ds = hf_ds.select(range(-10000, 0))\n        return HFDatasetTorch(hf_ds, get_transforms(config))\n\n    # ------------------------------------------------------------------ #\n    # Unknown dataset\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented.\")\n\n\n# ------------------------------------------------------------------------- #\n# DataLoader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", max(1, os.cpu_count() // 2))\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
    "model_py": "# src/model.py\n\"\"\"Model architecture implementations (baseline + Auto-ASE variants).\n\nImplemented architectures:\n  • unet32              – CIFAR-10 (32×32)\n  • unet64              – ImageNet-64 (64×64)\n  • unet512_latent      – Stable-Diffusion latent UNet (64×64 latent, 512 px images)\nEach architecture is built from the same SimpleUNet template but with different\ncapacity.  Auto-ASE gating is available through the `lambda_gates` parameter:\n    • lambda_gates == 0.0   → no gates (baseline / ASE-linear)\n    • lambda_gates  > 0.0   → gates are active and regularised.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport re\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# 1.  Positional timestep embeddings                                       #\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"Creates sinusoidal timestep embeddings (as in ADM/DDPM code).\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n    return emb\n\n\n# ------------------------------------------------------------------------- #\n# 2.  Auto-ASE Gated wrapper                                               #\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an nn.Module and applies a learnable gate g_k(t).\n\n    During training gates are soft (sigmoid).  During evaluation they are\n    binarised via a straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initialise at 0 → gate≈0.5\n        self.t_proj = nn.Linear(t_dim, 1)\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)       # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()\n\n\n# ------------------------------------------------------------------------- #\n# 3.  Building blocks                                                      #\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    \"\"\"Two-conv residual block with timestep conditioning.\"\"\"\n\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.act = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.act(self.conv1(x))\n        h = h + self.emb_proj(temb)[:, :, None, None]\n        h = self.act(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\n# ------------------------------------------------------------------------- #\n# 4.  Simple UNet backbone                                                 #\n# ------------------------------------------------------------------------- #\n\nclass SimpleUNet(nn.Module):\n    def __init__(\n        self,\n        img_channels: int,\n        base_channels: int,\n        image_size: int,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n        num_timesteps: int = 1000,\n        beta_schedule: str = \"linear\",\n        noise_scale: float = 1.0,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n        self.num_timesteps = num_timesteps\n        self.beta_schedule = beta_schedule\n        self.noise_scale = noise_scale\n        self.image_size = image_size\n\n        # time embedding MLP\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels)\n        self.pool1 = nn.AvgPool2d(2)\n        self.down2 = self._make_block(base_channels, base_channels * 2)\n        self.pool2 = nn.AvgPool2d(2)\n\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2)\n\n        # Decoder\n        self.up1 = self._make_block(base_channels * 2 + base_channels * 2, base_channels)\n        # Final conv\n        self.out_conv = nn.Conv2d(base_channels + base_channels, img_channels, 1)\n\n    # ------------------------------------------------------------------ #\n    # internal helpers                                                   #\n    # ------------------------------------------------------------------ #\n    def _make_block(self, in_ch: int, out_ch: int):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if self.gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    def _apply_block(self, block, x, temb, train: bool, stats: List):\n        if isinstance(block, GatedBlock):\n            y, stat = block(x, temb, train=train)\n            stats.append(stat)\n            return y\n        else:\n            return block(x, temb)\n\n    # ------------------------------------------------------------------ #\n    # Forward / sampling                                                 #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n        gate_stats: List[torch.Tensor] = []\n\n        # Encoder\n        d1 = self._apply_block(self.down1, x, temb, train, gate_stats)\n        p1 = self.pool1(d1)\n        d2 = self._apply_block(self.down2, p1, temb, train, gate_stats)\n        p2 = self.pool2(d2)\n\n        # Bottleneck\n        bn = self._apply_block(self.bottleneck, p2, temb, train, gate_stats)\n\n        # Decoder step 1 (upsample + concat with d2)\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = self._apply_block(self.up1, up, temb, train, gate_stats)\n\n        # Final upsample, concat with d1 and project to image\n        up = F.interpolate(up, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gate_stats\n\n    # ------------------------------------------------------------------ #\n    # Training step                                                     #\n    # ------------------------------------------------------------------ #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        device = x0.device\n        B = x0.size(0)\n        t = torch.randint(0, self.num_timesteps, (B,), device=device)\n\n        # Linear beta schedule (only schedule currently supported)\n        betas = torch.linspace(1e-4, 0.02, self.num_timesteps, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0) * self.noise_scale\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\"loss\": total_loss, \"noise_loss\": noise_loss.detach(), \"gate_loss\": gate_reg.detach()}\n\n    # ------------------------------------------------------------------ #\n    # Simple ancestral sampling (for FID)                                 #\n    # ------------------------------------------------------------------ #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            x = torch.randn(num_samples, 3, self.image_size, self.image_size, device=device)\n            T = 100  # shorter sampling for speed during evaluation\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_idx in reversed(range(T)):\n                t_batch = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t_batch, train=False)\n                alpha_bar_t = alpha_bars[t_batch][:, None, None, None]\n                beta_t = betas[t_batch][:, None, None, None]\n                coef1 = 1 / torch.sqrt(alphas[t_batch][:, None, None, None])\n                coef2 = beta_t / torch.sqrt(1 - alpha_bar_t)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_idx > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta_t) * noise\n            return torch.clamp(x, -1.0, 1.0).cpu()\n\n\n# ------------------------------------------------------------------------- #\n# 5.  Model factory                                                        #\n# ------------------------------------------------------------------------- #\n\n_DEF_ARCH = {\n    \"unet32\": {\"img_size\": 32, \"base_channels\": 64},\n    \"unet64\": {\"img_size\": 64, \"base_channels\": 128},\n    \"unet512_latent\": {\"img_size\": 64, \"base_channels\": 256},  # latent 64×64\n}\n\n\ndef get_model(config: dict) -> nn.Module:\n    name = config.get(\"model\").lower()\n    diff_cfg = config.get(\"diffusion\", {})\n\n    # Identify architecture key (substring match)\n    arch_key = None\n    for k in _DEF_ARCH.keys():\n        if name.startswith(k):\n            arch_key = k\n            break\n    if arch_key is None:\n        raise ValueError(f\"Unknown/unsupported model architecture in name '{name}'.\")\n\n    gated = diff_cfg.get(\"lambda_gates\", 0.0) > 0.0\n    arch = _DEF_ARCH[arch_key]\n\n    return SimpleUNet(\n        img_channels=3,\n        base_channels=arch[\"base_channels\"],\n        image_size=arch[\"img_size\"],\n        gated=gated,\n        lambda_gate=diff_cfg.get(\"lambda_gates\", 0.0),\n        num_timesteps=diff_cfg.get(\"timesteps\", 1000),\n        beta_schedule=diff_cfg.get(\"beta_schedule\", \"linear\"),\n        noise_scale=diff_cfg.get(\"corrupt_sigma\", 1.0),\n    )\n",
    "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ndatasets = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight integrity test covering all five run variations on a dummy dataset.\n\nexperiments:\n  - run_id: dummy-ase-linear\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.02\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.05\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-lam0.10\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy-auto-ase-70prune-corrupt\n    dataset: dummy\n    model: unet32\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 8\n      amp: False\n    diffusion:\n      timesteps: 50\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: False\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete experiment matrix for exp-2-robust-ablation (three datasets × five\n# schedule variants).\n\nexperiments:\n  # ---------------------------------------------------------------------\n  # CIFAR-10 32×32 (UNet32)\n  # ---------------------------------------------------------------------\n  - run_id: cifar10-ase-linear\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.02\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.05\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-lam0.10\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: cifar10-auto-ase-70prune-corrupt\n    dataset: cifar10_hf\n    model: unet32\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # ImageNet-64 (UNet64)\n  # ---------------------------------------------------------------------\n  - run_id: imagenet64-ase-linear\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.02\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.05\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-lam0.10\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: imagenet64-auto-ase-70prune-corrupt\n    dataset: imagenet64\n    model: unet64\n    seed: 43\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # ---------------------------------------------------------------------\n  # LSUN-Bedroom / Stable-Diffusion latent UNet (UNet512_latent)\n  # ---------------------------------------------------------------------\n  - run_id: lsun-ase-linear\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.02\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.02\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.05\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-lam0.10\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.10\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n\n  - run_id: lsun-auto-ase-70prune-corrupt\n    dataset: lsun_bedroom\n    model: unet512_latent\n    seed: 44\n    training:\n      epochs: 1\n      batch_size: 64\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n      corrupt_sigma: 1.1\n      force_prune_perc: 0.7\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 20\n"
}
