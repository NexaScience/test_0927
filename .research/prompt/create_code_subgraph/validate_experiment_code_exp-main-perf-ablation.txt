
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "ρBYOL and most contrastive/self-distillation methods pick two random temporal crops from the same clip and force their representations to match equally, no matter whether the crops are 1 frame or 3 s apart. On rapidly changing rodent behaviour this causes:\n1) Positive pairs that are too far apart in time to depict the same action, blurring fine-grained motion cues.\n2) Weak ordering of the latent space with respect to temporal distance, which hurts downstream behaviour segmentation and rare-action recognition.\nA fix that does not require new architectures is to make the objective aware of the temporal distance between the two views.",
    "Methods": "Time-Weighted BYOL (TW-BYOL)\nMinimal change: multiply BYOL’s mean-squared error loss between the online and target projections by an exponential weight that decays with frame distance Δt between the two augmented views.\nL_TW = w(Δt) * || q( f_θ( v_i ) ) – stop_grad( f_ϕ( v_j ) ) ||² ,  \nwith w(Δt)=exp(-Δt / τ) and a single hyper-parameter τ (e.g. 30 frames).\nTheoretical motivation: For small Δt the two views are likely to be the same action, so the model should align them strongly; for large Δt alignment is weakened, allowing the network to separate different actions and preserve temporal information. The weighting acts as a soft regulariser imposing temporal locality without introducing extra negatives or additional networks.",
    "Experimental Setup": "Dataset: MABe22 mice-triplet videos (video modality only).\nTraining: reproduce original ρBYOL recipe (SlowFast-8×8 backbone, same augmentations) and TW-BYOL (identical but with the weighted loss).\nHyper-parameter sweep: τ ∈ {15, 30, 60} frames.\nEvaluation: linear probe F1 on the 8 hidden downstream mice tasks; report mean and per-task scores.\nBaseline: official ρBYOL numbers from MABe22 leaderboard.\nCompute: 4×A100 GPUs, 50 epochs (same as baseline).",
    "Experimental Code": "# only loss change shown\nimport torch, math\n\ndef time_weighted_byol_loss(p_online, z_target, frame_dist, tau=30.):\n    \"\"\"p_online and z_target: (B,D) tensors; frame_dist: (B,) ints\"\"\"\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\n    loss = (p_online - z_target.detach()).pow(2).sum(dim=1)  # (B,)\n    weighted_loss = (weight * loss).mean()\n    return weighted_loss\n\n# inside training loop\np = projector_online(backbone_online(view_i))      # (B,D)\nz = projector_target(backbone_target(view_j))      # (B,D)\nframe_dist = torch.abs(frame_idx_i - frame_idx_j)  # (B,)\nloss = time_weighted_byol_loss(p, z, frame_dist, tau=30)",
    "Expected Result": "Across three random seeds the mean F1 on the mice tasks is expected to rise by ~2-3 points (e.g. ρBYOL 68.5 → TW-BYOL 71.0). Improvements should be largest on tasks requiring discrimination of short, bursty actions (e.g. grooming vs sniffing) while remaining neutral on slowly varying experimental condition classification.",
    "Expected Conclusion": "A single, two-line weighting term makes the self-distillation objective respect temporal locality, sharpening representations for fast rodent behaviours. Because no extra negatives, memory, or architectural changes are introduced, the method keeps BYOL’s simplicity and training speed while delivering measurable accuracy gains on fine-grained behaviour annotation tasks."
}

# Experimental Design
## Experiment Strategy
Global Goal
Prove that the proposed Time-Weighted BYOL (TW-BYOL) yields temporally better-ordered, more behaviour-discriminative and equally efficient video representations than existing self-supervised alternatives, while remaining robust to hyper-parameter choices and generalising across rodent datasets.

1. Core Hypotheses to Validate
   H1 – Performance: TW-BYOL improves downstream behaviour recognition (overall F1, rare-action recall, few-shot transfer).
   H2 – Temporal Awareness: embeddings respect temporal proximity (distance in latent space grows with frame gap).
   H3 – Efficiency: training speed, GPU memory and wall-clock cost stay within ±5 % of ρBYOL.
   H4 – Robustness: gains hold under different τ values, random seeds, crop strategies and limited labelled data.
   H5 – Generalisation: improvements transfer to unseen rodents/tasks and to a second behaviour dataset.

2. Comparative Framework
   a. Baseline: reproduced ρBYOL recipe.
   b. State-of-the-art self-supervised video baselines: MoCo-v3, SimCLR-v2, TimeContrast.
   c. Supervised upper bound: same backbone trained with full labels (for context only).
   d. Ablations:
      • No weighting (ρBYOL loss) – “Uniform”.
      • Hard cut-off weighting – “Binary”.
      • Alternative decays (linear, inverse square) to test the importance of exponential form.
      • τ sweep (15, 30, 60 frames).

3. Experimental Angles
   3.1 Quantitative Performance
       • Linear-probe F1 per task and averaged (primary metric).
       • k-NN accuracy (label-free evaluation of representation quality).
       • Rare-action recall (top-20 % least frequent labels).
   3.2 Temporal Locality Analysis
       • Spearman correlation ρ between embedding distance and frame gap Δt.
       • Temporal retrieval: mean reciprocal rank when querying a frame for its 5 nearest temporal neighbours.
   3.3 Efficiency Metrics
       • GPU hours per pre-training run.
       • Samples / sec and peak VRAM.
   3.4 Qualitative
       • t-SNE / UMAP plots coloured by action and by timestamp.
       • Video retrieval demos.
   3.5 Robustness & Generalisation
       • Sensitivity curves over τ and crop policies.
       • Subset-of-data training (25 %, 50 % of unlabelled video) to test data efficiency.
       • Cross-dataset transfer: pre-train on MABe22, evaluate on a second rodent-behaviour set (e.g., RatSI).

4. Validation Criteria for Success
   Pass if ALL are met:
   • +2 F1 absolute (≥ p<0.05, paired t-test over 3 seeds) versus ρBYOL on mean of 8 tasks.
   • At least 6/8 tasks individually improve or remain equal.
   • Embedding–time correlation improves by ≥10 % over ρBYOL.
   • GPU hours increase ≤5 %.
   • Variance of F1 across seeds not higher than ρBYOL.
   • Improvements persist (≥75 % retained) when τ∈[15,60] or when only 50 % of unlabelled video is available.

5. Experimental Protocol
   • Hardware: up to 4×A100 80 GB per run; mixed-precision training; identical data-loading pipeline for all methods.
   • Controlled compute: fix batch size, epochs (50), optimiser and augmentation suite; record seeds.
   • Run each configuration 3× for statistics.
   • Hyper-parameter grid executed with identical wall-clock budget; schedule runs via SLURM to exploit 2 TB RAM node.
   • Evaluation code placed in a separate repo; blind-test labels kept hidden until final metrics are logged to ensure fairness.

6. Multi-Perspective Demonstration Strategy
   a. Start with baseline vs TW-BYOL to establish headline gains.
   b. Add ablation study to attribute gains specifically to exponential weighting and to choice of τ.
   c. Compare against external SOTA to position method in field.
   d. Present temporal locality analyses to back mechanistic claim.
   e. Provide efficiency table to show “no free lunch” avoided.
   f. Show robustness curves and cross-dataset transfer to argue for broad applicability.

This unified strategy will be executed for every subsequent experiment, ensuring that each study supplies comparable evidence along performance, temporal fidelity, efficiency, robustness and generalisation axes while sharing compute budgets and evaluation protocols across the research programme.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"\nevaluate.py \u2013 Aggregate \u0026 compare results of the run variations.\nReads all sub-directories in --results-dir that contain results.json, compiles\ncomparison tables \u0026 figures and writes them to stdout + images/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_TOPIC_FINAL_LOSS = \"final_loss\"\n\n################################################################################\n# -----------------------------  utilities  -----------------------------------#\n################################################################################\n\ndef collect_results(results_dir: Path) -\u003e List[Dict]:\n    records = []\n    for run_dir in results_dir.iterdir():\n        file = run_dir / \"results.json\"\n        if file.exists():\n            with open(file, \"r\", encoding=\"utf-8\") as fp:\n                records.append(json.load(fp))\n    return records\n\n################################################################################\n# --------------------------  figure helpers  ---------------------------------#\n################################################################################\n\ndef plot_final_loss(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(data=df, x=\"run_id\", y=\"final_val_loss\", hue=\"algorithm\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Loss\")\n    ax.set_title(\"Final Validation Loss Across Experiments\")\n\n    # annotate each bar\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=8)\n\n    plt.tight_layout()\n    fname = f\"{FIG_TOPIC_FINAL_LOSS}.pdf\"\n    images_dir = out_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n    plt.savefig(images_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef main(results_dir: Path):\n    records = collect_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n\n    df = pd.DataFrame(records)\n    # ---------------------------------------------------------------- figures\n    fig_files = []\n    fig_files.append(plot_final_loss(df, results_dir))\n\n    # --------------------------------------------------------- stdout outputs\n    comparison = {\n        \"num_runs\": len(records),\n        \"best_final_val_loss\": df[\"final_val_loss\"].min(),\n        \"worst_final_val_loss\": df[\"final_val_loss\"].max(),\n        \"figure_files\": fig_files,\n    }\n\n    print(\"\\n===== Cross-Run Comparison Summary =====\")\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(Path(args.results_dir))", "full_experiment_yaml": "# Full experiment configuration \u2013 place only PLACEHOLDERs here.\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: |\n      # PLACEHOLDER: Replace with actual experiment description\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      root: DATASET_PATH_PLACEHOLDER\n      params: {}\n    model:\n      type: MODEL_PLACEHOLDER\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: ALGORITHM_PLACEHOLDER  # e.g., BYOL, TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 50  # PLACEHOLDER: override if needed\n      batch_size: 64\n      learning_rate: 1e-3\n  # Add additional experiment blocks as needed\n", "main_py": "\"\"\"\nmain.py \u2013 Experiment orchestrator.\nReads smoke_test.yaml or full_experiment.yaml, spawns src/train.py sequentially\nfor each run variation, captures logs, and finally launches src/evaluate.py.\n\"\"\"\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n################################################################################\n# -----------------------------  log helpers  ---------------------------------#\n################################################################################\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run *cmd* while tee-ing stdout / stderr to the given files + parent console.\"\"\"\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as out_fp, open(stderr_path, \"w\", encoding=\"utf-8\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                sys.stdout.write(stdout_line)\n                out_fp.write(stdout_line)\n            if stderr_line:\n                sys.stderr.write(stderr_line)\n                err_fp.write(stderr_line)\n            if stdout_line == \"\" and stderr_line == \"\" and process.poll() is not None:\n                break\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n################################################################################\n# -----------------------------  orchestrator  --------------------------------#\n################################################################################\n\ndef load_yaml(path: Path) -\u003e Dict:\n    with open(path, \"r\", encoding=\"utf-8\") as fp:\n        return yaml.safe_load(fp)\n\n\ndef create_temp_run_config(run_cfg: Dict) -\u003e Path:\n    \"\"\"Write *run_cfg* to a NamedTemporaryFile and return its path.\"\"\"\n    tmp = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\")\n    json.dump(run_cfg, tmp)\n    tmp.flush()\n    return Path(tmp.name)\n\n\ndef main(args):\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    exp_cfg = load_yaml(cfg_path)\n\n    runs = exp_cfg.get(\"experiments\", [])\n    if not runs:\n        print(f\"No experiments defined in {cfg_path}\")\n        sys.exit(1)\n\n    for run in runs:\n        run_id = run[\"run_id\"]\n        print(f\"\\n=== Launching run: {run_id} ===\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        temp_cfg_path = create_temp_run_config(run)\n        cmd = [\n            sys.executable, \"-m\", \"src.train\",\n            \"--run-config\", str(temp_cfg_path),\n            \"--results-dir\", str(results_dir),\n        ]\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # ------------------------------------------------- post-hoc evaluation\n    print(\"\\n===== All runs finished, starting evaluation =====\")\n    eval_cmd = [\n        sys.executable, \"-m\", \"src.evaluate\",\n        \"--results-dir\", str(results_dir),\n    ]\n    tee_subprocess(\n        eval_cmd,\n        results_dir / \"evaluation_stdout.log\",\n        results_dir / \"evaluation_stderr.log\",\n    )\n\n\n################################################################################\n# --------------------------------  CLI  --------------------------------------#\n################################################################################\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run full experimental pipeline.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results are stored.\")\n\n    args_parsed = parser.parse_args()\n    main(args_parsed)", "model_py": "\"\"\"\nmodel.py \u2013 Model architectures \u0026 BYOL utilities common to all experiments.\nThis file contains the COMPLETE implementation of BYOL \u0026 TW-BYOL algorithms\nexcept for dataset-specific modules.\n\"\"\"\nfrom typing import Tuple\nimport copy\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models as tv_models\n\n################################################################################\n# --------------------------   projection heads   -----------------------------#\n################################################################################\n\nclass MLPHead(nn.Module):\n    \"\"\"2-layer MLP projection/prediction head for BYOL.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 4096, out_dim: int = 256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n################################################################################\n# ------------------------------  backbones  ----------------------------------#\n################################################################################\n\n\ndef build_backbone_and_heads(model_cfg: dict):\n    \"\"\"Returns (backbone, projector, predictor) all as nn.Module.\n\n    backbone is WITHOUT final classification layer.\n    \"\"\"\n    model_type = model_cfg[\"type\"].lower()\n    if model_type == \"resnet18\":\n        backbone = tv_models.resnet18(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"resnet50\":\n        backbone = tv_models.resnet50(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"model_placeholder\":  # PLACEHOLDER for specialised video backbone\n        # PLACEHOLDER: insert actual backbone creation logic\n        raise NotImplementedError(\"MODEL_PLACEHOLDER needs to be replaced with real model.\")\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n    # projector \u0026 predictor\n    proj_hidden = model_cfg.get(\"proj_hidden_dim\", 4096)\n    proj_out = model_cfg.get(\"proj_output_dim\", 256)\n    predictor_hidden = model_cfg.get(\"predictor_hidden_dim\", 4096)\n\n    projector = MLPHead(feat_dim, proj_hidden, proj_out)\n    predictor = MLPHead(proj_out, predictor_hidden, proj_out)\n    return backbone, projector, predictor\n\n\n################################################################################\n# -------------------------------  BYOL  --------------------------------------#\n################################################################################\n\nclass BYOL(nn.Module):\n    \"\"\"Minimal BYOL implementation supporting TW-BYOL loss computation.\"\"\"\n\n    def __init__(self, backbone: nn.Module, projector: nn.Module, predictor: nn.Module,\n                 moving_average_decay: float = 0.996):\n        super().__init__()\n        self.online_backbone = backbone\n        self.online_projector = projector\n        self.predictor = predictor\n\n        # create target encoder as EMA copy\n        self.target_backbone = copy.deepcopy(backbone)\n        self.target_projector = copy.deepcopy(projector)\n        for p in self.target_backbone.parameters():\n            p.requires_grad = False\n        for p in self.target_projector.parameters():\n            p.requires_grad = False\n\n        self.moving_average_decay = moving_average_decay\n\n    @torch.no_grad()\n    def update_target_network(self):\n        \"\"\"EMA update of target network.\"\"\"\n        self._update_moving_average(self.online_backbone, self.target_backbone)\n        self._update_moving_average(self.online_projector, self.target_projector)\n\n    @torch.no_grad()\n    def _update_moving_average(self, online: nn.Module, target: nn.Module):\n        for p_o, p_t in zip(online.parameters(), target.parameters()):\n            p_t.data = p_t.data * self.moving_average_decay + p_o.data * (1.0 - self.moving_average_decay)\n\n    def forward(self, view1, view2):\n        # Online network on view1\n        o1 = self.online_backbone(view1)\n        p1 = self.online_projector(o1)\n        p_online = self.predictor(p1)\n\n        # Target network (no grad) on view2\n        with torch.no_grad():\n            t2 = self.target_backbone(view2)\n            z_target = self.target_projector(t2).detach()\n        return p_online, z_target\n\n\n################################################################################\n# ----------------------------  loss functions  -------------------------------#\n################################################################################\n\ndef byol_loss(p_online: torch.Tensor, z_target: torch.Tensor) -\u003e torch.Tensor:\n    return F.mse_loss(p_online, z_target)\n\n\ndef time_weighted_byol_loss(p_online: torch.Tensor, z_target: torch.Tensor,\n                            frame_dist: torch.Tensor, tau: float = 30.0) -\u003e torch.Tensor:\n    \"\"\"Time-weighted BYOL loss as described in the methodology.\"\"\"\n    if frame_dist is None:\n        raise RuntimeError(\"frame_dist tensor is required for TW-BYOL.\")\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)\n    per_sample_loss = (p_online - z_target).pow(2).sum(dim=1)\n    return (weight * per_sample_loss).mean()\n", "preprocess_py": "\"\"\"\npreprocess.py \u2013 Common dataset loading \u0026 preprocessing utilities.\nAll dataset-specific logic is FORBIDDEN in this foundation layer and therefore\nplaced behind explicit placeholders.\n\"\"\"\nfrom typing import Dict\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\n\n# =============================================================================\n# Placeholders that WILL be replaced in later stages\n# =============================================================================\nclass DatasetPlaceholder(Dataset):\n    \"\"\"PLACEHOLDER: Replace with actual dataset implementation.\n\n    The dataset must return a dict with keys:\n        - \u0027view1\u0027: Tensor\n        - \u0027view2\u0027: Tensor\n        - \u0027frame_dist\u0027: Tensor or int (optional, required for TW-BYOL)\n    \"\"\"\n\n    def __init__(self, root: Path, split: str, transform=None, **kwargs):\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.transform = transform\n        self.data = []  # PLACEHOLDER: populate with actual data indices\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # PLACEHOLDER: implement real loading \u0026 augmentation\n        dummy = torch.randn(3, 224, 224)\n        if self.transform:\n            dummy = self.transform(dummy)\n        sample = {\n            \"view1\": dummy,\n            \"view2\": dummy.clone(),\n            \"frame_dist\": torch.tensor(0),\n        }\n        return sample\n\n\n# =============================================================================\n# Public API\n# =============================================================================\n\ndef get_transforms(train: bool = True, cfg: Dict = None):\n    cfg = cfg or {}\n    if train:\n        # basic augmentation pipeline (can be overridden)\n        return T.Compose([\n            T.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n        ])\n    else:\n        return T.Compose([\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n        ])\n\n\ndef get_dataset(cfg: Dict, split: str):\n    \"\"\"Factory that returns correct dataset instance.\n\n    cfg: The \u0027dataset\u0027 section of the run configuration.\n    split: \u0027train\u0027, \u0027val\u0027, or \u0027test\u0027\n    \"\"\"\n    name = cfg[\"name\"].lower()\n    root = Path(cfg.get(\"root\", \"DATASET_ROOT_PLACEHOLDER\"))  # PLACEHOLDER path\n    params = cfg.get(\"params\", {})\n    transform = get_transforms(train=(split == \"train\"), cfg=params.get(\"transforms\"))\n\n    if name == \"dataset_placeholder\":\n        return DatasetPlaceholder(root, split, transform=transform, **params)\n    else:\n        raise ValueError(\n            f\"Dataset \u0027{name}\u0027 not recognised. \"\n            \"# PLACEHOLDER: register dataset in preprocess.get_dataset().\"\n        )", "pyproject_toml": "[project]\nname = \"tw_byol_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common foundation for BYOL / TW-BYOL experimental variations\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntorchvision = \"\u003e=0.15.0\"\npyyaml = \"^6.0\"\ntqdm = \"^4.66.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\nscikit-learn = \"^1.3.0\"\npandas = \"^2.1.0\"\n", "smoke_test_yaml": "# Smoke test configuration \u2013 intentionally small for CI / GitHub Actions\nexperiments:\n  - run_id: smoke_baseline\n    description: |\n      Smoke test baseline BYOL using placeholder dataset \u0026 ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: replace with actual dataset name\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n\n  - run_id: smoke_tw_byol\n    description: |\n      Smoke test TW-BYOL (tau=30) using placeholder dataset \u0026 ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n", "train_py": "\"\"\"\ntrain.py \u2013 Train a single self-supervised run variation (BYOL / TW-BYOL, etc.)\nThe script is launched ONLY by src/main.py. It therefore assumes that all CLI\narguments originate from main.py and are validated there.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport time\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom src import preprocess as pp\nfrom src import model as models\n\n################################################################################\n# ------------------------------   helpers   ----------------------------------#\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -\u003e torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n################################################################################\n# ------------------------------   training   ---------------------------------#\n################################################################################\n\ndef byol_step(batch: Dict[str, torch.Tensor], learner, optimizer, scaler, config):\n    \"\"\"One optimisation step for BYOL/TW-BYOL.\n\n    Args\n    ----\n    batch : Dict \u2013 must have keys \u0027view1\u0027, \u0027view2\u0027, \u0027frame_dist\u0027 (frame_dist optional)\n    learner : models.BYOL \u2013 model wrapper that returns p_online \u0026 z_target\n    optimizer : torch Optimizer\n    scaler : GradScaler or None\n    config : dict \u2013 algorithm section of YAML\n    \"\"\"\n    view1 = batch[\"view1\"].to(get_device(), non_blocking=True)\n    view2 = batch[\"view2\"].to(get_device(), non_blocking=True)\n    frame_dist = batch.get(\"frame_dist\")  # may be None for ordinary BYOL\n    if frame_dist is not None:\n        frame_dist = frame_dist.to(get_device(), non_blocking=True)\n\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.cuda.amp.autocast(enabled=config.get(\"mixed_precision\", True)):\n        p_online, z_target = learner(view1, view2)\n        if config[\"type\"].lower() == \"tw-byol\":\n            tau = config[\"params\"].get(\"tau\", 30.0)\n            loss = models.time_weighted_byol_loss(\n                p_online, z_target, frame_dist=frame_dist, tau=tau\n            )\n        else:  # ordinary BYOL\n            loss = models.byol_loss(p_online, z_target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    learner.update_target_network()\n    return loss.item()\n\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef run_training(cfg: Dict, results_dir: Path):\n    description = cfg.get(\"description\", \"No description provided.\")\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------ paths\n    run_dir = results_dir / run_id\n    images_dir = run_dir / \"images\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------- device\n    device = get_device()\n\n    # --------------------------------------------------------- dataset / dataloader\n    dataset_cfg = cfg[\"dataset\"]\n    train_ds = pp.get_dataset(dataset_cfg, split=\"train\")\n    val_ds = pp.get_dataset(dataset_cfg, split=\"val\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        shuffle=True,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg[\"training\"].get(\"val_batch_size\", cfg[\"training\"][\"batch_size\"]),\n        shuffle=False,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n\n    # ------------------------------------------------------------- model / opt\n    model_cfg = cfg[\"model\"]\n    algorithm_cfg = cfg[\"algorithm\"]\n\n    online_backbone, projector, predictor = models.build_backbone_and_heads(model_cfg)\n    learner = models.BYOL(\n        backbone=online_backbone,\n        projector=projector,\n        predictor=predictor,\n        moving_average_decay=algorithm_cfg.get(\"ema_decay\", 0.996),\n    ).to(device)\n\n    optimizer = optim.Adam(\n        learner.parameters(), lr=cfg[\"training\"][\"learning_rate\"], weight_decay=1e-6\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=algorithm_cfg.get(\"mixed_precision\", True))\n\n    # ------------------------------------------------------------- training loop\n    epochs = cfg[\"training\"][\"epochs\"]\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"time_sec\": [],\n    }\n\n    best_val_loss = float(\"inf\")\n    start_time_total = time.time()\n    for epoch in range(1, epochs + 1):\n        learner.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            loss_val = byol_step(batch, learner, optimizer, scaler, algorithm_cfg)\n            train_losses.append(loss_val)\n            pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n        # ---------------- validation (BYOL self-supervised loss on val set)\n        learner.eval()\n        with torch.no_grad():\n            val_losses = []\n            for batch in val_loader:\n                view1 = batch[\"view1\"].to(device, non_blocking=True)\n                view2 = batch[\"view2\"].to(device, non_blocking=True)\n                frame_dist = batch.get(\"frame_dist\")\n                if frame_dist is not None:\n                    frame_dist = frame_dist.to(device, non_blocking=True)\n\n                with torch.cuda.amp.autocast(enabled=algorithm_cfg.get(\"mixed_precision\", True)):\n                    p_online, z_target = learner(view1, view2)\n                    if algorithm_cfg[\"type\"].lower() == \"tw-byol\":\n                        tau = algorithm_cfg[\"params\"].get(\"tau\", 30.0)\n                        val_loss_val = models.time_weighted_byol_loss(\n                            p_online, z_target, frame_dist=frame_dist, tau=tau\n                        ).item()\n                    else:\n                        val_loss_val = models.byol_loss(p_online, z_target).item()\n                val_losses.append(val_loss_val)\n\n        mean_train_loss = float(np.mean(train_losses))\n        mean_val_loss = float(np.mean(val_losses))\n        epoch_time = time.time() - start_time_total\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(mean_train_loss)\n        history[\"val_loss\"].append(mean_val_loss)\n        history[\"time_sec\"].append(epoch_time)\n\n        # Save best model checkpoint\n        if mean_val_loss \u003c best_val_loss:\n            best_val_loss = mean_val_loss\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save({\"epoch\": epoch, \"state_dict\": learner.state_dict()}, ckpt_path)\n\n        # Epoch-level JSON logging (append-safe)\n        with open(run_dir / \"epoch_metrics.jsonl\", \"a\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps({\n                \"epoch\": epoch,\n                \"train_loss\": mean_train_loss,\n                \"val_loss\": mean_val_loss,\n                \"time_sec\": epoch_time,\n            }) + \"\\n\")\n\n    total_time = time.time() - start_time_total\n\n    # --------------------------------------------------------- save figures\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set(style=\"whitegrid\")\n\n    # Training \u0026 validation loss curve\n    plt.figure(figsize=(8, 4))\n    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss \u2013 {run_id}\")\n    # Annotate final value\n    plt.annotate(f\"{history[\u0027val_loss\u0027][-1]:.4f}\",\n                 xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n                 xytext=(5, -10), textcoords=\u0027offset points\u0027)\n    plt.legend()\n    plt.tight_layout()\n    fig_name = f\"training_loss_{run_id}.pdf\"\n    plt.savefig(images_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------------------------------------------------- final results\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm\": algorithm_cfg[\"type\"],\n        \"dataset\": dataset_cfg[\"name\"],\n        \"model\": model_cfg[\"type\"],\n        \"epochs\": epochs,\n        \"best_val_loss\": best_val_loss,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"total_time_sec\": total_time,\n        \"figure_files\": [fig_name],\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # ----------------------------------------------------- stdout requirements\n    print(\"\\n===== Experiment Description =====\")\n    print(description)\n    print(\"===== Numerical Results (JSON) =====\")\n    print(json.dumps(results))\n\n\n################################################################################\n# ------------------------------   CLI   --------------------------------------#\n################################################################################\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True,\n                        help=\"Path to JSON or YAML file with a SINGLE run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where outputs will be written.\")\n    return parser.parse_args()\n\n\ndef load_run_config(path: str) -\u003e Dict:\n    path = Path(path)\n    if path.suffix in {\".yaml\", \".yml\"}:\n        import yaml\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = yaml.safe_load(fp)\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = json.load(fp)\n    return cfg\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = load_run_config(args.run_config)\n    run_training(cfg, Path(args.results_dir))"}

# Current Experiment (To be validated)
- Experiment ID: exp-main-perf-ablation
- Description: Objective / Hypothesis: Quantitatively demonstrate that the proposed exponential time–weighted loss (TW-BYOL) yields higher behaviour-recognition performance and stronger temporal ordering than the original ρBYOL, and that these gains are attributable to the weighting scheme rather than chance. The experiment also positions TW-BYOL against a strong non-BYOL baseline (MoCo-v3) while keeping backbone, data pipeline and compute identical.

Models
• SlowFast-8×8 backbone for all BYOL variants.  
• ResNet-50 + MoCo-v3 (image→video adaptation via clip-level averaging) for an external SSL baseline.

Datasets
• MABe22 mice-triplet videos (RGB, 1024×512). 30-frame clips at 15 fps.  
Data split: official train/val/test (70 / 15 / 15 %). Cross-validation not used to keep GPU budget manageable.

Pre-processing
• Random spatial crop (80–100 %), horizontal flip, colour jitter, grayscale, Gaussian blur.  
• Two temporal crops: sample starting indices i,j; frame gap Δt recorded to compute weight.  
• Clips are normalised with per-channel dataset mean/σ.

Training Protocol
• 50 epochs, batch = 64 clips/GPU, AdamW (lr = 3e-4, cosine decay).  
• 3 random seeds per variation (total 15 BYOL + 3 MoCo runs).  
• Online/target momentum 0.996; projector MLP 2048→256.

Evaluation
1. Linear probe: train a single-layer logistic classifier on frozen features (20 epochs, lr = 0.1, SGD) – Primary metric: macro-F1.  
2. k-NN (k = 20) accuracy – secondary.  
3. Temporal locality: Spearman ρ between ‖z_t − z_{t+Δt}‖₂ and Δt on 10 K sampled pairs.  
4. Efficiency: GPU hours, peak VRAM, throughput (clips/s), measured with PyTorch profiler.

Hyper-parameter Analysis
• τ is swept implicitly via variations.  
• For each seed, save F1 vs epoch to detect early/late overfitting.  
• Learning-rate restart at 80 % of training to test stability (single pilot run).

Robustness Checks
• Inject 5 % salt-and-pepper noise into 10 % of frames during evaluation; record ΔF1.  
• Distribution shift: evaluate on night-vision subset (4 % of test videos) unseen during training.

Success Criteria (linked to H1–H3)
• TW-BYOL-tau30 ≥ +2 F1 over ρBYOL (p < 0.05 paired t-test across seeds).  
• ρ(embedding-distance,Δt) ≥ +10 % over ρBYOL.  
• GPU hours within +5 % of baseline.

Example Code Snippet (abbreviated)
```python
for clips, frame_idxs in loader:      # clips: (B,3,T,H,W)
    (v_i,v_j), Δt = temporal_augment(clips, frame_idxs)
    p = online_proj(online_backbone(v_i))
    z = target_proj(target_backbone(v_j))
    loss = time_weighted_byol_loss(p, z, Δt, tau=args.tau)
```

Outputs stored per run in exp-main-perf-ablation/<variation>/<seed>/ with JSON logs for automated aggregation.
- Run Variations: ['rhoBYOL-uniform', 'TW-BYOL-tau15', 'TW-BYOL-tau30', 'TW-BYOL-tau60', 'TW-BYOL-binary-cutoff']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"\nevaluate.py \u2013 Aggregate \u0026 compare results of the run variations.\nReads all sub-directories in --results-dir that contain results.json, compiles\ncomparison tables \u0026 figures and writes them to stdout + images/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_TOPIC_FINAL_LOSS = \"final_loss\"\n\n################################################################################\n# -----------------------------  utilities  -----------------------------------#\n################################################################################\n\ndef collect_results(results_dir: Path) -\u003e List[Dict]:\n    records = []\n    for run_dir in results_dir.iterdir():\n        file = run_dir / \"results.json\"\n        if file.exists():\n            with open(file, \"r\", encoding=\"utf-8\") as fp:\n                records.append(json.load(fp))\n    return records\n\n################################################################################\n# --------------------------  figure helpers  ---------------------------------#\n################################################################################\n\ndef plot_final_loss(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(data=df, x=\"run_id\", y=\"final_val_loss\", hue=\"algorithm\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Loss\")\n    ax.set_title(\"Final Validation Loss Across Experiments\")\n\n    # annotate each bar\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=8)\n\n    plt.tight_layout()\n    fname = f\"{FIG_TOPIC_FINAL_LOSS}.pdf\"\n    images_dir = out_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n    plt.savefig(images_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef main(results_dir: Path):\n    records = collect_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n\n    df = pd.DataFrame(records)\n    # ---------------------------------------------------------------- figures\n    fig_files = []\n    fig_files.append(plot_final_loss(df, results_dir))\n\n    # --------------------------------------------------------- stdout outputs\n    comparison = {\n        \"num_runs\": len(records),\n        \"best_final_val_loss\": df[\"final_val_loss\"].min(),\n        \"worst_final_val_loss\": df[\"final_val_loss\"].max(),\n        \"figure_files\": fig_files,\n    }\n\n    print(\"\\n===== Cross-Run Comparison Summary =====\")\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(Path(args.results_dir))", "full_experiment_yaml": "# Full experiment configuration for exp-main-perf-ablation\nexperiments:\n  - run_id: rhoBYOL-uniform\n    description: |\n      Baseline implementation of \u03c1BYOL with uniform weighting (standard BYOL\n      loss). SlowFast-like video backbone approximated by r3d_18.\n    seed: 101\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-tau15\n    description: |\n      Time-Weighted BYOL with exponential weighting (\u03c4=15 frames).\n    seed: 102\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 15\n        weight_mode: exponential\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-tau30\n    description: |\n      Time-Weighted BYOL with exponential weighting (\u03c4=30 frames).\n    seed: 103\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: exponential\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-tau60\n    description: |\n      Time-Weighted BYOL with exponential weighting (\u03c4=60 frames).\n    seed: 104\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 60\n        weight_mode: exponential\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n\n  - run_id: TW-BYOL-binary-cutoff\n    description: |\n      Ablation \u2013 Time-Weighted BYOL with hard binary cutoff (\u03c4=30 frames).\n    seed: 105\n    dataset:\n      name: mabe22\n      root: /data/mabe22\n      params:\n        clip_len: 30\n    model:\n      type: r3d_18\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: binary\n    training:\n      epochs: 50\n      batch_size: 64\n      learning_rate: 3e-4\n", "main_py": "\"\"\"\nmain.py \u2013 Experiment orchestrator.\nReads smoke_test.yaml or full_experiment.yaml, spawns src/train.py sequentially\nfor each run variation, captures logs, and finally launches src/evaluate.py.\n\"\"\"\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n################################################################################\n# -----------------------------  log helpers  ---------------------------------#\n################################################################################\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run *cmd* while tee-ing stdout / stderr to the given files + parent console.\"\"\"\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as out_fp, open(stderr_path, \"w\", encoding=\"utf-8\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                sys.stdout.write(stdout_line)\n                out_fp.write(stdout_line)\n            if stderr_line:\n                sys.stderr.write(stderr_line)\n                err_fp.write(stderr_line)\n            if stdout_line == \"\" and stderr_line == \"\" and process.poll() is not None:\n                break\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n################################################################################\n# -----------------------------  orchestrator  --------------------------------#\n################################################################################\n\ndef load_yaml(path: Path) -\u003e Dict:\n    with open(path, \"r\", encoding=\"utf-8\") as fp:\n        return yaml.safe_load(fp)\n\n\ndef create_temp_run_config(run_cfg: Dict) -\u003e Path:\n    \"\"\"Write *run_cfg* to a NamedTemporaryFile and return its path.\"\"\"\n    tmp = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\")\n    json.dump(run_cfg, tmp)\n    tmp.flush()\n    return Path(tmp.name)\n\n\ndef main(args):\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    exp_cfg = load_yaml(cfg_path)\n\n    runs = exp_cfg.get(\"experiments\", [])\n    if not runs:\n        print(f\"No experiments defined in {cfg_path}\")\n        sys.exit(1)\n\n    for run in runs:\n        run_id = run[\"run_id\"]\n        print(f\"\\n=== Launching run: {run_id} ===\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        temp_cfg_path = create_temp_run_config(run)\n        cmd = [\n            sys.executable, \"-m\", \"src.train\",\n            \"--run-config\", str(temp_cfg_path),\n            \"--results-dir\", str(results_dir),\n        ]\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # ------------------------------------------------- post-hoc evaluation\n    print(\"\\n===== All runs finished, starting evaluation =====\")\n    eval_cmd = [\n        sys.executable, \"-m\", \"src.evaluate\",\n        \"--results-dir\", str(results_dir),\n    ]\n    tee_subprocess(\n        eval_cmd,\n        results_dir / \"evaluation_stdout.log\",\n        results_dir / \"evaluation_stderr.log\",\n    )\n\n\n################################################################################\n# --------------------------------  CLI  --------------------------------------#\n################################################################################\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run full experimental pipeline.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results are stored.\")\n\n    args_parsed = parser.parse_args()\n    main(args_parsed)", "model_py": "\"\"\"\nmodel.py \u2013 Model architectures \u0026 BYOL utilities.\nContains:\n1. Backbone builders for 2D (image) and lightweight 3D (video) variants.\n2. BYOL wrapper with target network EMA.\n3. Loss functions including time-weighted (exponential or binary) variants.\n\"\"\"\nfrom typing import Tuple\nimport copy\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models as tv_models\nfrom torchvision.models import video as tvm_video\n\n################################################################################\n# -------------------------  Utility / helper classes  ------------------------#\n################################################################################\n\nclass ResNetVideoWrapper(nn.Module):\n    \"\"\"Wraps a 2D ResNet and applies it frame-wise, then averages over time.\"\"\"\n\n    def __init__(self, resnet_2d: nn.Module):\n        super().__init__()\n        self.backbone = resnet_2d  # without fc layer (identity)\n\n    def forward(self, x: torch.Tensor):  # x: (B,C,T,H,W)\n        B, C, T, H, W = x.shape\n        x = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)  # (B*T,C,H,W)\n        feats = self.backbone(x)  # (B*T, F)\n        feats = feats.view(B, T, -1).mean(dim=1)  # temporal average -\u003e (B,F)\n        return feats\n\n################################################################################\n# --------------------------   projection heads   -----------------------------#\n################################################################################\n\nclass MLPHead(nn.Module):\n    \"\"\"2-layer MLP projection/prediction head for BYOL.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 4096, out_dim: int = 256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n################################################################################\n# ------------------------------  backbones  ----------------------------------#\n################################################################################\n\ndef _remove_fc(module: nn.Module) -\u003e Tuple[nn.Module, int]:\n    \"\"\"Sets .fc to Identity and returns the backbone + feature dim.\"\"\"\n    feat_dim = module.fc.in_features\n    module.fc = nn.Identity()\n    return module, feat_dim\n\n\ndef build_backbone_and_heads(model_cfg: dict):\n    \"\"\"Returns (backbone, projector, predictor) for BYOL.\"\"\"\n    model_type = model_cfg[\"type\"].lower()\n\n    # -------------------- 2D ResNets wrapped to video --------------------\n    if model_type in {\"resnet18_video\", \"resnet18\"}:\n        backbone_2d, feat_dim = _remove_fc(tv_models.resnet18(weights=None))\n        backbone = ResNetVideoWrapper(backbone_2d)\n    elif model_type in {\"resnet50_video\", \"resnet50\"}:\n        backbone_2d, feat_dim = _remove_fc(tv_models.resnet50(weights=None))\n        backbone = ResNetVideoWrapper(backbone_2d)\n\n    # --------------------- lightweight 3D CNNs ---------------------------\n    elif model_type == \"r3d_18\":\n        r3d = tvm_video.r3d_18(weights=None)\n        feat_dim = r3d.fc.in_features\n        r3d.fc = nn.Identity()\n        backbone = r3d  # already processes (B,C,T,H,W)\n\n    elif model_type == \"mc3_18\":\n        mc3 = tvm_video.mc3_18(weights=None)\n        feat_dim = mc3.fc.in_features\n        mc3.fc = nn.Identity()\n        backbone = mc3\n\n    else:\n        raise ValueError(f\"Unknown model type \u0027{model_type}\u0027 in config.\")\n\n    # -------------------------- heads ------------------------------\n    proj_hidden = model_cfg.get(\"proj_hidden_dim\", 4096)\n    proj_out = model_cfg.get(\"proj_output_dim\", 256)\n    predictor_hidden = model_cfg.get(\"predictor_hidden_dim\", 4096)\n\n    projector = MLPHead(feat_dim, proj_hidden, proj_out)\n    predictor = MLPHead(proj_out, predictor_hidden, proj_out)\n    return backbone, projector, predictor\n\n################################################################################\n# -------------------------------  BYOL  --------------------------------------#\n################################################################################\n\nclass BYOL(nn.Module):\n    \"\"\"Minimal BYOL implementation supporting TW-BYOL loss computation.\"\"\"\n\n    def __init__(self, backbone: nn.Module, projector: nn.Module, predictor: nn.Module, moving_average_decay: float = 0.996):\n        super().__init__()\n        self.online_backbone = backbone\n        self.online_projector = projector\n        self.predictor = predictor\n\n        self.target_backbone = copy.deepcopy(backbone)\n        self.target_projector = copy.deepcopy(projector)\n        for p in self.target_backbone.parameters():\n            p.requires_grad = False\n        for p in self.target_projector.parameters():\n            p.requires_grad = False\n\n        self.moving_average_decay = moving_average_decay\n\n    @torch.no_grad()\n    def update_target_network(self):\n        self._update_moving_average(self.online_backbone, self.target_backbone)\n        self._update_moving_average(self.online_projector, self.target_projector)\n\n    @torch.no_grad()\n    def _update_moving_average(self, online: nn.Module, target: nn.Module):\n        for p_o, p_t in zip(online.parameters(), target.parameters()):\n            p_t.data.mul_(self.moving_average_decay).add_(p_o.data, alpha=1.0 - self.moving_average_decay)\n\n    # -------------------------------------------------------------\n    def forward(self, view1, view2):\n        p_online = self.predictor(self.online_projector(self.online_backbone(view1)))\n        with torch.no_grad():\n            z_target = self.target_projector(self.target_backbone(view2)).detach()\n        return p_online, z_target\n\n################################################################################\n# ----------------------------  loss functions  -------------------------------#\n################################################################################\n\ndef byol_loss(p_online: torch.Tensor, z_target: torch.Tensor) -\u003e torch.Tensor:\n    return F.mse_loss(p_online, z_target)\n\n\ndef time_weighted_byol_loss(\n    p_online: torch.Tensor,\n    z_target: torch.Tensor,\n    frame_dist: torch.Tensor,\n    tau: float = 30.0,\n    mode: str = \"exponential\",\n) -\u003e torch.Tensor:\n    \"\"\"Time-weighted BYOL loss.\n\n    mode=\u0027exponential\u0027  : weight = exp(-\u0394t / \u03c4)\n    mode=\u0027binary\u0027       : weight = 1 if \u0394t \u003c= \u03c4 else 0\n    \"\"\"\n    if frame_dist is None:\n        raise RuntimeError(\"frame_dist tensor is required for TW-BYOL.\")\n\n    if mode == \"exponential\":\n        weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)\n    elif mode == \"binary\":\n        weight = (frame_dist.float() \u003c= tau).float().to(p_online.device)\n    else:\n        raise ValueError(f\"Unknown weight_mode \u0027{mode}\u0027.\")\n\n    per_sample = (p_online - z_target).pow(2).sum(dim=1)\n    return (weight * per_sample).mean()", "preprocess_py": "\"\"\"\npreprocess.py \u2013 Dataset loading \u0026 augmentation utilities for all experiments.\nImplements two datasets:\n1. SyntheticRandomVideoDataset \u2013 tiny random-tensor videos used for smoke tests.\n2. MABe22Dataset \u2013 loader for the real MABe22 mice-behaviour video dataset. The\n   loader expects the following directory structure (RGB frames or MP4 files):\n\n    \u003croot\u003e/\n        train/\n            vid_0001.mp4  (or folder of jpg/png frames)\n            vid_0002.mp4\n            ...\n        val/\n            ...\n        test/\n            ...\n\nIf instead of MP4 files each video is stored as a folder of frames, simply place\nall frames in a sub-directory (e.g. vid_0001/000001.jpg) and the loader will\npick that up automatically.\n\"\"\"\nfrom typing import Dict, List, Tuple\nfrom pathlib import Path\nimport random\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\nfrom torchvision.io import read_video\nfrom PIL import Image\n\n################################################################################\n# ----------------------------  video transforms  -----------------------------#\n################################################################################\n\nclass _PerFrameTransform:\n    \"\"\"Wrapper that applies a torchvision transform to each frame individually.\"\"\"\n\n    def __init__(self, img_transform):\n        self.img_transform = img_transform\n\n    def __call__(self, frames: List[torch.Tensor]) -\u003e torch.Tensor:\n        processed: List[torch.Tensor] = []\n        for fr in frames:\n            # fr: Tensor (H,W,C) uint8 in [0,255]\n            if isinstance(fr, torch.Tensor):\n                fr_pil = Image.fromarray(fr.numpy())\n            else:  # already PIL\n                fr_pil = fr\n            processed.append(self.img_transform(fr_pil))  # -\u003e Tensor (C,H,W) float\n        clip = torch.stack(processed, dim=1)  # (C,T,H,W)\n        return clip\n\n################################################################################\n# ----------------------------  Dataset classes  ------------------------------#\n################################################################################\n\nclass SyntheticRandomVideoDataset(Dataset):\n    \"\"\"Tiny synthetic dataset \u2013 spits out random videos. Used for CI smoke tests.\"\"\"\n\n    def __init__(self, num_samples: int = 32, clip_len: int = 8, img_size: int = 112, split=\"train\"):\n        super().__init__()\n        self.num_samples = num_samples\n        self.clip_len = clip_len\n        self.img_size = img_size\n        self.rng = random.Random(0 if split == \"train\" else 1)\n        tf_train = T.Compose(\n            [\n                T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n                T.RandomHorizontalFlip(),\n                T.ToTensor(),\n            ]\n        )\n        tf_val = T.Compose([T.Resize(img_size), T.CenterCrop(img_size), T.ToTensor()])\n        self.transform = _PerFrameTransform(tf_train if split == \"train\" else tf_val)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        clip = [\n            torch.randint(0, 256, (self.img_size, self.img_size, 3), dtype=torch.uint8)\n            for _ in range(self.clip_len)\n        ]\n        view1 = self.transform(clip)\n        view2 = self.transform(clip)\n        frame_dist = torch.tensor(random.randint(0, self.clip_len - 1), dtype=torch.long)\n        return {\"view1\": view1, \"view2\": view2, \"frame_dist\": frame_dist}\n\n\nclass MABe22Dataset(Dataset):\n    \"\"\"Dataset loader for the MABe22 mice-triplet video corpus.\n\n    For simplicity and robustness the loader supports *either* MP4 files or\n    folders containing individual RGB frames. The temporal augmentation (two\n    distinct views + frame distance) is performed on-the-fly.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: Path,\n        split: str,\n        clip_len: int = 30,\n        fps: int = 15,\n        transforms: Dict = None,\n    ):\n        super().__init__()\n        self.root = Path(root).expanduser()\n        if not self.root.exists():\n            raise FileNotFoundError(f\"Dataset root folder \u0027{self.root}\u0027 not found.\")\n        self.split = split\n        self.clip_len = clip_len\n        self.fps = fps\n        self.video_paths = self._collect_videos()\n        if len(self.video_paths) == 0:\n            raise RuntimeError(f\"No videos found under {self.root}/{split}\")\n\n        # build transform pipeline\n        self.transforms = self._build_transforms(train=(split == \"train\")) if transforms is None else transforms\n\n    # --------------------------------------------------------------------- utils\n    def _collect_videos(self) -\u003e List[Path]:\n        split_dir = self.root / self.split\n        mp4s = list(split_dir.rglob(\"*.mp4\")) + list(split_dir.rglob(\"*.avi\"))\n        frame_folders = [p for p in split_dir.iterdir() if p.is_dir()]\n        return mp4s + frame_folders\n\n    def _build_transforms(self, train: bool):\n        if train:\n            img_tf = T.Compose(\n                [\n                    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                    T.RandomHorizontalFlip(),\n                    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n                    T.RandomGrayscale(p=0.2),\n                    T.GaussianBlur(kernel_size=3),\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ]\n            )\n        else:\n            img_tf = T.Compose(\n                [\n                    T.Resize(256),\n                    T.CenterCrop(224),\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ]\n            )\n        return _PerFrameTransform(img_tf)\n\n    # ------------------------------------------------------------------ helpers\n    def _read_clip_from_video(self, path: Path, start: int, end: int) -\u003e List[torch.Tensor]:\n        # Use torchvision.io for mp4 files\n        video, _, _ = read_video(str(path), start_pts=None, end_pts=None, pts_unit=\"sec\")\n        # video: (T,H,W,C) uint8\n        if end \u003e= video.shape[0]:\n            # loop the video if not enough frames\n            idxs = list(range(start, video.shape[0])) + [video.shape[0] - 1] * (end - video.shape[0] + 1)\n            frames = [video[i] for i in idxs]\n        else:\n            frames = video[start:end]\n        return [fr for fr in frames]\n\n    def _read_clip_from_folder(self, folder: Path, start: int, end: int) -\u003e List[torch.Tensor]:\n        frame_files = sorted(folder.glob(\"*.jpg\")) + sorted(folder.glob(\"*.png\"))\n        if len(frame_files) \u003c end:\n            frame_files = frame_files + [frame_files[-1]] * (end - len(frame_files))\n        selected = frame_files[start:end]\n        frames: List[torch.Tensor] = []\n        for f in selected:\n            img = Image.open(f).convert(\"RGB\")\n            frames.append(torch.tensor(img))  # will be converted in transform\n        return frames\n\n    # ---------------------------------------------------------------- dataset API\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        vid_path = self.video_paths[idx]\n        # For each video sample TWO temporal crops \u0026 compute frame distance\n        # ------------------------------------------------------------- video meta\n        if vid_path.is_file():\n            # Read meta by reading entire video once to get frame count (cheap for mp4 headers)\n            video, _, _ = read_video(str(vid_path), pts_unit=\"sec\")\n            num_frames = video.shape[0]\n        else:\n            num_frames = len(list(vid_path.glob(\"*.jpg\"))) + len(list(vid_path.glob(\"*.png\")))\n            video = None  # loaded later lazily\n        if num_frames \u003c self.clip_len + 1:\n            raise RuntimeError(f\"Video too short ({num_frames} frames): {vid_path}\")\n\n        # sample two start indices\n        start1 = random.randint(0, num_frames - self.clip_len)\n        start2 = random.randint(0, num_frames - self.clip_len)\n        frame_dist = abs(start1 - start2)\n\n        end1 = start1 + self.clip_len\n        end2 = start2 + self.clip_len\n\n        # -------------------------------- load clips\n        if vid_path.is_file():\n            if video is None:\n                video, _, _ = read_video(str(vid_path), pts_unit=\"sec\")\n            clip_np1 = video[start1:end1]\n            clip_np2 = video[start2:end2]\n            frames1 = [fr for fr in clip_np1]\n            frames2 = [fr for fr in clip_np2]\n        else:\n            frames1 = self._read_clip_from_folder(vid_path, start1, end1)\n            frames2 = self._read_clip_from_folder(vid_path, start2, end2)\n\n        view1 = self.transforms(frames1)  # (C,T,H,W)\n        view2 = self.transforms(frames2)\n        return {\n            \"view1\": view1,\n            \"view2\": view2,\n            \"frame_dist\": torch.tensor(frame_dist, dtype=torch.long),\n        }\n\n################################################################################\n# ------------------------------  public API  ----------------------------------\n################################################################################\n\ndef get_dataset(cfg: Dict, split: str):\n    \"\"\"Factory that returns a dataset according to cfg.\"\"\"\n    name = cfg[\"name\"].lower()\n    root = Path(cfg.get(\"root\", \"./data\")).expanduser()\n    params = cfg.get(\"params\", {})\n\n    # Synthetic dataset (used in CI / smoke tests)\n    if name == \"synthetic\":\n        return SyntheticRandomVideoDataset(split=split, **params)\n\n    # Real MABe22 dataset\n    if name in {\"mabe22\", \"mabe\", \"mabe22_dataset\"}:\n        return MABe22Dataset(root=root, split=split, **params)\n\n    raise ValueError(f\"Dataset \u0027{name}\u0027 not recognised in preprocess.get_dataset().\")", "pyproject_toml": "[project]\nname = \"tw_byol_experiments\"\nversion = \"0.1.0\"\ndescription = \"Time-Weighted BYOL experimental codebase\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntorchvision = \"\u003e=0.15.0\"\npyyaml = \"^6.0\"\ntqdm = \"^4.66.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\nscikit-learn = \"^1.3.0\"\npandas = \"^2.1.0\"\npytorchvideo = \"^0.1.5\"", "smoke_test_yaml": "# Smoke test configuration \u2013 lightweight versions of all five run variations.\nexperiments:\n  - run_id: smoke_rhoBYOL_uniform\n    description: |\n      Smoke-test: Uniform BYOL (baseline) with synthetic dataset.\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_tau15\n    description: |\n      Smoke-test: TW-BYOL with \u03c4=15 (synthetic dataset).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 15\n        weight_mode: exponential\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_tau30\n    description: |\n      Smoke-test: TW-BYOL with \u03c4=30 (synthetic dataset).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: exponential\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_tau60\n    description: |\n      Smoke-test: TW-BYOL with \u03c4=60 (synthetic dataset).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 60\n        weight_mode: exponential\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4\n\n  - run_id: smoke_TW_BYOL_binary_cutoff\n    description: |\n      Smoke-test: TW-BYOL with binary cutoff weighting (\u03c4=30).\n    seed: 123\n    dataset:\n      name: synthetic\n      root: ./data/synthetic\n      params:\n        num_samples: 20\n        clip_len: 8\n    model:\n      type: r3d_18\n      proj_hidden_dim: 512\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n        weight_mode: binary\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 3e-4", "train_py": "\"\"\"\ntrain.py \u2013 Train a single self-supervised run variation (BYOL / TW-BYOL, etc.)\nThe script is launched ONLY by src/main.py. It therefore assumes that all CLI\narguments originate from main.py and are validated there.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport time\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom src import preprocess as pp\nfrom src import model as models\n\n################################################################################\n# ------------------------------   helpers   ----------------------------------#\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -\u003e torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n################################################################################\n# ------------------------------   training   ---------------------------------#\n################################################################################\n\ndef byol_step(batch: Dict[str, torch.Tensor], learner, optimizer, scaler, config):\n    \"\"\"One optimisation step for BYOL / TW-BYOL.\n\n    Args\n    ----\n    batch : Dict \u2013 must have keys \u0027view1\u0027, \u0027view2\u0027, \u0027frame_dist\u0027 (frame_dist optional)\n    learner : models.BYOL \u2013 model wrapper that returns p_online \u0026 z_target\n    optimizer : torch Optimizer\n    scaler : GradScaler or None\n    config : dict \u2013 algorithm section of YAML\n    \"\"\"\n    device = get_device()\n    view1 = batch[\"view1\"].to(device, non_blocking=True)\n    view2 = batch[\"view2\"].to(device, non_blocking=True)\n    frame_dist = batch.get(\"frame_dist\")\n    if frame_dist is not None:\n        frame_dist = frame_dist.to(device, non_blocking=True)\n\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.cuda.amp.autocast(enabled=config.get(\"mixed_precision\", True)):\n        p_online, z_target = learner(view1, view2)\n        algo_type = config[\"type\"].lower()\n        if algo_type == \"tw-byol\":\n            tau = config[\"params\"].get(\"tau\", 30.0)\n            weight_mode = config[\"params\"].get(\"weight_mode\", \"exponential\")\n            loss = models.time_weighted_byol_loss(\n                p_online,\n                z_target,\n                frame_dist=frame_dist,\n                tau=tau,\n                mode=weight_mode,\n            )\n        else:  # ordinary BYOL\n            loss = models.byol_loss(p_online, z_target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    learner.update_target_network()\n    return loss.item()\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef run_training(cfg: Dict, results_dir: Path):\n    description = cfg.get(\"description\", \"No description provided.\")\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------ paths\n    run_dir = results_dir / run_id\n    images_dir = run_dir / \"images\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------- device\n    device = get_device()\n\n    # --------------------------------------------------------- dataset / dataloader\n    dataset_cfg = cfg[\"dataset\"]\n    train_ds = pp.get_dataset(dataset_cfg, split=\"train\")\n    val_ds = pp.get_dataset(dataset_cfg, split=\"val\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        shuffle=True,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg[\"training\"].get(\"val_batch_size\", cfg[\"training\"][\"batch_size\"]),\n        shuffle=False,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n\n    # ------------------------------------------------------------- model / opt\n    model_cfg = cfg[\"model\"]\n    algorithm_cfg = cfg[\"algorithm\"]\n\n    online_backbone, projector, predictor = models.build_backbone_and_heads(model_cfg)\n    learner = models.BYOL(\n        backbone=online_backbone,\n        projector=projector,\n        predictor=predictor,\n        moving_average_decay=algorithm_cfg.get(\"ema_decay\", 0.996),\n    ).to(device)\n\n    optimizer = optim.Adam(\n        learner.parameters(), lr=cfg[\"training\"][\"learning_rate\"], weight_decay=1e-6\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=algorithm_cfg.get(\"mixed_precision\", True))\n\n    # ------------------------------------------------------------- training loop\n    epochs = cfg[\"training\"][\"epochs\"]\n    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"time_sec\": []}\n\n    best_val_loss = float(\"inf\")\n    start_time_total = time.time()\n    for epoch in range(1, epochs + 1):\n        learner.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            loss_val = byol_step(batch, learner, optimizer, scaler, algorithm_cfg)\n            train_losses.append(loss_val)\n            pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n        # ---------------- validation (BYOL self-supervised loss on val set)\n        learner.eval()\n        with torch.no_grad():\n            val_losses = []\n            for batch in val_loader:\n                view1 = batch[\"view1\"].to(device, non_blocking=True)\n                view2 = batch[\"view2\"].to(device, non_blocking=True)\n                frame_dist = batch.get(\"frame_dist\")\n                if frame_dist is not None:\n                    frame_dist = frame_dist.to(device, non_blocking=True)\n\n                with torch.cuda.amp.autocast(enabled=algorithm_cfg.get(\"mixed_precision\", True)):\n                    p_online, z_target = learner(view1, view2)\n                    if algorithm_cfg[\"type\"].lower() == \"tw-byol\":\n                        tau = algorithm_cfg[\"params\"].get(\"tau\", 30.0)\n                        weight_mode = algorithm_cfg[\"params\"].get(\"weight_mode\", \"exponential\")\n                        val_loss_val = models.time_weighted_byol_loss(\n                            p_online, z_target, frame_dist=frame_dist, tau=tau, mode=weight_mode\n                        ).item()\n                    else:\n                        val_loss_val = models.byol_loss(p_online, z_target).item()\n                val_losses.append(val_loss_val)\n\n        mean_train_loss = float(np.mean(train_losses))\n        mean_val_loss = float(np.mean(val_losses))\n        epoch_time = time.time() - start_time_total\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(mean_train_loss)\n        history[\"val_loss\"].append(mean_val_loss)\n        history[\"time_sec\"].append(epoch_time)\n\n        # Save best model checkpoint\n        if mean_val_loss \u003c best_val_loss:\n            best_val_loss = mean_val_loss\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save({\"epoch\": epoch, \"state_dict\": learner.state_dict()}, ckpt_path)\n\n        # Epoch-level JSON logging (append-safe)\n        with open(run_dir / \"epoch_metrics.jsonl\", \"a\", encoding=\"utf-8\") as fp:\n            fp.write(\n                json.dumps(\n                    {\n                        \"epoch\": epoch,\n                        \"train_loss\": mean_train_loss,\n                        \"val_loss\": mean_val_loss,\n                        \"time_sec\": epoch_time,\n                    }\n                )\n                + \"\\n\"\n            )\n\n    total_time = time.time() - start_time_total\n\n    # --------------------------------------------------------- save figures\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set(style=\"whitegrid\")\n\n    # Training \u0026 validation loss curve\n    plt.figure(figsize=(8, 4))\n    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss \u2013 {run_id}\")\n    plt.annotate(\n        f\"{history[\u0027val_loss\u0027][-1]:.4f}\",\n        xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n        xytext=(5, -10),\n        textcoords=\"offset points\",\n    )\n    plt.legend()\n    plt.tight_layout()\n    fig_name = f\"training_loss_{run_id}.pdf\"\n    plt.savefig(images_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------------------------------------------------- final results\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm\": algorithm_cfg[\"type\"],\n        \"dataset\": dataset_cfg[\"name\"],\n        \"model\": model_cfg[\"type\"],\n        \"epochs\": epochs,\n        \"best_val_loss\": best_val_loss,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"total_time_sec\": total_time,\n        \"figure_files\": [fig_name],\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # ----------------------------------------------------- stdout requirements\n    print(\"\\n===== Experiment Description =====\")\n    print(description)\n    print(\"===== Numerical Results (JSON) =====\")\n    print(json.dumps(results))\n\n################################################################################\n# ------------------------------   CLI   --------------------------------------#\n################################################################################\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to JSON or YAML file with a SINGLE run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be written.\")\n    return parser.parse_args()\n\n\ndef load_run_config(path: str) -\u003e Dict:\n    path = Path(path)\n    if path.suffix in {\".yaml\", \".yml\"}:\n        import yaml\n\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = yaml.safe_load(fp)\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = json.load(fp)\n    return cfg\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = load_run_config(args.run_config)\n    run_training(cfg, Path(args.results_dir))"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": true,
    "experiment_code_issue": ""
}
