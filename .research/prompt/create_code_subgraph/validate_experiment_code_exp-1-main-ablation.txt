
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "Encoder-caching methods (e.g. Faster Diffusion) still lose image quality when the stride between “key” time-steps becomes large (≤10–12 steps are usually safe, larger strides cause blur / artifacts). The root cause is that encoder features, although relatively stable, are not perfectly time invariant. How can we make the encoder features even more consistent so that we can safely reuse them for many more skipped steps and obtain larger speed-ups?",
    "Methods": "Feature Consistency Regularization (FCR)\n1. During (re)training or a short fine-tuning session, add an auxiliary loss that explicitly encourages the encoder feature map to be invariant over time:\n   L_FCR = λ · E_{t,Δ} [ ‖ Enc(x_t) − Enc(x_{t−Δ}) ‖^2 ]\n   where t ~ Uniform(Δ , T), Δ ~ Uniform(1 , Δ_max).\n2. The total loss becomes  L_total = L_denoise + L_FCR.\n3. No network architecture change is required; only an extra forward pass of the encoder on a second, more-noisy latent x_{t−Δ} drawn within the same mini-batch.\n\nTheoretical motivation: If the encoder learns to output nearly identical representations for nearby (or even moderately distant) noise levels, then at inference we can safely reuse a cached encoder feature over more steps (larger stride) without harming the decoder’s conditioning quality. This directly addresses the stride-vs-quality trade-off with a single, simple regularizer.",
    "Experimental Setup": "Model: the open-source Stable Diffusion v1.5 UNet.\nData: 50 k randomly selected LAION-Aesthetics captions & images (10 k training steps are sufficient for a proof-of-concept).\nBaselines:\n  a) Original model + Faster Diffusion sampling with stride 5 (FD-5).\n  b) Original model + Faster Diffusion sampling with stride 10 (FD-10).\nProposed:\n  c) Model fine-tuned with FCR (λ=0.1, Δ_max=10) + Faster Diffusion sampling with stride 10 (FCR-FD-10).\nMetrics: FID (↓) on 30 k MS-COCO validation prompts, CLIPScore (↑), and wall-clock sampling time per image on one RTX-4090 GPU.\nExpectation: (c) keeps FID & CLIPScore close to (a) while matching the speed of (b).",
    "Experimental Code": "# core training snippet (PyTorch)\nimport torch, torch.nn.functional as F\n\nlambda_fcr = 0.1\nD = unet                      # loaded Stable-Diffusion UNet\nenc = lambda feats: feats[\"encoder_hidden_states\"]  # assumes encoder features are returned in dict\n\nfor batch in dataloader:\n    imgs, text = batch\n    z0 = autoencoder.encode(imgs)            # latent\n    t = torch.randint(1, T, (len(z0),), device=z0.device)\n    eps = torch.randn_like(z0)\n    zt = q_sample(z0, t, eps)                # standard diffusion marche\n\n    # main denoising loss\n    out, feats_t = D(zt, t, return_dict=True)\n    loss_main = F.mse_loss(out, eps)\n\n    # second time-step for FCR\n    delta = torch.randint(1, 11, (len(z0),), device=z0.device)\n    t2 = torch.clamp(t - delta, min=1)\n    zt2 = q_sample(z0, t2, eps)\n    _, feats_t2 = D(zt2, t2, return_dict=True, encoder_only=True)  # small extra cost\n\n    loss_fcr = (enc(feats_t) - enc(feats_t2)).pow(2).mean()\n\n    loss = loss_main + lambda_fcr * loss_fcr\n    loss.backward()\n    optimizer.step(); optimizer.zero_grad()",
    "Expected Result": "• FD-5 (baseline):   FID ≈ 6.3,   time ≈ 0.28 s/img.\n• FD-10 (baseline):  FID ≈ 7.9,   time ≈ 0.18 s/img.  (quality drops)\n• FCR-FD-10:         FID ≈ 6.5,   time ≈ 0.18 s/img.\n\nThus the proposed method recovers almost all lost quality while preserving the larger speed-up from the doubled stride.",
    "Expected Conclusion": "A single, easily-implemented regularization term that aligns encoder features across time makes cached-encoder sampling robust to much larger strides. This yields substantial extra acceleration (≈1.5× over already-accelerated FD-5) at virtually no inference-time cost and only a brief fine-tuning cost. The idea is generic and can be plugged into any UNet-based diffusion model or used jointly with other acceleration or distillation techniques."
}

# Experimental Design
## Experiment Strategy
Global Experimental Strategy for Validating Feature Consistency Regularization (FCR)

1. Objectives to Validate
   1.1 Quality-vs-Speed Trade-off: Does FCR maintain image quality while enabling larger sampling strides?  
   1.2 Computational Efficiency: How much wall-clock speed-up, FLOP reduction and GPU-memory saving are achieved at inference?  
   1.3 Robustness: Does quality remain stable across random seeds, prompt difficulty, higher resolutions and perturbed noise schedules?  
   1.4 Generalization: Does the benefit transfer to (i) different diffusion backbones, (ii) different domains (text-to-image, in-painting, depth-to-image) and (iii) different datasets?  
   1.5 Mechanistic Validation: Does FCR actually reduce temporal variance in encoder features and is this reduction predictive of downstream quality?  
   1.6 Cost of Adoption: What is the additional training cost and is the inference overhead negligible?

2. Comparison Grid Used in All Experiments
   Baselines (held fixed across studies):  
   • Original model with conservative stride (FD-5) → «quality upper-bound».  
   • Original model with large stride (FD-10/15/20) → «speed upper-bound».  
   • State-of-the-art accelerators (e.g. DPMSolver++, Latent-Distillation) at matched step counts.  
   Ablations:  
   • FCR λ ∈ {0, 0.05, 0.1, 0.2}.  
   • Δ_max ∈ {5, 10, 20}.  
   • FCR applied only to early / mid / late UNet blocks.  
   • One-pass vs two-pass encoder caching.  
   • Stride sweep: 5, 10, 15, 20.  
   Combined Methods: FCR + DPMSolver, FCR + Distillation.

3. Experimental Angles (each experiment will pick a subset, but the philosophy stays the same)
   A. Quantitative Quality: FID, CLIPScore, Inception Score, and prompt-conditional precision / recall over ≥30k COCO prompts with 3 seeds.  
   B. Human Preference: 1k A/B pairs rated on MTurk; significance @95 % CI.  
   C. Computational Cost: (i) Wall-clock time per image on one A100, (ii) #UNet forward passes, (iii) peak GPU RAM, (iv) added fine-tuning time.  
   D. Feature Analysis: Variance(Enc(x_t)–Enc(x_{t-Δ})) plotted against Δ for all compared models; Pearson correlation of this variance with FID drop.  
   E. Stress & Robustness: (i) 1024² images, (ii) out-of-distribution prompts (abstract, medical), (iii) noisy schedules with ε-perturbations, (iv) adversarial latent noise.  
   F. Cross-Model Transfer: repeat core benchmarks on SD-v2.1, OpenJourney, and a latent video UNet.

4. Multi-Perspective Demonstration Plan
   Phase-0: Reproduce paper numbers on SD-v1.5 (sanity check).  
   Phase-1 (Controlled): Full ablation grid on SD-v1.5 @512²; collect all metrics A–D.  
   Phase-2 (Generalization): Repeat best λ, Δ_max settings on two new backbones & two new datasets (ImageNet, Flickr30k); collect metrics A, C, D.  
   Phase-3 (Combination): Integrate FCR with DPMSolver++ and latent-distillation; benchmark quality & cost (metrics A, C).  
   Phase-4 (Robustness): Run stress tests (metrics A, B, C).  
   Phase-5 (Human Study): Crowd-source preference on a mixture of output from Phases 1-4.

5. Success Criteria (must satisfy all to claim win)
   • Quality Preservation: For stride 10, ΔFID ≤ +5 % over FD-5 AND ΔCLIPScore ≥ –2 %.  
   • Speed-up: ≥1.4 × real-time speed-up vs FD-5 at equal quality, with ≤2 % extra VRAM.  
   • Robustness: Quality drop under stress tests ≤ original model’s drop.  
   • Generalization: All three backbones pass Quality & Speed criteria.  
   • Mechanistic Proof: Encoder-feature variance reduced by ≥30 %, with R² ≥0.6 when regressing variance against FID difference.  
   • Training Overhead: Extra fine-tune compute ≤3 % of full model pre-training compute.

6. Practical Considerations
   • Hardware: every timing number reported on a single NVIDIA A100-80 GB; batch size 1 & 8.  
   • Reproducibility: 3 random seeds per setting; publish code, hyper-params, and evaluation scripts.  
   • Statistical Tests: Paired t-test or bootstrap; report 95 % CI for all key metrics.  
   • Stopping Rule: If any candidate fails Quality Preservation, it is excluded from later phases.

This unified strategy ensures that every subsequent experiment, regardless of model or dataset, is anchored to the same validation axes, comparison baselines, measurement tools, and success thresholds, enabling a coherent narrative of FCR’s effectiveness from empirical performance down to mechanistic evidence.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates the results of all experiment variations and produces comparison\nfigures under \u003cresults-dir\u003e/images/.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\n################################################################################\n# Plot helpers                                                                  \n################################################################################\n\ndef _make_bar(ax, names: List[str], values: List[float], title: str, ylabel: str):\n    palette = sns.color_palette(\"Set2\", len(names))\n    bars = ax.bar(names, values, color=palette)\n    ax.set_title(title)\n    ax.set_ylabel(ylabel)\n    for bar, val in zip(bars, values):\n        height = bar.get_height()\n        ax.annotate(f\"{val:.3f}\", xy=(bar.get_x() + bar.get_width() / 2, height), xytext=(0, 3), textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate all experimental runs and plot comparisons\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Path where individual run folders live\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    image_dir = results_dir / \"images\"\n    image_dir.mkdir(exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # 1. Load result JSONs\n    # ------------------------------------------------------------------\n    run_results: Dict[str, Dict] = {}\n    for run_path in results_dir.iterdir():\n        if run_path.is_dir() and (run_path / \"results.json\").exists():\n            with open(run_path / \"results.json\", \"r\") as f:\n                run_results[run_path.name] = json.load(f)\n\n    if not run_results:\n        print(\"{}\")\n        return\n\n    names = list(run_results.keys())\n    fid_values = [run_results[n][\"metrics\"].get(\"fid\", None) for n in names]\n    clip_values = [run_results[n][\"metrics\"].get(\"clip_score\", None) for n in names]\n    lat_values = [run_results[n][\"metrics\"].get(\"inference_time\", None) for n in names]\n\n    # ------------------------------------------------------------------\n    # 2. Plot FID and CLIPScore\n    # ------------------------------------------------------------------\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    if any(v is not None for v in fid_values):\n        _make_bar(axes[0], names, [v if v is not None else 0 for v in fid_values], \"FID (lower=better)\", \"FID\")\n    if any(v is not None for v in clip_values):\n        _make_bar(axes[1], names, [v if v is not None else 0 for v in clip_values], \"CLIPScore (higher=better)\", \"CLIPScore\")\n    plt.tight_layout()\n    plt.savefig(image_dir / \"quality_metrics.pdf\", bbox_inches=\"tight\")\n\n    # ------------------------------------------------------------------\n    # 3. Plot inference latency\n    # ------------------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    _make_bar(ax, names, lat_values, \"Inference latency\", \"seconds / image\")\n    plt.tight_layout()\n    plt.savefig(image_dir / \"inference_latency.pdf\", bbox_inches=\"tight\")\n\n    # ------------------------------------------------------------------\n    # 4. Print structured comparison summary\n    # ------------------------------------------------------------------\n    summary = {\n        \"best_fid_run\": min([(v, n) for n, v in zip(names, fid_values) if v is not None], default=(None, None))[1],\n        \"best_clip_run\": max([(v, n) for n, v in zip(names, clip_values) if v is not None], default=(None, None))[1],\n        \"fastest_run\": min([(v, n) for n, v in zip(names, lat_values) if v is not None], default=(None, None))[1],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# PLACEHOLDER configuration. Specific datasets and models will be filled in\n# the next derivation phase.\n\nruns:\n  - name: BASELINE_FD5\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with LAION/COCO dataset configuration\n      params:\n        SPECIFIC_CONFIG_PLACEHOLDER: value\n    model:\n      name: MODEL_PLACEHOLDER  # PLACEHOLDER: Pre-trained Stable Diffusion UNet (no FCR)\n      params:\n        stride: 5\n    training:\n      epochs: 0  # Inference-only baseline\n      batch_size: 1\n      lr: 0.0\n    evaluation:\n      compute_fid: true\n  - name: FCR_FD10\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: same as above\n      params:\n        SPECIFIC_CONFIG_PLACEHOLDER: value\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5\n        lambda_fcr: 0.1\n        delta_max: 10\n        use_fp16: true\n    training:\n      epochs: 10\n      batch_size: 4\n      lr: 1e-4\n    evaluation:\n      compute_fid: true\n  # Additional experiment variations (ablations, combined methods, etc.)\n  # will be appended here in later phases.\n", "main_py": "\"\"\"src/main.py\nOrchestrates the execution of all experiment variations defined in a YAML file.\nFor each run it spawns src.train as a subprocess, tee-ing stdout/stderr into\nboth the console and per-run log files. After all runs complete, it calls\nsrc.evaluate to aggregate and visualise the results.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\n################################################################################\n# Utilities                                                                     \n################################################################################\n\ndef _tee_stream(stream, log_file):\n    \"\"\"Mirrors a stream (stdout/stderr) into a log file and the parent stream.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        sys.stdout.buffer.write(line) if log_file.name.endswith(\"stdout.log\") else sys.stderr.buffer.write(line)\n        log_file.buffer.write(line)\n    stream.close()\n\n\ndef _run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n\n    with open(stdout_path, \"wb\") as fout, open(stderr_path, \"wb\") as ferr:\n        threads = [\n            threading.Thread(target=_tee_stream, args=(proc.stdout, fout), daemon=True),\n            threading.Thread(target=_tee_stream, args=(proc.stderr, ferr), daemon=True),\n        ]\n        for t in threads:\n            t.start()\n        proc.wait()\n        for t in threads:\n            t.join()\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess {\u0027 \u0027.join(cmd)} failed with exit code {proc.returncode}\")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run all experiments defined in a config YAML\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Where all outputs should be saved\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Sequentially run each variation\n    # ------------------------------------------------------------------\n    for run in cfg[\"runs\"]:\n        run_id = run[\"name\"]\n        print(f\"==== Running experiment: {run_id} ====\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(exist_ok=True)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--run-id\",\n            run_id,\n            \"--config-file\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n        ]\n        _run_subprocess(cmd, cwd=Path.cwd(), stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs \u2192 aggregate / visualise\n    # ------------------------------------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel registry and core algorithm implementations including the Feature\nConsistency Regularisation (FCR) diffusion model.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n_MODEL_REGISTRY: Dict[str, nn.Module] = {}\n\n################################################################################\n# Utilities                                                                     \n################################################################################\n\ndef register_model(name: str):\n    def decorator(cls):\n        _MODEL_REGISTRY[name] = cls\n        return cls\n\n    return decorator\n\n\ndef build_model(model_cfg: Dict[str, Any], device: torch.device):\n    name = model_cfg[\"name\"]\n    params = model_cfg.get(\"params\", {})\n    ModelCls = _MODEL_REGISTRY.get(name)\n    if ModelCls is None:\n        raise ValueError(\n            f\"Model \u0027{name}\u0027 is not registered. Available: {list(_MODEL_REGISTRY.keys())}.\"\n        )\n    model: BaseExperimentModel = ModelCls(params=params, device=device)  # type: ignore\n    return model.to(device)\n\n################################################################################\n# Base class                                                                    \n################################################################################\n\nclass BaseExperimentModel(nn.Module):\n    \"\"\"Abstract base class that every model follows.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    # ---------------------------------------------------------------------\n    # Required API                                                          \n    # ---------------------------------------------------------------------\n    def training_step(self, batch):\n        raise NotImplementedError\n\n    def validation_step(self, batch):\n        raise NotImplementedError\n\n    def generate(self, prompts: List[str]):\n        \"\"\"Optional. Needed for diffusion models to produce samples at inference.\"\"\"\n        raise NotImplementedError\n\n################################################################################\n# Toy classification model                                                      \n################################################################################\n\n@register_model(\"toy\")\nclass ToyModel(BaseExperimentModel):\n    \"\"\"Simple 3-layer MLP classifier on flattened images. Used by smoke tests.\"\"\"\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        self.input_dim = params[\"input_dim\"]\n        hidden_dim = params.get(\"hidden_dim\", 128)\n        num_classes = params.get(\"num_classes\", 10)\n        self.net = nn.Sequential(\n            nn.Linear(self.input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes),\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.num_classes = num_classes\n        self.to(device)\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x.view(x.size(0), -1))\n\n    def training_step(self, batch):\n        imgs, _ = batch  # toy prompts ignored\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {\"loss\": loss}\n\n    def validation_step(self, batch):\n        imgs, _ = batch\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {\"val_loss\": loss}\n\n    def generate(self, prompts):\n        # Not applicable \u2013 return random tensors for API compatibility\n        return torch.randn(1, 3, 32, 32, device=next(self.parameters()).device)\n\n################################################################################\n# Diffusion model with Feature Consistency Regularisation                       \n################################################################################\n\n# Import heavy deps lazily to speed up smoke tests.\ntry:\n    from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n    from transformers import CLIPTextModel, CLIPTokenizer\nexcept ImportError:\n    AutoencoderKL = UNet2DConditionModel = DDPMScheduler = CLIPTextModel = CLIPTokenizer = None  # type: ignore\n\n\n@register_model(\"diffusion_fcr\")\nclass DiffusionFCRModel(BaseExperimentModel):\n    \"\"\"Stable-Diffusion UNet with Feature Consistency Regularisation.\"\"\"\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        if AutoencoderKL is None:\n            raise ImportError(\"diffusers and transformers must be installed for diffusion models\")\n\n        model_name = params.get(\"pretrained_model_name_or_path\", \"runwayml/stable-diffusion-v1-5\")\n        dtype = torch.float16 if params.get(\"use_fp16\", True) else torch.float32\n\n        # Components\n        self.vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\", torch_dtype=dtype).to(device)\n        self.unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\", torch_dtype=dtype).to(device)\n        self.text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\", torch_dtype=dtype).to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n        self.scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n\n        self.lambda_fcr = params.get(\"lambda_fcr\", 0.1)\n        self.delta_max = params.get(\"delta_max\", 10)\n        self.device = device\n        self.dtype = dtype\n\n        # Hook to capture encoder features (last down block activations)\n        self._enc_feats = None\n\n        def _hook_fn(_, __, output):\n            self._enc_feats = output\n\n        self.unet.down_blocks[-1].register_forward_hook(_hook_fn)\n\n    # ------------------------------------------------------------------\n    # Helper functions                                                   \n    # ------------------------------------------------------------------\n    def _encode_images(self, imgs: torch.Tensor) -\u003e torch.Tensor:\n        latents = self.vae.encode(imgs).latent_dist.sample()\n        return latents * 0.18215\n\n    def _get_text_embeddings(self, prompts: List[str]):\n        tokens = self.tokenizer(\n            prompts,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).to(self.device)\n        return self.text_encoder(tokens.input_ids)[0]\n\n    # ------------------------------------------------------------------\n    # Required API                                                      \n    # ------------------------------------------------------------------\n    def training_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n\n        # Sample primary timestep t\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n\n        text_emb = self._get_text_embeddings(prompts)\n        # First forward pass\n        self._enc_feats = None\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=text_emb).sample\n        loss_main = F.mse_loss(noise_pred, noise)\n        f1 = self._enc_feats.detach()\n\n        # Second timestep for FCR\n        delta = torch.randint(1, self.delta_max + 1, (bsz,), device=self.device)\n        t2 = torch.clamp(t - delta, min=0)\n        noisy_latents2 = self.scheduler.add_noise(latents, noise, t2)\n\n        self._enc_feats = None\n        _ = self.unet(noisy_latents2, t2, encoder_hidden_states=text_emb)\n        f2 = self._enc_feats.detach()\n\n        loss_fcr = (f1 - f2).pow(2).mean()\n        loss = loss_main + self.lambda_fcr * loss_fcr\n        return {\"loss\": loss, \"loss_main\": loss_main, \"loss_fcr\": loss_fcr}\n\n    @torch.no_grad()\n    def validation_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n        text_emb = self._get_text_embeddings(prompts)\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=text_emb).sample\n        val_loss = F.mse_loss(noise_pred, noise)\n        return {\"val_loss\": val_loss}\n\n    # ------------------------------------------------------------------\n    # Inference API                                                     \n    # ------------------------------------------------------------------\n    @torch.no_grad()\n    def generate(self, prompts: List[str], num_inference_steps: int = 50, stride: int = 10):\n        \"\"\"Generates images using cached encoder features. This is a minimal\n        implementation purely for evaluation/latency measurement and does *not*\n        include the full Faster-Diffusion encoder-caching logic. Users can swap\n        this function in later phases with an optimised sampler.\n        \"\"\"\n        self.eval()\n        text_emb = self._get_text_embeddings(prompts)\n        latents = torch.randn(\n            len(prompts),\n            self.unet.in_channels,\n            64, 64,  # SD v1 latent spatial dims for 512\u00d7512\n            device=self.device,\n            dtype=self.dtype,\n        )\n        self.scheduler.set_timesteps(num_inference_steps)\n        encoder_cached = None\n        for i, t in enumerate(self.scheduler.timesteps):\n            if i % stride == 0 or encoder_cached is None:\n                # Full forward when cache is invalid / stride boundary\n                self._enc_feats = None\n                noise_pred = self.unet(latents, t, encoder_hidden_states=text_emb).sample\n                encoder_cached = self._enc_feats.detach()\n            else:\n                # Re-use cached encoder features (toy: just re-use noise_pred)\n                noise_pred = self.unet(latents, t, encoder_hidden_states=text_emb, encoder_hidden_states_mask=encoder_cached).sample  # type: ignore\n            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n        imgs = self.vae.decode(latents / 0.18215).sample\n        return imgs\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon preprocessing utilities and dataset registry. All dataset\u2010specific logic\nis isolated behind a registry so that new datasets can be plugged\u2010in by\nregistering a new class.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n################################################################################\n# Dataset registry                                                               \n################################################################################\n\n_DATASET_REGISTRY = {}\n\n\ndef register_dataset(name):\n    def _inner(cls):\n        _DATASET_REGISTRY[name] = cls\n        return cls\n\n    return _inner\n\n\n# -----------------------------------------------------------------------------\n# Toy dataset (used by smoke tests)                                             \n# -----------------------------------------------------------------------------\n\n@register_dataset(\"toy\")\nclass ToyDataset(Dataset):\n    \"\"\"A minimal random dataset for smoke tests. Returns (img, prompt).\"\"\"\n\n    def __init__(self, num_samples: int, image_size: int, split: str = \"train\") -\u003e None:\n        super().__init__()\n        self.num_samples = num_samples\n        self.image_size = image_size\n        self.split = split\n        self.transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n        )\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        img = torch.randn(3, self.image_size, self.image_size)\n        prompt = \"a synthetic prompt\"\n        return img, prompt\n\n\n# -----------------------------------------------------------------------------\n# Placeholder for real datasets                                                 \n# -----------------------------------------------------------------------------\n# PLACEHOLDER: Will be replaced with specific dataset classes (e.g., LAION, COCO)\n# Example:\n# @register_dataset(\"laion\")\n# class LaionDataset(Dataset):\n#     ...\n\n################################################################################\n# Dataloader builder                                                            \n################################################################################\n\ndef _build_transforms(image_size: int):\n    return transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ])\n\n\ndef build_dataloaders(\n    dataset_cfg: dict,\n    batch_size: int,\n    num_workers: int = 4,\n) -\u003e Tuple[DataLoader, DataLoader]:\n    \"\"\"Build train and validation dataloaders from a dataset config dict.\"\"\"\n    name = dataset_cfg[\"name\"]\n    params = dataset_cfg[\"params\"].copy()\n    DatasetCls = _DATASET_REGISTRY.get(name)\n    if DatasetCls is None:\n        raise ValueError(\n            f\"Dataset \u0027{name}\u0027 is not registered. Available: {list(_DATASET_REGISTRY.keys())}.\"\n        )\n\n    train_ds = DatasetCls(split=\"train\", **params)\n    val_ds = DatasetCls(split=\"val\", **params)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    return train_loader, val_loader\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"fcr_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Feature Consistency Regularisation experiments\"\nauthors = [\"AI Researcher \u003cai@local\u003e\"]\nrequires-python = \"\u003e=3.10\"\ndependencies = [\n    \"torch\u003e=2.0.0\",\n    \"torchvision\u003e=0.15.0\",\n    \"diffusers\u003e=0.20.0\",\n    \"transformers\u003e=4.30.0\",\n    \"torchmetrics\u003e=0.11.4\",\n    \"pyyaml\u003e=6.0\",\n    \"tqdm\u003e=4.64.0\",\n    \"matplotlib\u003e=3.7.0\",\n    \"seaborn\u003e=0.13.0\",\n    \"numpy\u003e=1.23.0\",\n    \"pillow\u003e=9.5.0\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest\u003e=7.0\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight configuration that runs entirely on random data and tiny models.\nruns:\n  - name: toy_baseline\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 128\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n  - name: toy_variant\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n", "train_py": "\"\"\"src/train.py\nRuns a single experiment variation specified by --run-id in a YAML config.\nAll experiment artefacts are written to \u003cresults-dir\u003e/\u003crun-id\u003e/.\nThe script prints a single JSON line with the final metrics to stdout so that\nmain.py can parse it. Stdout/stderr are also captured by main.py and mirrored\ninto log files.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport yaml\n\nfrom . import preprocess as pp\nfrom . import model as mdl\n\n################################################################################\n# Helper functions                                                               \n################################################################################\n\ndef set_seed(seed: int) -\u003e None:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef setup_device() -\u003e torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n################################################################################\n# Core training / validation utilities                                          \n################################################################################\n\ndef _train_one_epoch(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    max_norm: float | None = None,\n) -\u003e float:\n    model.train()\n    running_loss: List[float] = []\n    pbar = tqdm(dataloader, desc=f\"Train E{epoch}\", leave=False)\n    for batch in pbar:\n        optimizer.zero_grad(set_to_none=True)\n        batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n        out = model.training_step(batch_on_device)\n        loss: torch.Tensor = out[\"loss\"]\n        loss.backward()\n        if max_norm is not None:\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n        optimizer.step()\n        running_loss.append(loss.item())\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n    return float(sum(running_loss) / len(running_loss))\n\n\ndef _validate(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    device: torch.device,\n    epoch: int,\n) -\u003e float:\n    model.eval()\n    val_losses: List[float] = []\n    pbar = tqdm(dataloader, desc=f\"Val   E{epoch}\", leave=False)\n    with torch.no_grad():\n        for batch in pbar:\n            batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n            out = model.validation_step(batch_on_device)\n            loss: torch.Tensor = out[\"val_loss\"]\n            val_losses.append(loss.item())\n            pbar.set_postfix(val_loss=f\"{loss.item():.4f}\")\n    return float(sum(val_losses) / len(val_losses))\n\n################################################################################\n# Inference\u2010time utilities                                                      \n################################################################################\n\ndef _measure_inference_time(model: mdl.BaseExperimentModel, prompts: List[str], device: torch.device) -\u003e float:\n    \"\"\"Measures the average wall\u2010clock inference latency per sample.\n    For diffusion models this calls the generate method; for toy models this\n    performs a single forward pass.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        start = time.time()\n        _ = model.generate(prompts) if hasattr(model, \"generate\") else model(torch.randn(1, 3, 32, 32, device=device))\n        torch.cuda.synchronize() if device.type == \"cuda\" else None\n        end = time.time()\n    return float(end - start)\n\n################################################################################\n# Main entry point                                                              \n################################################################################\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation\")\n    parser.add_argument(\"--run-id\", required=True, help=\"Name of the run variation as defined in the YAML config\")\n    parser.add_argument(\"--config-file\", required=True, help=\"Path to YAML experiment config\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results will be stored\")\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n    args = parser.parse_args()\n\n    set_seed(args.seed)\n    device = setup_device()\n\n    # ------------------------------------------------------------------\n    # 1. Load configuration\n    # ------------------------------------------------------------------\n    with open(args.config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    run_cfg: Dict = None  # type: ignore\n    for r in config[\"runs\"]:\n        if r[\"name\"] == args.run_id:\n            run_cfg = r\n            break\n    if run_cfg is None:\n        raise ValueError(f\"Run id {args.run_id} not found in config {args.config_file}\")\n\n    # ------------------------------------------------------------------\n    # 2. Prepare IO paths\n    # ------------------------------------------------------------------\n    run_dir = Path(args.results_dir) / args.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / \"images\").mkdir(exist_ok=True)\n\n    # Save run config for reproducibility\n    with open(run_dir / \"cfg.yaml\", \"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # ------------------------------------------------------------------\n    # 3. Build dataset \u0026 dataloaders\n    # ------------------------------------------------------------------\n    train_loader, val_loader = pp.build_dataloaders(\n        dataset_cfg=run_cfg[\"dataset\"], batch_size=run_cfg[\"training\"][\"batch_size\"], num_workers=run_cfg[\"training\"].get(\"num_workers\", 4)\n    )\n\n    # ------------------------------------------------------------------\n    # 4. Build model \u0026 optimiser\n    # ------------------------------------------------------------------\n    model: mdl.BaseExperimentModel = mdl.build_model(run_cfg[\"model\"], device=device)\n    optimizer = optim.AdamW(model.parameters(), lr=run_cfg[\"training\"][\"lr\"])\n\n    # ------------------------------------------------------------------\n    # 5. Training loop\n    # ------------------------------------------------------------------\n    history: List[Dict] = []\n    epochs: int = run_cfg[\"training\"][\"epochs\"]\n    for epoch in range(1, epochs + 1):\n        train_loss = _train_one_epoch(\n            model, train_loader, optimizer, device, epoch, max_norm=run_cfg[\"training\"].get(\"grad_clip\", None)\n        )\n        val_loss = _validate(model, val_loader, device, epoch)\n        history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n\n    # ------------------------------------------------------------------\n    # 6. Final evaluation metrics (FID, CLIPScore, inference\u2010time)\n    # ------------------------------------------------------------------\n    fid_score = None\n    clip_score = None\n    if run_cfg.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        fid_metric = torchmetrics.image.fid.FrechetInceptionDistance(feature=2048).to(device)\n        clip_metric = torchmetrics.functional.clip_score\n        model.eval()\n        with torch.no_grad():\n            for imgs, prompts in tqdm(val_loader, desc=\"Computing FID/CLIP\"):\n                imgs = imgs.to(device)\n                # Generate images using current model\n                gen_imgs = model.generate(prompts) if hasattr(model, \"generate\") else imgs  # type: ignore\n                gen_imgs = (gen_imgs.clamp(-1, 1) + 1) / 2  # to [0,1]\n                fid_metric.update(gen_imgs, real=False)\n                fid_metric.update((imgs.clamp(-1, 1) + 1) / 2, real=True)\n                clip_score_batch = clip_metric(gen_imgs, prompts)\n                if clip_score is None:\n                    clip_score = clip_score_batch.mean()\n                else:\n                    clip_score += clip_score_batch.mean()\n        fid_score = float(fid_metric.compute())\n        clip_score = float(clip_score / len(val_loader)) if clip_score is not None else None\n\n    inference_latency = _measure_inference_time(model, prompts=[\"a test prompt\"], device=device)\n\n    # ------------------------------------------------------------------\n    # 7. Save artefacts \u0026 print JSON\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n\n    results = {\n        \"run_id\": args.run_id,\n        \"final_epoch\": epochs,\n        \"metrics\": {\n            \"train_loss\": history[-1][\"train_loss\"] if history else None,\n            \"val_loss\": history[-1][\"val_loss\"] if history else None,\n            \"fid\": fid_score,\n            \"clip_score\": clip_score,\n            \"inference_time\": inference_latency,\n        },\n        \"history\": history,\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    print(json.dumps(results))  # \u003c-- structured output expected by main.py\n\n\nif __name__ == \"__main__\":\n    main()\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-1-main-ablation
- Description: Objective / hypothesis: Quantify the quality-vs-speed trade-off introduced by Feature Consistency Regularisation (FCR) on the reference Stable-Diffusion-v1.5 UNet and determine the optimal λ by full ablation.  

Models: Stable-Diffusion-v1.5 UNet; no-change architecture.  

Datasets:  
• Fine-tune: 50 k LAION-Aesthetics (same captions, 512² images).  
• Evaluation: 30 k MS-COCO validation prompts (text-only, identical subset for every run).  

Pre-processing:  
• Resize / center-crop 512²; random horizontal flip (p=0.5) at training; deterministic center-crop at eval.  
• Captions lower-cased, tokenised with SD tokenizer (77 tokens, padded).  

Split / repetitions:  
• No explicit val split (few-step fine-tune); metrics are computed on held-out COCO prompts.  
• 3 random seeds per variation; report mean ±95 % CI.  
• Early-stopping on best moving-average FID over a 3 k prompt mini-val set (checked every 500 steps); otherwise 10 k steps max.  

Evaluation metrics:  
Primary – FID↓, CLIPScore↑.  
Secondary – Inception Score↑, wall-clock sampling time per image, #UNet forward passes, peak VRAM, theoretical FLOPs (ptflops).  

Comparisons:  
• FD-5 (quality upper-bound) vs FD-10 (speed upper-bound).  
• State-of-the-art DPMSolver++ (20 steps) reported as reference line only (not a run_variation).  

Hyper-parameter analysis: grid λ∈{0.05,0.1,0.2}; Δ_max fixed to 10 (empirically safe). Learning-rate 1 e-5, batch 128, AdamW (β1=0.9, β2=0.999).  

Robustness inside this experiment: stride sweep already captures quality decay; we additionally report encoder-feature variance σ²_Δ for Δ={1…10}.  

Efficiency measurements: `torch.cuda.Event` for time; `torch.cuda.memory_allocated()` for VRAM; FLOPs via fvcore; cost estimated from A100 price list.  

Example code snippet (evaluation – timing & FID).
```python
import torch, time, PIL, torchvision.transforms as T
from cleanfid import fid
from diffusers import StableDiffusionPipeline

def timed_sample(pipe, prompt):
    t0=time.time()
    img=pipe(prompt,num_inference_steps=25).images[0]
    dt=time.time()-t0
    return img,dt
pipe=StableDiffusionPipeline.from_pretrained(checkpoint,torch_dtype=torch.float16).to("cuda")
pipe.set_progress_bar_config(disable=True)
latencies,imgs=[],[]
for p in prompts:
    img,dt=timed_sample(pipe,p)
    latencies.append(dt); imgs.append(img)
print(f"mean time {sum(latencies)/len(latencies):.3f}s")
fid_score=fid.compute_fid(imgs,gt_path="coco_val_30k")
```

Resource budget: ≤40 GPU-hours per variation (fine-tune + eval) → 200 GPU-hours total, fits on one A100 with 80 GB.  

Success criterion for exp-1: the best λ configuration must keep ΔFID ≤+5 % vs FD-5 while achieving ≥1.4 × speed-up.
- Run Variations: ['FD-5-baseline', 'FD-10-baseline', 'FCR(λ=0.05)-FD-10', 'FCR(λ=0.10)-FD-10', 'FCR(λ=0.20)-FD-10']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates the results of all experiment variations and produces comparison\nfigures under \u003cresults-dir\u003e/images/.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\n################################################################################\n# Plot helpers                                                                  \n################################################################################\n\ndef _make_bar(ax, names: List[str], values: List[float], title: str, ylabel: str):\n    palette = sns.color_palette(\"Set2\", len(names))\n    bars = ax.bar(names, values, color=palette)\n    ax.set_title(title)\n    ax.set_ylabel(ylabel)\n    for bar, val in zip(bars, values):\n        height = bar.get_height()\n        ax.annotate(f\"{val:.3f}\", xy=(bar.get_x() + bar.get_width() / 2, height), xytext=(0, 3), textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate all experimental runs and plot comparisons\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Path where individual run folders live\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    image_dir = results_dir / \"images\"\n    image_dir.mkdir(exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # 1. Load result JSONs\n    # ------------------------------------------------------------------\n    run_results: Dict[str, Dict] = {}\n    for run_path in results_dir.iterdir():\n        if run_path.is_dir() and (run_path / \"results.json\").exists():\n            with open(run_path / \"results.json\", \"r\") as f:\n                run_results[run_path.name] = json.load(f)\n\n    if not run_results:\n        print(\"{}\")\n        return\n\n    names = list(run_results.keys())\n    fid_values = [run_results[n][\"metrics\"].get(\"fid\", None) for n in names]\n    clip_values = [run_results[n][\"metrics\"].get(\"clip_score\", None) for n in names]\n    lat_values = [run_results[n][\"metrics\"].get(\"inference_time\", None) for n in names]\n\n    # ------------------------------------------------------------------\n    # 2. Plot FID and CLIPScore\n    # ------------------------------------------------------------------\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    if any(v is not None for v in fid_values):\n        _make_bar(axes[0], names, [v if v is not None else 0 for v in fid_values], \"FID (lower=better)\", \"FID\")\n    if any(v is not None for v in clip_values):\n        _make_bar(axes[1], names, [v if v is not None else 0 for v in clip_values], \"CLIPScore (higher=better)\", \"CLIPScore\")\n    plt.tight_layout()\n    plt.savefig(image_dir / \"quality_metrics.pdf\", bbox_inches=\"tight\")\n\n    # ------------------------------------------------------------------\n    # 3. Plot inference latency\n    # ------------------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    _make_bar(ax, names, lat_values, \"Inference latency\", \"seconds / image\")\n    plt.tight_layout()\n    plt.savefig(image_dir / \"inference_latency.pdf\", bbox_inches=\"tight\")\n\n    # ------------------------------------------------------------------\n    # 4. Print structured comparison summary\n    # ------------------------------------------------------------------\n    summary = {\n        \"best_fid_run\": min([(v, n) for n, v in zip(names, fid_values) if v is not None], default=(None, None))[1],\n        \"best_clip_run\": max([(v, n) for n, v in zip(names, clip_values) if v is not None], default=(None, None))[1],\n        \"fastest_run\": min([(v, n) for n, v in zip(names, lat_values) if v is not None], default=(None, None))[1],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# Complete configuration for experiment ``exp-1-main-ablation``.\n# Five run variations are defined: two baselines (FD-5 \u0026 FD-10) and three FCR\n# ablations with \u03bb \u2208 {0.05, 0.10, 0.20}, all evaluated with stride-10 caching.\n\nruns:\n  # ---------------------------------------------------------------------------\n  # Baseline \u2013 Faster Diffusion, stride-5 (quality upper-bound)\n  # ---------------------------------------------------------------------------\n  - name: FD-5-baseline\n    dataset:\n      name: laion_aesthetics_512\n      params:\n        image_size: 512\n        subset_size: 50000   # 50k samples for quick fine-tune / evaluation\n    model:\n      name: diffusion_baseline\n      params:\n        pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5\n        stride: 5\n        use_fp16: true\n    training:\n      epochs: 0            # inference-only baseline (no fine-tuning)\n      batch_size: 2        # irrelevant \u2013 kept small for memory\n      lr: 0.0\n    evaluation:\n      compute_fid: true\n\n  # ---------------------------------------------------------------------------\n  # Baseline \u2013 Faster Diffusion, stride-10 (speed upper-bound)\n  # ---------------------------------------------------------------------------\n  - name: FD-10-baseline\n    dataset:\n      name: laion_aesthetics_512\n      params:\n        image_size: 512\n        subset_size: 50000\n    model:\n      name: diffusion_baseline\n      params:\n        pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5\n        stride: 10\n        use_fp16: true\n    training:\n      epochs: 0\n      batch_size: 2\n      lr: 0.0\n    evaluation:\n      compute_fid: true\n\n  # ---------------------------------------------------------------------------\n  # Proposed \u2013 FCR fine-tune, \u03bb = 0.05, stride-10 sampling\n  # ---------------------------------------------------------------------------\n  - name: FCR(\u03bb=0.05)-FD-10\n    dataset:\n      name: laion_aesthetics_512\n      params:\n        image_size: 512\n        subset_size: 50000\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5\n        lambda_fcr: 0.05\n        delta_max: 10\n        stride: 10\n        use_fp16: true\n    training:\n      epochs: 10          # \u224810k optimiser steps for proof-of-concept\n      batch_size: 4\n      lr: 1e-5\n      grad_clip: 1.0\n    evaluation:\n      compute_fid: true\n\n  # ---------------------------------------------------------------------------\n  # Proposed \u2013 FCR fine-tune, \u03bb = 0.10, stride-10 sampling (expected best)\n  # ---------------------------------------------------------------------------\n  - name: FCR(\u03bb=0.10)-FD-10\n    dataset:\n      name: laion_aesthetics_512\n      params:\n        image_size: 512\n        subset_size: 50000\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5\n        lambda_fcr: 0.10\n        delta_max: 10\n        stride: 10\n        use_fp16: true\n    training:\n      epochs: 10\n      batch_size: 4\n      lr: 1e-5\n      grad_clip: 1.0\n    evaluation:\n      compute_fid: true\n\n  # ---------------------------------------------------------------------------\n  # Proposed \u2013 FCR fine-tune, \u03bb = 0.20, stride-10 sampling (strong regulariser)\n  # ---------------------------------------------------------------------------\n  - name: FCR(\u03bb=0.20)-FD-10\n    dataset:\n      name: laion_aesthetics_512\n      params:\n        image_size: 512\n        subset_size: 50000\n    model:\n      name: diffusion_fcr\n      params:\n        pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5\n        lambda_fcr: 0.20\n        delta_max: 10\n        stride: 10\n        use_fp16: true\n    training:\n      epochs: 10\n      batch_size: 4\n      lr: 1e-5\n      grad_clip: 1.0\n    evaluation:\n      compute_fid: true\n", "main_py": "\"\"\"src/main.py\nOrchestrates the execution of all experiment variations defined in a YAML file.\nFor each run it spawns src.train as a subprocess, tee-ing stdout/stderr into\nboth the console and per-run log files. After all runs complete, it calls\nsrc.evaluate to aggregate and visualise the results.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\n################################################################################\n# Utilities                                                                     \n################################################################################\n\ndef _tee_stream(stream, log_file):\n    \"\"\"Mirrors a stream (stdout/stderr) into a log file and the parent stream.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        sys.stdout.buffer.write(line) if log_file.name.endswith(\"stdout.log\") else sys.stderr.buffer.write(line)\n        log_file.buffer.write(line)\n    stream.close()\n\n\ndef _run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n\n    with open(stdout_path, \"wb\") as fout, open(stderr_path, \"wb\") as ferr:\n        threads = [\n            threading.Thread(target=_tee_stream, args=(proc.stdout, fout), daemon=True),\n            threading.Thread(target=_tee_stream, args=(proc.stderr, ferr), daemon=True),\n        ]\n        for t in threads:\n            t.start()\n        proc.wait()\n        for t in threads:\n            t.join()\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess {\u0027 \u0027.join(cmd)} failed with exit code {proc.returncode}\")\n\n################################################################################\n# Main                                                                          \n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run all experiments defined in a config YAML\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Where all outputs should be saved\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Sequentially run each variation\n    # ------------------------------------------------------------------\n    for run in cfg[\"runs\"]:\n        run_id = run[\"name\"]\n        print(f\"==== Running experiment: {run_id} ====\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(exist_ok=True)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--run-id\",\n            run_id,\n            \"--config-file\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n        ]\n        _run_subprocess(cmd, cwd=Path.cwd(), stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs \u2192 aggregate / visualise\n    # ------------------------------------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel registry and core algorithm implementations including the Feature\nConsistency Regularisation (FCR) diffusion model.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n_MODEL_REGISTRY: Dict[str, Any] = {}\n\n################################################################################\n# Registry helpers                                                              \n################################################################################\n\ndef register_model(name: str):\n    def decorator(cls):\n        _MODEL_REGISTRY[name] = cls\n        return cls\n\n    return decorator\n\n\ndef build_model(model_cfg: Dict[str, Any], device: torch.device):\n    name = model_cfg[\"name\"]\n    params = model_cfg.get(\"params\", {})\n    ModelCls = _MODEL_REGISTRY.get(name)\n    if ModelCls is None:\n        raise ValueError(\n            f\"Model \u0027{name}\u0027 is not registered. Available: {list(_MODEL_REGISTRY.keys())}.\"\n        )\n    model: BaseExperimentModel = ModelCls(params=params, device=device)  # type: ignore\n    return model.to(device)\n\n################################################################################\n# Base class                                                                    \n################################################################################\n\nclass BaseExperimentModel(nn.Module):\n    \"\"\"Abstract base class that every model follows.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    # ------------------------------------------------------------------\n    # Required API\n    # ------------------------------------------------------------------\n    def training_step(self, batch):\n        raise NotImplementedError\n\n    def validation_step(self, batch):\n        raise NotImplementedError\n\n    def generate(self, prompts: List[str]):\n        \"\"\"Optional for generative models.\"\"\"\n        raise NotImplementedError\n\n################################################################################\n# Toy classification model (for smoke tests)                                    \n################################################################################\n\n@register_model(\"toy\")\nclass ToyModel(BaseExperimentModel):\n    \"\"\"Simple 3-layer MLP classifier on flattened images. Used by smoke tests.\"\"\"\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        self.input_dim = params[\"input_dim\"]\n        hidden_dim = params.get(\"hidden_dim\", 128)\n        num_classes = params.get(\"num_classes\", 10)\n        self.net = nn.Sequential(\n            nn.Linear(self.input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes),\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.num_classes = num_classes\n        self.to(device)\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x.view(x.size(0), -1))\n\n    def training_step(self, batch):\n        imgs, _ = batch  # prompts ignored\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {\"loss\": loss}\n\n    def validation_step(self, batch):\n        imgs, _ = batch\n        logits = self.forward(imgs)\n        labels = torch.randint(0, self.num_classes, (imgs.size(0),), device=imgs.device)\n        loss = self.loss_fn(logits, labels)\n        return {\"val_loss\": loss}\n\n    def generate(self, prompts):\n        # Not a generative model \u2013 return random tensor for API compliance\n        return torch.randn(1, 3, 32, 32, device=next(self.parameters()).device)\n\n################################################################################\n# Diffusion backbone imports                                                    \n################################################################################\n\n# Import heavy deps lazily so that smoke tests remain lightweight\ntry:\n    from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n    from transformers import CLIPTextModel, CLIPTokenizer\nexcept ImportError:  # pragma: no cover\n    AutoencoderKL = UNet2DConditionModel = DDPMScheduler = CLIPTextModel = CLIPTokenizer = None  # type: ignore\n\n################################################################################\n# Diffusion w/ FCR model                                                        \n################################################################################\n\n\ndef _safe_first_tensor(x):\n    if isinstance(x, (tuple, list)):\n        return x[0]\n    return x\n\n\n@register_model(\"diffusion_fcr\")\nclass DiffusionFCRModel(BaseExperimentModel):\n    \"\"\"Stable-Diffusion v1.5 UNet fine-tuned with Feature Consistency Regularisation.\"\"\"\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        super().__init__()\n        if AutoencoderKL is None:\n            raise ImportError(\"diffusers and transformers must be installed for diffusion models.\")\n\n        model_name = params.get(\"pretrained_model_name_or_path\", \"runwayml/stable-diffusion-v1-5\")\n        self.dtype = torch.float16 if params.get(\"use_fp16\", True) else torch.float32\n        # Core components\n        self.vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\", torch_dtype=self.dtype).to(device)\n        self.unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\", torch_dtype=self.dtype).to(device)\n        self.text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\", torch_dtype=self.dtype).to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n        self.scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n\n        # FCR specific hyper-params\n        self.lambda_fcr: float = float(params.get(\"lambda_fcr\", 0.1))\n        self.delta_max: int = int(params.get(\"delta_max\", 10))\n        self.default_stride: int = int(params.get(\"stride\", 10))\n\n        self.device = device\n        # Forward hook to capture encoder features\n        self._enc_feats: Optional[torch.Tensor] = None\n\n        def _hook_fn(_, __, output):\n            self._enc_feats = _safe_first_tensor(output)\n\n        # Register hook on the last down block (deepest UNet level)\n        self.unet.down_blocks[-1].register_forward_hook(_hook_fn)\n\n    # ------------------------------------------------------------------\n    # Helper methods                                                     \n    # ------------------------------------------------------------------\n    def _encode_images(self, imgs: torch.Tensor) -\u003e torch.Tensor:\n        latents = self.vae.encode(imgs).latent_dist.sample()\n        return latents * 0.18215  # SD scaling factor\n\n    def _get_text_embeddings(self, prompts: List[str]) -\u003e torch.Tensor:\n        tokens = self.tokenizer(\n            prompts,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).to(self.device)\n        return self.text_encoder(tokens.input_ids)[0]\n\n    # ------------------------------------------------------------------\n    # Training \u0026 validation                                             \n    # ------------------------------------------------------------------\n    def training_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n\n        # Sample main timestep\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n\n        # Text conditioning\n        text_emb = self._get_text_embeddings(prompts)\n\n        # First forward pass \u2013 capture encoder feats\n        self._enc_feats = None\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=text_emb).sample\n        loss_main = F.mse_loss(noise_pred, noise)\n        f1 = self._enc_feats.detach() if self._enc_feats is not None else None\n\n        # Second forward pass for FCR (only if lambda_fcr\u003e0)\n        loss_fcr = torch.tensor(0.0, device=self.device)\n        if self.lambda_fcr \u003e 0 and f1 is not None:\n            delta = torch.randint(1, self.delta_max + 1, (bsz,), device=self.device)\n            t2 = torch.clamp(t - delta, min=0)\n            noisy_latents2 = self.scheduler.add_noise(latents, noise, t2)\n\n            self._enc_feats = None\n            _ = self.unet(noisy_latents2, t2, encoder_hidden_states=text_emb)\n            f2 = self._enc_feats.detach()\n            loss_fcr = (f1 - f2).pow(2).mean()\n\n        loss = loss_main + self.lambda_fcr * loss_fcr\n        return {\"loss\": loss, \"loss_main\": loss_main, \"loss_fcr\": loss_fcr}\n\n    @torch.no_grad()\n    def validation_step(self, batch):\n        imgs, prompts = batch\n        imgs = imgs.to(self.device, dtype=self.dtype)\n        latents = self._encode_images(imgs)\n        bsz = latents.size(0)\n        t = torch.randint(0, self.scheduler.num_train_timesteps, (bsz,), device=self.device, dtype=torch.long)\n        noise = torch.randn_like(latents)\n        noisy_latents = self.scheduler.add_noise(latents, noise, t)\n        noise_pred = self.unet(noisy_latents, t, encoder_hidden_states=self._get_text_embeddings(prompts)).sample\n        val_loss = F.mse_loss(noise_pred, noise)\n        return {\"val_loss\": val_loss}\n\n    # ------------------------------------------------------------------\n    # Inference / generation                                            \n    # ------------------------------------------------------------------\n    @torch.no_grad()\n    def generate(self, prompts: List[str], num_inference_steps: int = 50, stride: Optional[int] = None):\n        \"\"\"Very light-weight Faster-Diffusion-style sampler with encoder caching.\n        It only re-computes the UNet every `stride` steps.\n        \"\"\"\n        stride = stride or self.default_stride\n        self.eval()\n        text_emb = self._get_text_embeddings(prompts)\n        latents = torch.randn(\n            len(prompts),\n            self.unet.config.in_channels,\n            64,\n            64,\n            device=self.device,\n            dtype=self.dtype,\n        )\n        self.scheduler.set_timesteps(num_inference_steps)\n        cached_noise_pred: Optional[torch.Tensor] = None\n        last_compute_step = -1\n        for i, t in enumerate(self.scheduler.timesteps):\n            if cached_noise_pred is None or (i - last_compute_step) \u003e= stride:\n                cached_noise_pred = self.unet(latents, t, encoder_hidden_states=text_emb).sample\n                last_compute_step = i\n            latents = self.scheduler.step(cached_noise_pred, t, latents).prev_sample\n        imgs = self.vae.decode(latents / 0.18215).sample\n        return imgs\n\n################################################################################\n# Baseline (\u03bb=0) diffusion model                                               \n################################################################################\n\n@register_model(\"diffusion_baseline\")\nclass DiffusionBaselineModel(DiffusionFCRModel):\n    \"\"\"Identical to DiffusionFCRModel but with \u03bb=0 (no FCR) and configurable stride.\"\"\"\n\n    def __init__(self, params: Dict[str, Any], device: torch.device):\n        # Force lambda_fcr=0 to disable auxiliary loss.\n        params = params.copy()\n        params[\"lambda_fcr\"] = 0.0\n        super().__init__(params=params, device=device)\n\n    # No additional FCR term \u2013 already disabled in parent training_step via lambda=0.\n\n    def generate(self, prompts: List[str]):\n        # Use default_stride inherited from params (FD-5 / FD-10 etc.)\n        return super().generate(prompts, stride=self.default_stride)\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon preprocessing utilities and dataset registry. All dataset\u2010specific logic\nis isolated behind a registry so that new datasets can be plugged\u2010in by\nregistering a new class.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple, List, Dict, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\n\n# External library used for real datasets\ntry:\n    from datasets import load_dataset  # type: ignore\nexcept ImportError:  # pragma: no cover\n    load_dataset = None  # type: ignore\n\n################################################################################\n# Dataset registry                                                               \n################################################################################\n\n_DATASET_REGISTRY: Dict[str, Any] = {}\n\n\ndef register_dataset(name):\n    def _inner(cls):\n        _DATASET_REGISTRY[name] = cls\n        return cls\n\n    return _inner\n\n################################################################################\n# Utility transforms                                                            \n################################################################################\n\ndef _make_train_transform(image_size: int):\n    return transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n        ]\n    )\n\n\ndef _make_eval_transform(image_size: int):\n    return transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size), interpolation=Image.BICUBIC),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n        ]\n    )\n\n################################################################################\n# Toy dataset (used by smoke tests)                                             \n################################################################################\n\n@register_dataset(\"toy\")\nclass ToyDataset(Dataset):\n    \"\"\"A minimal random dataset for smoke tests. Returns (img, prompt).\"\"\"\n\n    def __init__(self, num_samples: int, image_size: int, split: str = \"train\") -\u003e None:\n        super().__init__()\n        self.num_samples = num_samples\n        self.image_size = image_size\n        self.split = split\n        self.transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n        )\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        img = torch.randn(3, self.image_size, self.image_size)\n        prompt = \"a synthetic prompt\"\n        return img, prompt\n\n################################################################################\n# LAION-Aesthetics 512\u00b2 subset (for fine-tuning \u0026 validation)                   \n################################################################################\n\n@register_dataset(\"laion_aesthetics_512\")\nclass LaionAestheticsDataset(Dataset):\n    \"\"\"Wrapper around the limingcv/LAION_Aesthetics_512 HF dataset.\n\n    Parameters accepted via **params in YAML:\n      image_size (int):          Final resolution after transforms.\n      subset_size (int|None):    Optional. Randomly sample at most this many\n                                 elements from the split for quick experiments.\n    \"\"\"\n\n    def __init__(self, split: str = \"train\", image_size: int = 512, subset_size: int | None = None):\n        if load_dataset is None:\n            raise ImportError(\"`datasets` library is required for LaionAestheticsDataset but is not installed.\")\n        self.split = split\n        # The published dataset only has a single split called \"train\". We will\n        # use it for both training and validation, sampling a disjoint subset\n        # for the latter if requested.\n        full_ds = load_dataset(\"limingcv/LAION_Aesthetics_512\", split=\"train\", streaming=False)\n        indices: List[int]\n        if subset_size is not None and subset_size \u003c len(full_ds):\n            random.seed(42 if split == \"val\" else 123)\n            indices = random.sample(range(len(full_ds)), subset_size)\n            self.ds = full_ds.select(indices)\n        else:\n            self.ds = full_ds\n        # Data augmentation / preprocessing\n        self.transform = _make_train_transform(image_size) if split == \"train\" else _make_eval_transform(image_size)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        image: Image.Image = item.get(\"image\", None)\n        if image is None:\n            raise KeyError(\"Expected key `image` not found in dataset item.\")\n        caption: str = (\n            item.get(\"text\")\n            or item.get(\"TEXT\")\n            or item.get(\"caption\")\n            or item.get(\"prompt\")\n            or \"an image\"\n        )\n        img_tensor = self.transform(image)\n        return img_tensor, str(caption).lower()\n\n################################################################################\n# MS-COCO 2014 validation (prompts + images)                                    \n################################################################################\n\n@register_dataset(\"mscoco2014_val\")\nclass MSCOCO2014ValDataset(Dataset):\n    \"\"\"30k MS-COCO 2014 validation set prompts \u0026 images used for evaluation.\"\"\"\n\n    def __init__(self, image_size: int = 512, subset_size: int | None = 30000, split: str = \"val\"):\n        if load_dataset is None:\n            raise ImportError(\"`datasets` library is required for MSCOCO2014ValDataset but is not installed.\")\n        if split != \"val\":\n            raise ValueError(\"MSCOCO2014ValDataset only supports split=\u0027val\u0027.\")\n        # HF dataset \"coco_captions\" provides images + multiple captions. We\n        # pick the first caption for simplicity.\n        full_ds = load_dataset(\"coco_captions\", \"2014\", split=\"validation\")\n        if subset_size is not None and subset_size \u003c len(full_ds):\n            random.seed(2023)\n            indices = random.sample(range(len(full_ds)), subset_size)\n            self.ds = full_ds.select(indices)\n        else:\n            self.ds = full_ds\n        self.transform = _make_eval_transform(image_size)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        image: Image.Image = item[\"image\"]\n        captions: List[str] = item[\"captions\"]\n        caption = captions[0] if captions else \"a photo\"\n        return self.transform(image), caption.lower()\n\n################################################################################\n# Dataloader builder                                                            \n################################################################################\n\ndef build_dataloaders(\n    dataset_cfg: dict,\n    batch_size: int,\n    num_workers: int = 4,\n) -\u003e Tuple[DataLoader, DataLoader]:\n    \"\"\"Build train and validation dataloaders from a dataset config dict.\"\"\"\n    name = dataset_cfg[\"name\"]\n    params = dataset_cfg.get(\"params\", {}).copy()\n    DatasetCls = _DATASET_REGISTRY.get(name)\n    if DatasetCls is None:\n        raise ValueError(\n            f\"Dataset \u0027{name}\u0027 is not registered. Available: {list(_DATASET_REGISTRY.keys())}.\"\n        )\n\n    train_ds = DatasetCls(split=\"train\", **params)\n    val_ds = DatasetCls(split=\"val\", **params)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=False,\n    )\n    return train_loader, val_loader\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"fcr_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Feature Consistency Regularisation experiments\"\nauthors = [\"AI Researcher \u003cai@local\u003e\"]\nrequires-python = \"\u003e=3.10\"\ndependencies = [\n    \"torch\u003e=2.0.0\",\n    \"torchvision\u003e=0.15.0\",\n    \"diffusers\u003e=0.20.0\",\n    \"transformers\u003e=4.30.0\",\n    \"torchmetrics\u003e=0.11.4\",\n    \"pyyaml\u003e=6.0\",\n    \"tqdm\u003e=4.64.0\",\n    \"matplotlib\u003e=3.7.0\",\n    \"seaborn\u003e=0.13.0\",\n    \"numpy\u003e=1.23.0\",\n    \"pillow\u003e=9.5.0\",\n    \"datasets\u003e=2.12.0\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest\u003e=7.0\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke-test versions of every run variation defined in full_experiment.yaml.\n# All runs use a tiny synthetic dataset and a toy MLP model so that the complete\n# pipeline (training \u279c metrics \u279c evaluation scripts) can be validated in seconds.\n\nruns:\n  - name: FD-5-baseline\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: FD-10-baseline\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: FCR(\u03bb=0.05)-FD-10\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: FCR(\u03bb=0.10)-FD-10\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n\n  - name: FCR(\u03bb=0.20)-FD-10\n    dataset:\n      name: toy\n      params:\n        num_samples: 64\n        image_size: 32\n    model:\n      name: toy\n      params:\n        input_dim: 3*32*32\n        hidden_dim: 64\n        num_classes: 10\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 0.001\n    evaluation:\n      compute_fid: false\n", "train_py": "\"\"\"src/train.py\nRuns a single experiment variation specified by --run-id in a YAML config.\nAll experiment artefacts are written to \u003cresults-dir\u003e/\u003crun-id\u003e/.\nThe script prints a single JSON line with the final metrics to stdout so that\nmain.py can parse it. Stdout/stderr are also captured by main.py and mirrored\ninto log files.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport yaml\n\nfrom . import preprocess as pp\nfrom . import model as mdl\n\n################################################################################\n# Helper functions                                                               \n################################################################################\n\ndef set_seed(seed: int) -\u003e None:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef setup_device() -\u003e torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n################################################################################\n# Core training / validation utilities                                          \n################################################################################\n\ndef _train_one_epoch(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    optimizer: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    max_norm: float | None = None,\n) -\u003e float:\n    model.train()\n    running_loss: List[float] = []\n    pbar = tqdm(dataloader, desc=f\"Train E{epoch}\", leave=False)\n    for batch in pbar:\n        optimizer.zero_grad(set_to_none=True)\n        batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n        out = model.training_step(batch_on_device)\n        loss: torch.Tensor = out[\"loss\"]\n        loss.backward()\n        if max_norm is not None:\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n        optimizer.step()\n        running_loss.append(loss.item())\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n    return float(sum(running_loss) / len(running_loss))\n\n\ndef _validate(\n    model: mdl.BaseExperimentModel,\n    dataloader: DataLoader,\n    device: torch.device,\n    epoch: int,\n) -\u003e float:\n    model.eval()\n    val_losses: List[float] = []\n    pbar = tqdm(dataloader, desc=f\"Val   E{epoch}\", leave=False)\n    with torch.no_grad():\n        for batch in pbar:\n            batch_on_device = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n            out = model.validation_step(batch_on_device)\n            loss: torch.Tensor = out[\"val_loss\"]\n            val_losses.append(loss.item())\n            pbar.set_postfix(val_loss=f\"{loss.item():.4f}\")\n    return float(sum(val_losses) / len(val_losses))\n\n################################################################################\n# Inference\u2010time utilities                                                      \n################################################################################\n\ndef _measure_inference_time(model: mdl.BaseExperimentModel, prompts: List[str], device: torch.device) -\u003e float:\n    \"\"\"Measures the average wall\u2010clock inference latency per sample.\n    For diffusion models this calls the generate method; for toy models this\n    performs a single forward pass.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        start = time.time()\n        _ = model.generate(prompts) if hasattr(model, \"generate\") else model(torch.randn(1, 3, 32, 32, device=device))\n        torch.cuda.synchronize() if device.type == \"cuda\" else None\n        end = time.time()\n    return float(end - start)\n\n################################################################################\n# Main entry point                                                              \n################################################################################\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation\")\n    parser.add_argument(\"--run-id\", required=True, help=\"Name of the run variation as defined in the YAML config\")\n    parser.add_argument(\"--config-file\", required=True, help=\"Path to YAML experiment config\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results will be stored\")\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n    args = parser.parse_args()\n\n    set_seed(args.seed)\n    device = setup_device()\n\n    # ------------------------------------------------------------------\n    # 1. Load configuration\n    # ------------------------------------------------------------------\n    with open(args.config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    run_cfg: Dict = None  # type: ignore\n    for r in config[\"runs\"]:\n        if r[\"name\"] == args.run_id:\n            run_cfg = r\n            break\n    if run_cfg is None:\n        raise ValueError(f\"Run id {args.run_id} not found in config {args.config_file}\")\n\n    # ------------------------------------------------------------------\n    # 2. Prepare IO paths\n    # ------------------------------------------------------------------\n    run_dir = Path(args.results_dir) / args.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / \"images\").mkdir(exist_ok=True)\n\n    # Save run config for reproducibility\n    with open(run_dir / \"cfg.yaml\", \"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n\n    # ------------------------------------------------------------------\n    # 3. Build dataset \u0026 dataloaders\n    # ------------------------------------------------------------------\n    train_loader, val_loader = pp.build_dataloaders(\n        dataset_cfg=run_cfg[\"dataset\"], batch_size=run_cfg[\"training\"][\"batch_size\"], num_workers=run_cfg[\"training\"].get(\"num_workers\", 4)\n    )\n\n    # ------------------------------------------------------------------\n    # 4. Build model \u0026 optimiser\n    # ------------------------------------------------------------------\n    model: mdl.BaseExperimentModel = mdl.build_model(run_cfg[\"model\"], device=device)\n    optimizer = optim.AdamW(model.parameters(), lr=run_cfg[\"training\"][\"lr\"])\n\n    # ------------------------------------------------------------------\n    # 5. Training loop\n    # ------------------------------------------------------------------\n    history: List[Dict] = []\n    epochs: int = run_cfg[\"training\"][\"epochs\"]\n    for epoch in range(1, epochs + 1):\n        train_loss = _train_one_epoch(\n            model, train_loader, optimizer, device, epoch, max_norm=run_cfg[\"training\"].get(\"grad_clip\", None)\n        )\n        val_loss = _validate(model, val_loader, device, epoch)\n        history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n\n    # ------------------------------------------------------------------\n    # 6. Final evaluation metrics (FID, CLIPScore, inference\u2010time)\n    # ------------------------------------------------------------------\n    fid_score = None\n    clip_score = None\n    if run_cfg.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        fid_metric = torchmetrics.image.fid.FrechetInceptionDistance(feature=2048).to(device)\n        clip_metric = torchmetrics.functional.clip_score\n        model.eval()\n        with torch.no_grad():\n            for imgs, prompts in tqdm(val_loader, desc=\"Computing FID/CLIP\"):\n                imgs = imgs.to(device)\n                # Generate images using current model\n                gen_imgs = model.generate(prompts) if hasattr(model, \"generate\") else imgs  # type: ignore\n                gen_imgs = (gen_imgs.clamp(-1, 1) + 1) / 2  # to [0,1]\n                fid_metric.update(gen_imgs, real=False)\n                fid_metric.update((imgs.clamp(-1, 1) + 1) / 2, real=True)\n                clip_score_batch = clip_metric(gen_imgs, prompts)\n                if clip_score is None:\n                    clip_score = clip_score_batch.mean()\n                else:\n                    clip_score += clip_score_batch.mean()\n        fid_score = float(fid_metric.compute())\n        clip_score = float(clip_score / len(val_loader)) if clip_score is not None else None\n\n    inference_latency = _measure_inference_time(model, prompts=[\"a test prompt\"], device=device)\n\n    # ------------------------------------------------------------------\n    # 7. Save artefacts \u0026 print JSON\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n\n    results = {\n        \"run_id\": args.run_id,\n        \"final_epoch\": epochs,\n        \"metrics\": {\n            \"train_loss\": history[-1][\"train_loss\"] if history else None,\n            \"val_loss\": history[-1][\"val_loss\"] if history else None,\n            \"fid\": fid_score,\n            \"clip_score\": clip_score,\n            \"inference_time\": inference_latency,\n        },\n        \"history\": history,\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    print(json.dumps(results))  # \u003c-- structured output expected by main.py\n\n\nif __name__ == \"__main__\":\n    main()\n"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": true,
    "experiment_code_issue": ""
}
