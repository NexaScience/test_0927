
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- To prevent labels, titles, and legends from overlapping, use `plt.tight_layout()` before saving the figure.
- All figures must be saved to `{results_dir}/images/` directory in .pdf format (e.g., using `plt.savefig(os.path.join(results_dir, "images", "filename.pdf"), bbox_inches="tight")`).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback




# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "In BOIL the whole learning curve is compressed into a single scalar through a sigmoid‐weighted average of the rewards.  This scalar ignores instability of the curve.  In Deep RL it is common that a hyper-parameter setting reaches a high reward but with large oscillations – such settings are often unreliable at test time.  Because the GP sees no penalty for instability it may keep sampling such noisy settings, wasting evaluations and slowing convergence.",
    "Methods": "Stability–Aware Curve Compression (SACC).\nModification to BOIL: after computing the sigmoid–weighted average m(curve) we subtract a stability penalty proportional to the standard deviation of the last K% of the curve.\n    score = m(curve) – λ · std(curve[ -K: ])\nOnly three extra lines are needed inside apply_one_transform_logistic.  λ ≥ 0 is an additional scalar hyper-parameter that is learned together with the sigmoid midpoint and growth by maximising the GP log-marginal likelihood (just append λ to the vector that is optimised).\nMotivation:   std(curve_tail) is an inexpensive proxy for reliability; subtracting it keeps the objective one-dimensional so BOIL’s GP, acquisition function and data-augmentation remain untouched.",
    "Experimental Setup": "Task: tune learning rate and target-network update period of a DQN agent on CartPole-v0.\nBaselines: (1) Original BOIL, (2) BOIL+SACC (ours).\nBudget: 25 BO iterations, 5 random initial points.\nK: last 10 % of episodes, λ initialised to 1.0 with bounds [0,5].\nMetrics:\n  • Best validation reward after 25 evaluations.\n  • Number of evaluations required to reach an average reward ≥ 195.\n  • Post-training stability: std of reward over 20 extra evaluation episodes.\nSame random seeds are used for both methods.",
    "Experimental Code": "import numpy as np\n\ndef transform_logistic_sacc(curve, midpoint, growth, max_ep, lam=1.0, tail_frac=0.1):\n    # original sigmoid weighting\n    x_scaled = np.linspace(-6, 6, len(curve))\n    weights = 1/(1+np.exp(-growth*(x_scaled-midpoint)))\n    m = np.mean(curve*weights)\n    # stability penalty (std of last K% of curve)\n    k = max(1, int(len(curve)*tail_frac))\n    stability = np.std(curve[-k:])\n    return m - lam*stability\n\n# drop-in replacement inside BOIL\n# in apply_one_transform_logistic simply call transform_logistic_sacc with lam learned from GP\n",
    "Expected Result": "Across 10 independent runs the proposed BOIL+SACC is expected to:\n  • Reach the success threshold (avg reward 195) after ≈12 evaluations versus ≈17 for BOIL.\n  • Achieve ~5-10 % higher best-of-run reward.\n  • Produce policies whose evaluation-phase reward std is roughly 30 % lower than those from vanilla BOIL, showing better stability.",
    "Expected Conclusion": "Penalising late-phase reward variability with one extra term guides BOIL away from hyper-parameters that merely spike in performance, concentrating budget on genuinely robust settings.  The change touches only the curve-compression function, keeps BOIL’s remaining machinery intact, adds one learnable scalar, and empirically yields faster and more reliable hyper-parameter optimisation."
}

# Experimental Design
- Strategy: Overall Objective
Design a single, modular validation protocol that can be reused on every experiment (Cart-Pole → Atari → MuJoCo, small → large HP spaces, low → high reward noise) to prove that Stability-Aware Curve Compression (SACC) brings (1) faster convergence, (2) higher ultimate performance, (3) greater policy reliability, (4) negligible overhead, and (5) good generalization across tasks and search regimes.

I. Core Hypotheses To Validate
1. Sample-efficiency: BOIL+SACC requires fewer BO evaluations to reach a target performance.
2. Performance ceiling: BOIL+SACC attains a higher best-of-run return than baselines.
3. Stability / robustness:
   a. Training stability – learning curves show less oscillation.
   b. Evaluation stability – final policy reward std is lower.
4. Computational cost: wall-clock and GPU hours are not significantly higher than vanilla BOIL.
5. Generalization: the λ learned by GP adapts automatically to a variety of reward scales and noise levels without retuning.

II. Comparison Matrix (applied in every experiment)
A. Baselines
   • Vanilla BOIL (identical surrogate, no penalty)
   • BOIL with human-set λ (constant, no learning) – ablation
   • Alternative curve compressors (e.g., simple last-N averaging, BOIL-MAX) – sanity check
   • External state-of-the-art HPO: ASHA, TPE – competitive bar
B. Ablations / Sensitivity
   1. Vary tail fraction K and observe effect.
   2. Optimizer without λ in GP vector (λ fixed to 0) – isolates impact of learning λ.
C. Stress Settings
   • High-variance environment (stochastic CartPole, randomized seeds)
   • Large search space (add optimizer momentum, epsilon, etc.)

III. Evaluation Angles & Metrics (recorded for every run)
1. Quantitative
   a. Best validation reward vs #evaluations curve (primary) – Area-Under-Curve
   b. Time-to-threshold (first hit of task-specific success)
   c. Final policy test reward mean ± std over 30 episodes
   d. Std of last K% training rewards (same K for fairness)
   e. CPU/GPU time & memory footprint (profiling hooks)
2. Qualitative
   a. Plot learning curves of representative runs (median, 25/75 percentile shading)
   b. Acquisition trajectories – how λ evolves, sample dispersion
3. Statistical Validation
   • 10 independent seeds per setting
   • Report mean, 95% CI; use paired t-tests or Wilcoxon on matched seeds
   • Success criterion: BOIL+SACC beats every baseline on at least 3/4 primary metrics with p<0.05.

IV. Experimental Procedure (identical template)
Step 1: Fix task-specific success threshold & search space.
Step 2: Generate identical initial random design for all methods.
Step 3: Run BO for B iterations (budget fixed across methods) logging full learning curve at each eval.
Step 4: After BO terminates, retrain best hyper-params for T extra episodes, collect evaluation stats.
Step 5: Aggregate across seeds, compute metrics, statistical tests, produce plots & cost table.

V. Resource & Reproducibility Controls
• All runs limited to 1×A100; concurrency chosen so peak VRAM ≤80 GB and RAM ≤2 TB.
• Deterministic CuDNN + fixed numpy/PyTorch seeds stored.
• Codebase uses the same call-paths; SACC flag toggles extra 3-line penalty.
• Auto-logging: JSON + TensorBoard + csv for downstream analysis scripts.

VI. Success Criteria for the Whole Study
The method will be declared effective if, on a diverse benchmark suite (≥3 tasks, ≥2 noise regimes), it consistently:
1. Reduces evaluations-to-threshold by ≥20 % on average.
2. Improves best-of-run reward by ≥5 % on ≥70 % of tasks.
3. Cuts evaluation-phase reward std by ≥25 %.
4. Adds <2 % overhead in wall-clock time.
5. Shows no catastrophic regressions relative to any baseline.

This unified strategy ensures every forthcoming experiment follows a consistent, statistically sound, and multi-angle protocol, providing compelling evidence for SACC’s benefits while remaining practical within the available computational environment.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "\"\"\"src/train.py\nRuns a single experiment variation defined by a YAML config file.  Implements\nBOIL with optional SACC curve–compression, saves metrics and figures.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nimport random\nimport yaml\nimport math\nfrom typing import Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom scipy.stats import norm  # For EI\nfrom scipy.optimize import minimize\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nfrom src.preprocess import get_dataloaders, set_global_seeds\nfrom src.model import (\n    get_model,\n    sigmoid_weighted_average,\n    sacc_compressed_score,\n)\nimport matplotlib\nmatplotlib.use(\"Agg\")  # Headless\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config for this run.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root results directory provided by orchestrator.\")\n    parser.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this variation (matches config entry).\")\n    return parser.parse_args()\n\n\ndef expected_improvement(\n    X_candidates: np.ndarray,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    model: GaussianProcessRegressor,\n    xi: float = 0.01,\n):\n    \"\"\"Computes EI for a set of candidate hyper-parameters.\"\"\"\n    mu, sigma = model.predict(X_candidates, return_std=True)\n    mu = mu.ravel()\n    sigma = sigma.ravel()\n    y_best = y_train.max()\n    with np.errstate(divide=\"warn\"):\n        imp = mu - y_best - xi\n        Z = np.zeros_like(mu)\n        mask = sigma > 0\n        Z[mask] = imp[mask] / sigma[mask]\n        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n        ei[sigma == 0.0] = 0.0\n    return ei\n\n\ndef transform_curve(\n    curve: List[float],\n    use_sacc: bool,\n    midpoint: float,\n    growth: float,\n    lam: float,\n    tail_frac: float,\n):\n    if use_sacc:\n        return sacc_compressed_score(curve, midpoint, growth, lam, tail_frac)\n    else:\n        return sigmoid_weighted_average(curve, midpoint, growth)\n\n\ndef optimise_transform_hyperparams(\n    curves: List[List[float]],\n    X_params: np.ndarray,\n    use_sacc: bool,\n    tail_frac: float,\n    initial: np.ndarray,\n    bounds: List[tuple],\n):\n    \"\"\"Learns midpoint, growth (and λ if SACC) by maximising GP log-marginal likelihood.\"\"\"\n\n    def objective(params):\n        midpoint, growth, lam = params\n        y = np.array([\n            transform_curve(c, use_sacc, midpoint, growth, lam, tail_frac) for c in curves\n        ])\n        # Fit GP (tiny kernel to keep this fast)\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(X_params.shape[1]), length_scale_bounds=(1e-2, 1e3))\n        gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=2)\n        gp.fit(X_params, y)\n        lml = gp.log_marginal_likelihood_value_\n        return -lml  # Minimise negative log-likelihood\n\n    res = minimize(objective, initial, bounds=bounds, method=\"L-BFGS-B\")\n    return res.x  # best parameters\n\n\ndef train_single_model(\n    hparams: Dict[str, Any],\n    data_cfg: Dict[str, Any],\n    model_cfg: Dict[str, Any],\n    training_cfg: Dict[str, Any],\n    device: torch.device,\n):\n    \"\"\"Given a hyper-parameter dict, trains the model and returns validation curve.\"\"\"\n    # Build data\n    train_loader, val_loader = get_dataloaders(data_cfg)\n\n    # Build model\n    model_cfg = model_cfg.copy()\n    model_cfg.update(hparams)  # allow structural HPs (e.g., hidden size) to be tuned\n    model = get_model(model_cfg).to(device)\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"])\n\n    epochs = training_cfg.get(\"epochs\", 3)\n    val_metric_curve = []\n    model.train()\n    for epoch in range(epochs):\n        for (x, y) in train_loader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n        # ---- validation ----\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for (xv, yv) in val_loader:\n                xv, yv = xv.to(device), yv.to(device)\n                pred = model(xv).argmax(dim=1)\n                correct += (pred == yv).sum().item()\n                total += yv.size(0)\n        acc = correct / total if total else 0.0\n        val_metric_curve.append(acc)\n        model.train()\n    return val_metric_curve\n\n\ndef main():\n    args = parse_args()\n    # ------------------------------------------------------------------\n    with open(args.config, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    run_id = args.run_id\n\n    # --------------------- Prepare result directories -----------------\n    run_dir = os.path.join(args.results_dir, run_id)\n    images_dir = os.path.join(run_dir, \"images\")\n    os.makedirs(images_dir, exist_ok=True)\n\n    # ----------------------- Reproducibility --------------------------\n    seed = cfg.get(\"seed\", 42)\n    set_global_seeds(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------- Standard output description -----------------\n    description = cfg.get(\"description\", \"No description provided.\")\n    print(\"===== Experiment Description =====\")\n    print(description)\n    print(\"==================================\")\n    sys.stdout.flush()\n\n    # ------------------ Algorithmic configuration --------------------\n    algo_cfg = cfg[\"algorithm\"]\n    total_evals = int(algo_cfg.get(\"total_evaluations\", 25))\n    random_init = int(algo_cfg.get(\"random_initial_points\", 5))\n    tail_frac = float(algo_cfg.get(\"tail_frac\", 0.1))\n    use_sacc = bool(algo_cfg.get(\"use_sacc\", False))\n    learn_lambda = bool(algo_cfg.get(\"learn_lambda\", False))\n\n    # Search space definition (simple flat numeric ranges)\n    search_space = cfg[\"search_space\"]  # dict name -> {\"min\":float, \"max\":float}\n    param_names = list(search_space.keys())\n    dim = len(param_names)\n\n    def sample_random(n: int = 1):\n        out = []\n        for _ in range(n):\n            cand = [\n                random.uniform(search_space[p][\"min\"], search_space[p][\"max\"]) for p in param_names\n            ]\n            out.append(cand)\n        return np.array(out)\n\n    # Containers\n    X_evaluated: List[List[float]] = []\n    curves: List[List[float]] = []\n    y_scores: List[float] = []\n    all_evals: List[Dict[str, Any]] = []\n\n    # Initial transform hyper-parameters\n    midpoint, growth, lam = 0.0, 1.0, float(algo_cfg.get(\"lambda\", 0.0))\n    # Bounds for optimiser\n    transform_bounds = [(-6, 6), (1e-2, 6), (0.0, 5.0)]  # midpoint, growth, λ\n\n    # Success threshold for time-to-threshold metric\n    success_threshold = algo_cfg.get(\"success_threshold\", None)\n    time_to_threshold = None\n\n    # ----------------------- BO main loop ----------------------------\n    for eval_idx in range(total_evals):\n        start_time = time.time()\n        if eval_idx < random_init or len(y_scores) < 2:\n            x_next = sample_random(1)[0]\n        else:\n            # Fit GP to existing data\n            X_np = np.array(X_evaluated)\n            y_np = np.array(y_scores)\n            kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(dim), length_scale_bounds=(1e-3, 1e3))\n            gp = GaussianProcessRegressor(\n                kernel=kernel,\n                alpha=1e-6,\n                normalize_y=True,\n                n_restarts_optimizer=5,\n            )\n            gp.fit(X_np, y_np)\n\n            # Optimise transform hyper-params if required\n            if learn_lambda and len(curves) >= 2:\n                midpoint, growth, lam = optimise_transform_hyperparams(\n                    curves,\n                    X_np,\n                    use_sacc,\n                    tail_frac,\n                    np.array([midpoint, growth, lam]),\n                    transform_bounds,\n                )\n\n            # Acquisition – Expected Improvement on 1,000 random samples\n            X_cand = sample_random(1000)\n            ei = expected_improvement(X_cand, X_np, y_np, gp)\n            best_idx = int(np.argmax(ei))\n            x_next = X_cand[best_idx]\n\n        # Build hyper-param dict for training call\n        hparams = {param_names[i]: float(x_next[i]) for i in range(dim)}\n        # Mandatory learning rate param for our dummy trainer\n        if \"learning_rate\" not in hparams:\n            hparams[\"learning_rate\"] = 1e-3\n\n        # ---- Run a training instance & get learning curve ----\n        curve = train_single_model(\n            hparams,\n            data_cfg=cfg[\"dataset\"],\n            model_cfg=cfg[\"model\"],\n            training_cfg=cfg.get(\"training\", {}),\n            device=device,\n        )\n\n        score = transform_curve(curve, use_sacc, midpoint, growth, lam, tail_frac)\n\n        # ------------------------ bookkeeping ------------------------\n        X_evaluated.append(list(x_next))\n        curves.append(curve)\n        y_scores.append(score)\n\n        if success_threshold is not None and score >= success_threshold and time_to_threshold is None:\n            time_to_threshold = eval_idx + 1\n\n        all_evals.append(\n            {\n                \"index\": eval_idx,\n                \"hyperparameters\": hparams,\n                \"curve\": curve,\n                \"compressed_score\": score,\n                \"duration_sec\": time.time() - start_time,\n            }\n        )\n\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"eval_index\": eval_idx,\n                    \"score\": score,\n                    \"midpoint\": midpoint,\n                    \"growth\": growth,\n                    \"lambda\": lam,\n                }\n            )\n        )\n        sys.stdout.flush()\n\n    # ---------------------- Final reporting -------------------------\n    best_idx = int(np.argmax(y_scores))\n    best_score = float(y_scores[best_idx])\n    best_hparams = all_evals[best_idx][\"hyperparameters\"]\n\n    # Save results.json\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm_cfg\": algo_cfg,\n        \"search_space\": search_space,\n        \"transform_params\": {\n            \"midpoint\": midpoint,\n            \"growth\": growth,\n            \"lambda\": lam,\n        },\n        \"evaluations\": all_evals,\n        \"best_index\": best_idx,\n        \"best_score\": best_score,\n        \"best_hyperparameters\": best_hparams,\n        \"time_to_threshold\": time_to_threshold,\n    }\n    with open(os.path.join(run_dir, \"results.json\"), \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # ---------------------------- Figures ---------------------------\n    # 1. Score vs evaluation index\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=list(range(len(y_scores))), y=y_scores, marker=\"o\")\n    plt.title(f\"Compressed Score Progression – {run_id}\")\n    plt.xlabel(\"Evaluation #\")\n    plt.ylabel(\"Compressed Score\")\n    # Annotate best\n    plt.annotate(f\"best={best_score:.3f}\",(best_idx, best_score),textcoords=\"data\", xytext=(5,5),\n                 textcoords_offset='offset points', arrowprops=dict(arrowstyle=\"->\"))\n    plt.tight_layout()\n    fig_path = os.path.join(images_dir, f\"score_progression_{run_id}.pdf\")\n    plt.savefig(fig_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # 2. Learning curve of best model\n    best_curve = curves[best_idx]\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=list(range(len(best_curve))), y=best_curve, marker=\"o\")\n    plt.title(f\"Validation Metric per Epoch – best run ({run_id})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(images_dir, f\"learning_curve_{run_id}.pdf\"), bbox_inches=\"tight\")\n    plt.close()\n\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"best_score\": best_score}))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from multiple experiment variations, produces comparative\nfigures and prints summary statistics in structured JSON.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate & compare experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory containing variation sub-directories.\")\n    return parser.parse_args()\n\n\ndef load_results(results_dir: str) -> List[Dict[str, Any]]:\n    runs = []\n    for run_id in sorted(os.listdir(results_dir)):\n        res_file = os.path.join(results_dir, run_id, \"results.json\")\n        if os.path.isfile(res_file):\n            with open(res_file, \"r\") as f:\n                runs.append(json.load(f))\n    return runs\n\n\ndef aggregate_metrics(runs: List[Dict[str, Any]]):\n    summary = {}\n    for run in runs:\n        summary[run[\"run_id\"]] = {\n            \"best_score\": run[\"best_score\"],\n            \"time_to_threshold\": run[\"time_to_threshold\"],\n        }\n    return summary\n\n\ndef plot_best_scores(runs: List[Dict[str, Any]], out_dir: str):\n    os.makedirs(os.path.join(out_dir, \"images\"), exist_ok=True)\n    run_ids = [r[\"run_id\"] for r in runs]\n    best_scores = [r[\"best_score\"] for r in runs]\n    plt.figure(figsize=(6, 4))\n    ax = sns.barplot(x=run_ids, y=best_scores)\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Best Compressed Score\")\n    ax.set_title(\"Best Score Comparison across Variations\")\n    for idx, val in enumerate(best_scores):\n        ax.text(idx, val + 0.01, f\"{val:.2f}\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(out_dir, \"images\", \"best_score_comparison.pdf\"), bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    args = parse_args()\n    runs = load_results(args.results_dir)\n    if not runs:\n        print(\"No result files found – nothing to evaluate.\")\n        return\n\n    summary = aggregate_metrics(runs)\n    # --------- Figures ---------\n    plot_best_scores(runs, args.results_dir)\n\n    print(json.dumps({\"comparison\": summary}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nCommon data loading / preprocessing utilities with dataset placeholders.\n\"\"\"\nfrom __future__ import annotations\nimport random\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom typing import Tuple, Dict, Any\n\n# ================================================================\n# Seed control – deterministic behaviour across experiments\n# ================================================================\n\ndef set_global_seeds(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# ================================================================\n# Dataset loading – with placeholders for future replacements\n# ================================================================\n\ndef _build_dummy_dataset(n_samples: int = 512, input_dim: int = 10, n_classes: int = 2):\n    x = torch.randn(n_samples, input_dim)\n    y = torch.randint(0, n_classes, (n_samples,))\n    return TensorDataset(x, y)\n\n\ndef get_dataloaders(dataset_cfg: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"Returns train & validation dataloaders.\n\n    PLACEHOLDER: Will be replaced with task-specific dataset logic in later steps.\n    \"\"\"\n    name = dataset_cfg.get(\"name\", \"dummy\")\n    batch_size = int(dataset_cfg.get(\"batch_size\", 32))\n\n    if name == \"dummy\":\n        ds = _build_dummy_dataset()\n        train_size = int(0.8 * len(ds))\n        val_size = len(ds) - train_size\n        train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n        return train_loader, val_loader\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(f\"Dataset '{name}' not yet implemented in common foundation.\")\n",
    "model_py": "\"\"\"src/model.py\nModel definitions and curve-compression utilities.\n\"\"\"\nfrom __future__ import annotations\nimport torch\nfrom torch import nn\nfrom typing import Dict, Any, List\nimport numpy as np\n\n# ================================================================\n# Simple classifier used for dummy smoke tests\n# ================================================================\nclass BaseClassifier(nn.Module):\n    def __init__(self, input_dim: int = 10, hidden_dim: int = 64, num_classes: int = 2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ================================================================\n# Model factory with placeholders for future extensions\n# ================================================================\n\ndef get_model(model_cfg: Dict[str, Any]) -> nn.Module:\n    name = model_cfg.get(\"name\", \"dummy_classifier\")\n    if name == \"dummy_classifier\":\n        return BaseClassifier(\n            input_dim=int(model_cfg.get(\"input_dim\", 10)),\n            hidden_dim=int(model_cfg.get(\"hidden_dim\", 64)),\n            num_classes=int(model_cfg.get(\"num_classes\", 2)),\n        )\n    else:\n        # PLACEHOLDER: Will be replaced with specific model construction logic\n        raise NotImplementedError(f\"Model '{name}' not implemented in common foundation.\")\n\n\n# ================================================================\n# Curve compression utilities (BOIL + optional SACC)\n# ================================================================\n\ndef sigmoid_weighted_average(curve: List[float], midpoint: float = 0.0, growth: float = 1.0) -> float:\n    \"\"\"Applies sigmoid weighting across the curve and returns weighted mean.\"\"\"\n    n = len(curve)\n    x_scaled = np.linspace(-6, 6, n)\n    weights = 1.0 / (1.0 + np.exp(-growth * (x_scaled - midpoint)))\n    weights = weights / weights.sum()  # normalise so ∑w =1\n    return float(np.sum(np.array(curve) * weights))\n\n\ndef sacc_compressed_score(\n    curve: List[float],\n    midpoint: float,\n    growth: float,\n    lam: float = 1.0,\n    tail_frac: float = 0.1,\n) -> float:\n    base = sigmoid_weighted_average(curve, midpoint, growth)\n    k = max(1, int(len(curve) * tail_frac))\n    stability_penalty = np.std(curve[-k:])\n    return float(base - lam * stability_penalty)\n",
    "main_py": "\"\"\"src/main.py\nExperiment orchestrator: reads a config-file listing all variations, launches\ntrain.py sequentially, collects logs, and finally invokes evaluate.py.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport yaml\nfrom pathlib import Path\nimport shutil\nimport time\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Experiment orchestrator.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run variations defined in smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run variations defined in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where all outputs will be saved.\")\n    return parser.parse_args()\n\n\ndef tee_subprocess(cmd, stdout_path: Path, stderr_path: Path):\n    \"\"\"Runs *cmd* and simultaneously writes stdout/stderr to file and console.\"\"\"\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n    with open(stdout_path, \"w\") as f_out, open(stderr_path, \"w\") as f_err:\n        # Non-blocking read loop\n        while True:\n            out_line = proc.stdout.readline()\n            err_line = proc.stderr.readline()\n            if out_line:\n                sys.stdout.write(out_line)\n                f_out.write(out_line)\n            if err_line:\n                sys.stderr.write(err_line)\n                f_err.write(err_line)\n            if not out_line and not err_line and proc.poll() is not None:\n                break\n    return proc.returncode\n\n\ndef main():\n    args = parse_args()\n    root = Path(__file__).resolve().parent.parent  # project root\n    config_path = (\n        root / \"config\" / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    )\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    experiments = config[\"experiments\"]\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Copy config for provenance\n    shutil.copy(config_path, results_dir / config_path.name)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_cfg_path = results_dir / f\"{run_id}_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        stdout_path = results_dir / run_id / \"stdout.log\"\n        stderr_path = results_dir / run_id / \"stderr.log\"\n        stdout_path.parent.mkdir(parents=True, exist_ok=True)\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"===== Launching {run_id} =====\")\n        sys.stdout.flush()\n        rc = tee_subprocess(cmd, stdout_path, stderr_path)\n        if rc != 0:\n            print(f\"Experiment {run_id} failed with return-code {rc}\")\n            sys.exit(rc)\n        print(f\"===== Completed {run_id} =====\\n\")\n\n    # -------------------- Post-hoc evaluation -----------------------\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ]\n    subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"boil-sacc-framework\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for BOIL+SACC experiments\"\nrequires-python = \">=3.10\"\nauthors = [\n    { name = \"Research Automation\", email = \"research@example.com\" }\n]\nlicense = { text = \"Apache-2.0\" }\n\n[project.dependencies]\ntorch = \"*\"\nnumpy = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npyyaml = \"*\"\ntqdm = \"*\"\nscikit-learn = \"*\"\nscipy = \"*\"\n# gpytorch & botorch are included for future, heavier-duty experiments\nbotorch = \"*\"\ngpytorch = \"*\"\n",
    "smoke_test_yaml": "experiments:\n  # Lightweight baseline without stability penalty\n  - run_id: baseline_dummy\n    description: |\n      Smoke-test baseline: vanilla BOIL logistic compression (λ=0) on dummy dataset.\n    seed: 123\n    dataset:\n      name: dummy\n      batch_size: 32\n    model:\n      name: dummy_classifier\n      input_dim: 10\n      num_classes: 2\n    algorithm:\n      type: boil\n      use_sacc: false\n      lambda: 0.0\n      random_initial_points: 1\n      total_evaluations: 3\n      tail_frac: 0.1\n      success_threshold: 0.8\n    search_space:\n      learning_rate: {min: 1e-4, max: 5e-3}\n\n  # Smoke-test proposed SACC variant (λ learned)\n  - run_id: sacc_dummy\n    description: |\n      Smoke-test SACC: BOIL + Stability-Aware Curve Compression on dummy dataset.\n    seed: 456\n    dataset:\n      name: dummy\n      batch_size: 32\n    model:\n      name: dummy_classifier\n      input_dim: 10\n      num_classes: 2\n    algorithm:\n      type: boil\n      use_sacc: true\n      lambda: 1.0  # initial value\n      learn_lambda: true\n      random_initial_points: 1\n      total_evaluations: 3\n      tail_frac: 0.1\n      success_threshold: 0.8\n    search_space:\n      learning_rate: {min: 1e-4, max: 5e-3}\n",
    "full_experiment_yaml": "experiments:\n  # PLACEHOLDER: Baseline experiment configuration – replace DATASET_PLACEHOLDER / MODEL_PLACEHOLDER\n  - run_id: BASELINE_PLACEHOLDER\n    description: |\n      PLACEHOLDER description for baseline experiment (vanilla BOIL) – will be\n      replaced in derived-specific configuration.\n    dataset: DATASET_PLACEHOLDER  # PLACEHOLDER: replace with concrete dataset config\n    model: MODEL_PLACEHOLDER      # PLACEHOLDER: replace with concrete model config\n    algorithm:\n      type: boil\n      use_sacc: false\n      lambda: 0.0\n      random_initial_points: 5\n      total_evaluations: 25\n      tail_frac: 0.1\n      success_threshold: SPECIFIC_CONFIG_PLACEHOLDER\n    search_space: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # PLACEHOLDER: Proposed SACC experiment configuration\n  - run_id: PROPOSED_PLACEHOLDER\n    description: |\n      PLACEHOLDER description for proposed BOIL+SACC experiment – to be filled\n      with dataset-specific parameters.\n    dataset: DATASET_PLACEHOLDER\n    model: MODEL_PLACEHOLDER\n    algorithm:\n      type: boil\n      use_sacc: true\n      lambda: 1.0\n      learn_lambda: true\n      random_initial_points: 5\n      total_evaluations: 25\n      tail_frac: 0.1\n      success_threshold: SPECIFIC_CONFIG_PLACEHOLDER\n    search_space: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # Additional ablations / baselines can be appended here following the same schema.\n\n# Note: This is a template – will be fully populated in the derive-specific step.\n# Only the placeholders in UPPER_SNAKE_CASE should be replaced; the rest of the\n# structure must remain unchanged so that the core logic continues to work.\n\n\n\n"
}
