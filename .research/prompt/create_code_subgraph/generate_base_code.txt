
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- To prevent labels, titles, and legends from overlapping, use `plt.tight_layout()` before saving the figure.
- All figures must be saved to `{results_dir}/images/` directory in .pdf format (e.g., using `plt.savefig(os.path.join(results_dir, "images", "filename.pdf"), bbox_inches="tight")`).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback




# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "ρBYOL and most contrastive/self-distillation methods pick two random temporal crops from the same clip and force their representations to match equally, no matter whether the crops are 1 frame or 3 s apart. On rapidly changing rodent behaviour this causes:\n1) Positive pairs that are too far apart in time to depict the same action, blurring fine-grained motion cues.\n2) Weak ordering of the latent space with respect to temporal distance, which hurts downstream behaviour segmentation and rare-action recognition.\nA fix that does not require new architectures is to make the objective aware of the temporal distance between the two views.",
    "Methods": "Time-Weighted BYOL (TW-BYOL)\nMinimal change: multiply BYOL’s mean-squared error loss between the online and target projections by an exponential weight that decays with frame distance Δt between the two augmented views.\nL_TW = w(Δt) * || q( f_θ( v_i ) ) – stop_grad( f_ϕ( v_j ) ) ||² ,  \nwith w(Δt)=exp(-Δt / τ) and a single hyper-parameter τ (e.g. 30 frames).\nTheoretical motivation: For small Δt the two views are likely to be the same action, so the model should align them strongly; for large Δt alignment is weakened, allowing the network to separate different actions and preserve temporal information. The weighting acts as a soft regulariser imposing temporal locality without introducing extra negatives or additional networks.",
    "Experimental Setup": "Dataset: MABe22 mice-triplet videos (video modality only).\nTraining: reproduce original ρBYOL recipe (SlowFast-8×8 backbone, same augmentations) and TW-BYOL (identical but with the weighted loss).\nHyper-parameter sweep: τ ∈ {15, 30, 60} frames.\nEvaluation: linear probe F1 on the 8 hidden downstream mice tasks; report mean and per-task scores.\nBaseline: official ρBYOL numbers from MABe22 leaderboard.\nCompute: 4×A100 GPUs, 50 epochs (same as baseline).",
    "Experimental Code": "# only loss change shown\nimport torch, math\n\ndef time_weighted_byol_loss(p_online, z_target, frame_dist, tau=30.):\n    \"\"\"p_online and z_target: (B,D) tensors; frame_dist: (B,) ints\"\"\"\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)  # (B,)\n    loss = (p_online - z_target.detach()).pow(2).sum(dim=1)  # (B,)\n    weighted_loss = (weight * loss).mean()\n    return weighted_loss\n\n# inside training loop\np = projector_online(backbone_online(view_i))      # (B,D)\nz = projector_target(backbone_target(view_j))      # (B,D)\nframe_dist = torch.abs(frame_idx_i - frame_idx_j)  # (B,)\nloss = time_weighted_byol_loss(p, z, frame_dist, tau=30)",
    "Expected Result": "Across three random seeds the mean F1 on the mice tasks is expected to rise by ~2-3 points (e.g. ρBYOL 68.5 → TW-BYOL 71.0). Improvements should be largest on tasks requiring discrimination of short, bursty actions (e.g. grooming vs sniffing) while remaining neutral on slowly varying experimental condition classification.",
    "Expected Conclusion": "A single, two-line weighting term makes the self-distillation objective respect temporal locality, sharpening representations for fast rodent behaviours. Because no extra negatives, memory, or architectural changes are introduced, the method keeps BYOL’s simplicity and training speed while delivering measurable accuracy gains on fine-grained behaviour annotation tasks."
}

# Experimental Design
- Strategy: Global Goal
Prove that the proposed Time-Weighted BYOL (TW-BYOL) yields temporally better-ordered, more behaviour-discriminative and equally efficient video representations than existing self-supervised alternatives, while remaining robust to hyper-parameter choices and generalising across rodent datasets.

1. Core Hypotheses to Validate
   H1 – Performance: TW-BYOL improves downstream behaviour recognition (overall F1, rare-action recall, few-shot transfer).
   H2 – Temporal Awareness: embeddings respect temporal proximity (distance in latent space grows with frame gap).
   H3 – Efficiency: training speed, GPU memory and wall-clock cost stay within ±5 % of ρBYOL.
   H4 – Robustness: gains hold under different τ values, random seeds, crop strategies and limited labelled data.
   H5 – Generalisation: improvements transfer to unseen rodents/tasks and to a second behaviour dataset.

2. Comparative Framework
   a. Baseline: reproduced ρBYOL recipe.
   b. State-of-the-art self-supervised video baselines: MoCo-v3, SimCLR-v2, TimeContrast.
   c. Supervised upper bound: same backbone trained with full labels (for context only).
   d. Ablations:
      • No weighting (ρBYOL loss) – “Uniform”.
      • Hard cut-off weighting – “Binary”.
      • Alternative decays (linear, inverse square) to test the importance of exponential form.
      • τ sweep (15, 30, 60 frames).

3. Experimental Angles
   3.1 Quantitative Performance
       • Linear-probe F1 per task and averaged (primary metric).
       • k-NN accuracy (label-free evaluation of representation quality).
       • Rare-action recall (top-20 % least frequent labels).
   3.2 Temporal Locality Analysis
       • Spearman correlation ρ between embedding distance and frame gap Δt.
       • Temporal retrieval: mean reciprocal rank when querying a frame for its 5 nearest temporal neighbours.
   3.3 Efficiency Metrics
       • GPU hours per pre-training run.
       • Samples / sec and peak VRAM.
   3.4 Qualitative
       • t-SNE / UMAP plots coloured by action and by timestamp.
       • Video retrieval demos.
   3.5 Robustness & Generalisation
       • Sensitivity curves over τ and crop policies.
       • Subset-of-data training (25 %, 50 % of unlabelled video) to test data efficiency.
       • Cross-dataset transfer: pre-train on MABe22, evaluate on a second rodent-behaviour set (e.g., RatSI).

4. Validation Criteria for Success
   Pass if ALL are met:
   • +2 F1 absolute (≥ p<0.05, paired t-test over 3 seeds) versus ρBYOL on mean of 8 tasks.
   • At least 6/8 tasks individually improve or remain equal.
   • Embedding–time correlation improves by ≥10 % over ρBYOL.
   • GPU hours increase ≤5 %.
   • Variance of F1 across seeds not higher than ρBYOL.
   • Improvements persist (≥75 % retained) when τ∈[15,60] or when only 50 % of unlabelled video is available.

5. Experimental Protocol
   • Hardware: up to 4×A100 80 GB per run; mixed-precision training; identical data-loading pipeline for all methods.
   • Controlled compute: fix batch size, epochs (50), optimiser and augmentation suite; record seeds.
   • Run each configuration 3× for statistics.
   • Hyper-parameter grid executed with identical wall-clock budget; schedule runs via SLURM to exploit 2 TB RAM node.
   • Evaluation code placed in a separate repo; blind-test labels kept hidden until final metrics are logged to ensure fairness.

6. Multi-Perspective Demonstration Strategy
   a. Start with baseline vs TW-BYOL to establish headline gains.
   b. Add ablation study to attribute gains specifically to exponential weighting and to choice of τ.
   c. Compare against external SOTA to position method in field.
   d. Present temporal locality analyses to back mechanistic claim.
   e. Provide efficiency table to show “no free lunch” avoided.
   f. Show robustness curves and cross-dataset transfer to argue for broad applicability.

This unified strategy will be executed for every subsequent experiment, ensuring that each study supplies comparable evidence along performance, temporal fidelity, efficiency, robustness and generalisation axes while sharing compute budgets and evaluation protocols across the research programme.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "\"\"\"\ntrain.py – Train a single self-supervised run variation (BYOL / TW-BYOL, etc.)\nThe script is launched ONLY by src/main.py. It therefore assumes that all CLI\narguments originate from main.py and are validated there.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport time\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom src import preprocess as pp\nfrom src import model as models\n\n################################################################################\n# ------------------------------   helpers   ----------------------------------#\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n################################################################################\n# ------------------------------   training   ---------------------------------#\n################################################################################\n\ndef byol_step(batch: Dict[str, torch.Tensor], learner, optimizer, scaler, config):\n    \"\"\"One optimisation step for BYOL/TW-BYOL.\n\n    Args\n    ----\n    batch : Dict – must have keys 'view1', 'view2', 'frame_dist' (frame_dist optional)\n    learner : models.BYOL – model wrapper that returns p_online & z_target\n    optimizer : torch Optimizer\n    scaler : GradScaler or None\n    config : dict – algorithm section of YAML\n    \"\"\"\n    view1 = batch[\"view1\"].to(get_device(), non_blocking=True)\n    view2 = batch[\"view2\"].to(get_device(), non_blocking=True)\n    frame_dist = batch.get(\"frame_dist\")  # may be None for ordinary BYOL\n    if frame_dist is not None:\n        frame_dist = frame_dist.to(get_device(), non_blocking=True)\n\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.cuda.amp.autocast(enabled=config.get(\"mixed_precision\", True)):\n        p_online, z_target = learner(view1, view2)\n        if config[\"type\"].lower() == \"tw-byol\":\n            tau = config[\"params\"].get(\"tau\", 30.0)\n            loss = models.time_weighted_byol_loss(\n                p_online, z_target, frame_dist=frame_dist, tau=tau\n            )\n        else:  # ordinary BYOL\n            loss = models.byol_loss(p_online, z_target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    learner.update_target_network()\n    return loss.item()\n\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef run_training(cfg: Dict, results_dir: Path):\n    description = cfg.get(\"description\", \"No description provided.\")\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    # ------------------------------------------------------------------ paths\n    run_dir = results_dir / run_id\n    images_dir = run_dir / \"images\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------- device\n    device = get_device()\n\n    # --------------------------------------------------------- dataset / dataloader\n    dataset_cfg = cfg[\"dataset\"]\n    train_ds = pp.get_dataset(dataset_cfg, split=\"train\")\n    val_ds = pp.get_dataset(dataset_cfg, split=\"val\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        shuffle=True,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg[\"training\"].get(\"val_batch_size\", cfg[\"training\"][\"batch_size\"]),\n        shuffle=False,\n        num_workers=dataset_cfg.get(\"num_workers\", 8),\n        pin_memory=True,\n    )\n\n    # ------------------------------------------------------------- model / opt\n    model_cfg = cfg[\"model\"]\n    algorithm_cfg = cfg[\"algorithm\"]\n\n    online_backbone, projector, predictor = models.build_backbone_and_heads(model_cfg)\n    learner = models.BYOL(\n        backbone=online_backbone,\n        projector=projector,\n        predictor=predictor,\n        moving_average_decay=algorithm_cfg.get(\"ema_decay\", 0.996),\n    ).to(device)\n\n    optimizer = optim.Adam(\n        learner.parameters(), lr=cfg[\"training\"][\"learning_rate\"], weight_decay=1e-6\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=algorithm_cfg.get(\"mixed_precision\", True))\n\n    # ------------------------------------------------------------- training loop\n    epochs = cfg[\"training\"][\"epochs\"]\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"time_sec\": [],\n    }\n\n    best_val_loss = float(\"inf\")\n    start_time_total = time.time()\n    for epoch in range(1, epochs + 1):\n        learner.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            loss_val = byol_step(batch, learner, optimizer, scaler, algorithm_cfg)\n            train_losses.append(loss_val)\n            pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n        # ---------------- validation (BYOL self-supervised loss on val set)\n        learner.eval()\n        with torch.no_grad():\n            val_losses = []\n            for batch in val_loader:\n                view1 = batch[\"view1\"].to(device, non_blocking=True)\n                view2 = batch[\"view2\"].to(device, non_blocking=True)\n                frame_dist = batch.get(\"frame_dist\")\n                if frame_dist is not None:\n                    frame_dist = frame_dist.to(device, non_blocking=True)\n\n                with torch.cuda.amp.autocast(enabled=algorithm_cfg.get(\"mixed_precision\", True)):\n                    p_online, z_target = learner(view1, view2)\n                    if algorithm_cfg[\"type\"].lower() == \"tw-byol\":\n                        tau = algorithm_cfg[\"params\"].get(\"tau\", 30.0)\n                        val_loss_val = models.time_weighted_byol_loss(\n                            p_online, z_target, frame_dist=frame_dist, tau=tau\n                        ).item()\n                    else:\n                        val_loss_val = models.byol_loss(p_online, z_target).item()\n                val_losses.append(val_loss_val)\n\n        mean_train_loss = float(np.mean(train_losses))\n        mean_val_loss = float(np.mean(val_losses))\n        epoch_time = time.time() - start_time_total\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(mean_train_loss)\n        history[\"val_loss\"].append(mean_val_loss)\n        history[\"time_sec\"].append(epoch_time)\n\n        # Save best model checkpoint\n        if mean_val_loss < best_val_loss:\n            best_val_loss = mean_val_loss\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save({\"epoch\": epoch, \"state_dict\": learner.state_dict()}, ckpt_path)\n\n        # Epoch-level JSON logging (append-safe)\n        with open(run_dir / \"epoch_metrics.jsonl\", \"a\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps({\n                \"epoch\": epoch,\n                \"train_loss\": mean_train_loss,\n                \"val_loss\": mean_val_loss,\n                \"time_sec\": epoch_time,\n            }) + \"\\n\")\n\n    total_time = time.time() - start_time_total\n\n    # --------------------------------------------------------- save figures\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set(style=\"whitegrid\")\n\n    # Training & validation loss curve\n    plt.figure(figsize=(8, 4))\n    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(f\"{history['val_loss'][-1]:.4f}\",\n                 xy=(history[\"epoch\"][-1], history[\"val_loss\"][-1]),\n                 xytext=(5, -10), textcoords='offset points')\n    plt.legend()\n    plt.tight_layout()\n    fig_name = f\"training_loss_{run_id}.pdf\"\n    plt.savefig(images_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ---------------------------------------------------------- final results\n    results = {\n        \"run_id\": run_id,\n        \"description\": description,\n        \"algorithm\": algorithm_cfg[\"type\"],\n        \"dataset\": dataset_cfg[\"name\"],\n        \"model\": model_cfg[\"type\"],\n        \"epochs\": epochs,\n        \"best_val_loss\": best_val_loss,\n        \"final_val_loss\": history[\"val_loss\"][-1],\n        \"total_time_sec\": total_time,\n        \"figure_files\": [fig_name],\n    }\n\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # ----------------------------------------------------- stdout requirements\n    print(\"\\n===== Experiment Description =====\")\n    print(description)\n    print(\"===== Numerical Results (JSON) =====\")\n    print(json.dumps(results))\n\n\n################################################################################\n# ------------------------------   CLI   --------------------------------------#\n################################################################################\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train one experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True,\n                        help=\"Path to JSON or YAML file with a SINGLE run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where outputs will be written.\")\n    return parser.parse_args()\n\n\ndef load_run_config(path: str) -> Dict:\n    path = Path(path)\n    if path.suffix in {\".yaml\", \".yml\"}:\n        import yaml\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = yaml.safe_load(fp)\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as fp:\n            cfg = json.load(fp)\n    return cfg\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = load_run_config(args.run_config)\n    run_training(cfg, Path(args.results_dir))",
    "evaluate_py": "\"\"\"\nevaluate.py – Aggregate & compare results of the run variations.\nReads all sub-directories in --results-dir that contain results.json, compiles\ncomparison tables & figures and writes them to stdout + images/.\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nFIG_TOPIC_FINAL_LOSS = \"final_loss\"\n\n################################################################################\n# -----------------------------  utilities  -----------------------------------#\n################################################################################\n\ndef collect_results(results_dir: Path) -> List[Dict]:\n    records = []\n    for run_dir in results_dir.iterdir():\n        file = run_dir / \"results.json\"\n        if file.exists():\n            with open(file, \"r\", encoding=\"utf-8\") as fp:\n                records.append(json.load(fp))\n    return records\n\n################################################################################\n# --------------------------  figure helpers  ---------------------------------#\n################################################################################\n\ndef plot_final_loss(df: pd.DataFrame, out_dir: Path):\n    plt.figure(figsize=(8, 4))\n    ax = sns.barplot(data=df, x=\"run_id\", y=\"final_val_loss\", hue=\"algorithm\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Loss\")\n    ax.set_title(\"Final Validation Loss Across Experiments\")\n\n    # annotate each bar\n    for p in ax.patches:\n        height = p.get_height()\n        ax.annotate(f\"{height:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha='center', va='bottom', fontsize=8)\n\n    plt.tight_layout()\n    fname = f\"{FIG_TOPIC_FINAL_LOSS}.pdf\"\n    images_dir = out_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n    plt.savefig(images_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n################################################################################\n# ------------------------------   main   -------------------------------------#\n################################################################################\n\ndef main(results_dir: Path):\n    records = collect_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n\n    df = pd.DataFrame(records)\n    # ---------------------------------------------------------------- figures\n    fig_files = []\n    fig_files.append(plot_final_loss(df, results_dir))\n\n    # --------------------------------------------------------- stdout outputs\n    comparison = {\n        \"num_runs\": len(records),\n        \"best_final_val_loss\": df[\"final_val_loss\"].min(),\n        \"worst_final_val_loss\": df[\"final_val_loss\"].max(),\n        \"figure_files\": fig_files,\n    }\n\n    print(\"\\n===== Cross-Run Comparison Summary =====\")\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(Path(args.results_dir))",
    "preprocess_py": "\"\"\"\npreprocess.py – Common dataset loading & preprocessing utilities.\nAll dataset-specific logic is FORBIDDEN in this foundation layer and therefore\nplaced behind explicit placeholders.\n\"\"\"\nfrom typing import Dict\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\n\n# =============================================================================\n# Placeholders that WILL be replaced in later stages\n# =============================================================================\nclass DatasetPlaceholder(Dataset):\n    \"\"\"PLACEHOLDER: Replace with actual dataset implementation.\n\n    The dataset must return a dict with keys:\n        - 'view1': Tensor\n        - 'view2': Tensor\n        - 'frame_dist': Tensor or int (optional, required for TW-BYOL)\n    \"\"\"\n\n    def __init__(self, root: Path, split: str, transform=None, **kwargs):\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.transform = transform\n        self.data = []  # PLACEHOLDER: populate with actual data indices\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # PLACEHOLDER: implement real loading & augmentation\n        dummy = torch.randn(3, 224, 224)\n        if self.transform:\n            dummy = self.transform(dummy)\n        sample = {\n            \"view1\": dummy,\n            \"view2\": dummy.clone(),\n            \"frame_dist\": torch.tensor(0),\n        }\n        return sample\n\n\n# =============================================================================\n# Public API\n# =============================================================================\n\ndef get_transforms(train: bool = True, cfg: Dict = None):\n    cfg = cfg or {}\n    if train:\n        # basic augmentation pipeline (can be overridden)\n        return T.Compose([\n            T.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n        ])\n    else:\n        return T.Compose([\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n        ])\n\n\ndef get_dataset(cfg: Dict, split: str):\n    \"\"\"Factory that returns correct dataset instance.\n\n    cfg: The 'dataset' section of the run configuration.\n    split: 'train', 'val', or 'test'\n    \"\"\"\n    name = cfg[\"name\"].lower()\n    root = Path(cfg.get(\"root\", \"DATASET_ROOT_PLACEHOLDER\"))  # PLACEHOLDER path\n    params = cfg.get(\"params\", {})\n    transform = get_transforms(train=(split == \"train\"), cfg=params.get(\"transforms\"))\n\n    if name == \"dataset_placeholder\":\n        return DatasetPlaceholder(root, split, transform=transform, **params)\n    else:\n        raise ValueError(\n            f\"Dataset '{name}' not recognised. \"\n            \"# PLACEHOLDER: register dataset in preprocess.get_dataset().\"\n        )",
    "model_py": "\"\"\"\nmodel.py – Model architectures & BYOL utilities common to all experiments.\nThis file contains the COMPLETE implementation of BYOL & TW-BYOL algorithms\nexcept for dataset-specific modules.\n\"\"\"\nfrom typing import Tuple\nimport copy\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models as tv_models\n\n################################################################################\n# --------------------------   projection heads   -----------------------------#\n################################################################################\n\nclass MLPHead(nn.Module):\n    \"\"\"2-layer MLP projection/prediction head for BYOL.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 4096, out_dim: int = 256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n################################################################################\n# ------------------------------  backbones  ----------------------------------#\n################################################################################\n\n\ndef build_backbone_and_heads(model_cfg: dict):\n    \"\"\"Returns (backbone, projector, predictor) all as nn.Module.\n\n    backbone is WITHOUT final classification layer.\n    \"\"\"\n    model_type = model_cfg[\"type\"].lower()\n    if model_type == \"resnet18\":\n        backbone = tv_models.resnet18(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"resnet50\":\n        backbone = tv_models.resnet50(weights=None)\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n    elif model_type == \"model_placeholder\":  # PLACEHOLDER for specialised video backbone\n        # PLACEHOLDER: insert actual backbone creation logic\n        raise NotImplementedError(\"MODEL_PLACEHOLDER needs to be replaced with real model.\")\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n    # projector & predictor\n    proj_hidden = model_cfg.get(\"proj_hidden_dim\", 4096)\n    proj_out = model_cfg.get(\"proj_output_dim\", 256)\n    predictor_hidden = model_cfg.get(\"predictor_hidden_dim\", 4096)\n\n    projector = MLPHead(feat_dim, proj_hidden, proj_out)\n    predictor = MLPHead(proj_out, predictor_hidden, proj_out)\n    return backbone, projector, predictor\n\n\n################################################################################\n# -------------------------------  BYOL  --------------------------------------#\n################################################################################\n\nclass BYOL(nn.Module):\n    \"\"\"Minimal BYOL implementation supporting TW-BYOL loss computation.\"\"\"\n\n    def __init__(self, backbone: nn.Module, projector: nn.Module, predictor: nn.Module,\n                 moving_average_decay: float = 0.996):\n        super().__init__()\n        self.online_backbone = backbone\n        self.online_projector = projector\n        self.predictor = predictor\n\n        # create target encoder as EMA copy\n        self.target_backbone = copy.deepcopy(backbone)\n        self.target_projector = copy.deepcopy(projector)\n        for p in self.target_backbone.parameters():\n            p.requires_grad = False\n        for p in self.target_projector.parameters():\n            p.requires_grad = False\n\n        self.moving_average_decay = moving_average_decay\n\n    @torch.no_grad()\n    def update_target_network(self):\n        \"\"\"EMA update of target network.\"\"\"\n        self._update_moving_average(self.online_backbone, self.target_backbone)\n        self._update_moving_average(self.online_projector, self.target_projector)\n\n    @torch.no_grad()\n    def _update_moving_average(self, online: nn.Module, target: nn.Module):\n        for p_o, p_t in zip(online.parameters(), target.parameters()):\n            p_t.data = p_t.data * self.moving_average_decay + p_o.data * (1.0 - self.moving_average_decay)\n\n    def forward(self, view1, view2):\n        # Online network on view1\n        o1 = self.online_backbone(view1)\n        p1 = self.online_projector(o1)\n        p_online = self.predictor(p1)\n\n        # Target network (no grad) on view2\n        with torch.no_grad():\n            t2 = self.target_backbone(view2)\n            z_target = self.target_projector(t2).detach()\n        return p_online, z_target\n\n\n################################################################################\n# ----------------------------  loss functions  -------------------------------#\n################################################################################\n\ndef byol_loss(p_online: torch.Tensor, z_target: torch.Tensor) -> torch.Tensor:\n    return F.mse_loss(p_online, z_target)\n\n\ndef time_weighted_byol_loss(p_online: torch.Tensor, z_target: torch.Tensor,\n                            frame_dist: torch.Tensor, tau: float = 30.0) -> torch.Tensor:\n    \"\"\"Time-weighted BYOL loss as described in the methodology.\"\"\"\n    if frame_dist is None:\n        raise RuntimeError(\"frame_dist tensor is required for TW-BYOL.\")\n    weight = torch.exp(-frame_dist.float() / tau).to(p_online.device)\n    per_sample_loss = (p_online - z_target).pow(2).sum(dim=1)\n    return (weight * per_sample_loss).mean()\n",
    "main_py": "\"\"\"\nmain.py – Experiment orchestrator.\nReads smoke_test.yaml or full_experiment.yaml, spawns src/train.py sequentially\nfor each run variation, captures logs, and finally launches src/evaluate.py.\n\"\"\"\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n################################################################################\n# -----------------------------  log helpers  ---------------------------------#\n################################################################################\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run *cmd* while tee-ing stdout / stderr to the given files + parent console.\"\"\"\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as out_fp, open(stderr_path, \"w\", encoding=\"utf-8\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                sys.stdout.write(stdout_line)\n                out_fp.write(stdout_line)\n            if stderr_line:\n                sys.stderr.write(stderr_line)\n                err_fp.write(stderr_line)\n            if stdout_line == \"\" and stderr_line == \"\" and process.poll() is not None:\n                break\n        return_code = process.wait()\n        if return_code != 0:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n################################################################################\n# -----------------------------  orchestrator  --------------------------------#\n################################################################################\n\ndef load_yaml(path: Path) -> Dict:\n    with open(path, \"r\", encoding=\"utf-8\") as fp:\n        return yaml.safe_load(fp)\n\n\ndef create_temp_run_config(run_cfg: Dict) -> Path:\n    \"\"\"Write *run_cfg* to a NamedTemporaryFile and return its path.\"\"\"\n    tmp = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\")\n    json.dump(run_cfg, tmp)\n    tmp.flush()\n    return Path(tmp.name)\n\n\ndef main(args):\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    exp_cfg = load_yaml(cfg_path)\n\n    runs = exp_cfg.get(\"experiments\", [])\n    if not runs:\n        print(f\"No experiments defined in {cfg_path}\")\n        sys.exit(1)\n\n    for run in runs:\n        run_id = run[\"run_id\"]\n        print(f\"\\n=== Launching run: {run_id} ===\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        temp_cfg_path = create_temp_run_config(run)\n        cmd = [\n            sys.executable, \"-m\", \"src.train\",\n            \"--run-config\", str(temp_cfg_path),\n            \"--results-dir\", str(results_dir),\n        ]\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # ------------------------------------------------- post-hoc evaluation\n    print(\"\\n===== All runs finished, starting evaluation =====\")\n    eval_cmd = [\n        sys.executable, \"-m\", \"src.evaluate\",\n        \"--results-dir\", str(results_dir),\n    ]\n    tee_subprocess(\n        eval_cmd,\n        results_dir / \"evaluation_stdout.log\",\n        results_dir / \"evaluation_stderr.log\",\n    )\n\n\n################################################################################\n# --------------------------------  CLI  --------------------------------------#\n################################################################################\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run full experimental pipeline.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory where results are stored.\")\n\n    args_parsed = parser.parse_args()\n    main(args_parsed)",
    "pyproject_toml": "[project]\nname = \"tw_byol_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common foundation for BYOL / TW-BYOL experimental variations\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \">=0.15.0\"\npyyaml = \"^6.0\"\ntqdm = \"^4.66.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\nscikit-learn = \"^1.3.0\"\npandas = \"^2.1.0\"\n",
    "smoke_test_yaml": "# Smoke test configuration – intentionally small for CI / GitHub Actions\nexperiments:\n  - run_id: smoke_baseline\n    description: |\n      Smoke test baseline BYOL using placeholder dataset & ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: replace with actual dataset name\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n\n  - run_id: smoke_tw_byol\n    description: |\n      Smoke test TW-BYOL (tau=30) using placeholder dataset & ResNet18.\n    seed: 123\n    dataset:\n      name: DATASET_PLACEHOLDER\n      root: data/placeholder\n      params: { split: smoke }\n    model:\n      type: resnet18\n      proj_hidden_dim: 1024\n      proj_output_dim: 128\n    algorithm:\n      type: TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params:\n        tau: 30\n    training:\n      epochs: 2\n      batch_size: 4\n      learning_rate: 1e-3\n",
    "full_experiment_yaml": "# Full experiment configuration – place only PLACEHOLDERs here.\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: |\n      # PLACEHOLDER: Replace with actual experiment description\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      root: DATASET_PATH_PLACEHOLDER\n      params: {}\n    model:\n      type: MODEL_PLACEHOLDER\n      proj_hidden_dim: 4096\n      proj_output_dim: 256\n    algorithm:\n      type: ALGORITHM_PLACEHOLDER  # e.g., BYOL, TW-BYOL\n      ema_decay: 0.996\n      mixed_precision: true\n      params: {}\n    training:\n      epochs: 50  # PLACEHOLDER: override if needed\n      batch_size: 64\n      learning_rate: 1e-3\n  # Add additional experiment blocks as needed\n"
}
