
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- To prevent labels, titles, and legends from overlapping, use `plt.tight_layout()` before saving the figure.
- All figures must be saved to `{results_dir}/images/` directory in .pdf format (e.g., using `plt.savefig(os.path.join(results_dir, "images", "filename.pdf"), bbox_inches="tight")`).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback




# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Adaptive Score Estimation (ASE) accelerates diffusion sampling by skipping UNet/Transformer blocks according to a hand-crafted, time-dependent dropping schedule.  The manual schedule is (1) sub-optimal, (2) model-specific, and (3) labor-intensive to tune.  How can we let the schedule be found automatically while keeping the ASE framework intact and requiring only a few extra lines of code?",
    "Methods": "Auto-ASE:  Replace the fixed schedule with a tiny, learnable gating function that decides—per time-step t and per network block k—whether the block is executed.  \n1. For every block k add a scalar logit wk (learnable) and define a continuous gate gk(t)=sigmoid(wk·h(t)), where h(t)=1−t (so gates tend to stay open near the data region).  \n2. During training multiply the block output by gk(t).  Use the straight-through estimator (STE) to binarise gk(t) (gk∈{0,1}) at inference time.  \n3. Loss=Lnoise+λ⋅Σk g̅k, where Lnoise is the standard noise-prediction loss and g̅k is the average gate activation over the batch; λ is a small positive constant.  The extra L1 term encourages gates to close, yielding speed-ups.  \n4. No architecture, solver or weight initialisation changes are required; only a few parameters (<100) are added.  Training can be done for a handful of epochs on the original data.",
    "Experimental Setup": "Model: the public DDPM CIFAR-10 UNet (32×32).\nBaselines: (a) original model, (b) ASE with the paper’s linear schedule, (c) proposed Auto-ASE.\nTraining: fine-tune for 1 epoch on CIFAR-10 with batch 128, AdamW lr=1e-4; λ=0.05.\nEvaluation: 50 sampling steps, measure FID (10 k images) and wall-clock time per image on a single RTX-3090.  Report mean gate activations to show the learned schedule.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass GatedBlock(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # initial gate ~0.5\n    def forward(self, x, t):\n        gate = torch.sigmoid(self.w * (1. - t).view(-1, 1, 1, 1))\n        if self.training:\n            return x + gate * (self.block(x, t) - x), gate.mean()\n        else:  # STE binarisation\n            hard_gate = (gate > 0.5).float()\n            return x + hard_gate * (self.block(x, t) - x), hard_gate.mean()\n\n# Wrap every UNet residual block with GatedBlock and tally gates in loss\n\ngate_loss_coeff = 0.05\n\nfor x, t, noise in dataloader:\n    pred, gate_stats = model(x, t)   # gate_stats is list of means\n    loss = ((pred - noise)**2).mean() + gate_loss_coeff * torch.stack(gate_stats).sum()\n    loss.backward(); optimizer.step()",
    "Expected Result": "Auto-ASE is expected to learn a non-uniform schedule that skips ~40-50 % of blocks at noisy time-steps while keeping almost all blocks near t≈0.  Compared with the hand-crafted ASE schedule, it should deliver (1) equal or slightly better FID (≈16 vs 16.5) and (2) an extra 5-10 % reduction in sampling time because the learned schedule is more aggressive where possible and more conservative where necessary.",
    "Expected Conclusion": "By turning the dropping schedule into a handful of learnable gates plus a sparsity penalty, we remove manual tuning from ASE and obtain a data-driven schedule that generalises across models and datasets.  The modification touches only the loss function and adds negligible parameters, yet it yields measurable speed gains without sacrificing image quality, demonstrating how tiny objective tweaks can meaningfully advance diffusion-model acceleration."
}

# Experimental Design
- Strategy: Goal: Build a single, rigorous experimental play-book that can be applied to every study of Auto-ASE, proving that the learnable gating schedule (a) preserves or improves sample quality, (b) reduces sampling cost, (c) is robust across models, datasets and solvers, and (d) removes the need for manual tuning.

1. Key hypotheses to validate
   H1  Performance: Auto-ASE delivers the same or better generative quality than the original network and the hand-crafted ASE schedule.
   H2  Efficiency: Auto-ASE cuts wall–clock sampling time, FLOPs and GPU energy by at least 20 % versus the full model and beats the best fixed schedule.
   H3  Robustness & Generalisation: The learned schedule works (without re-tuning) on multiple datasets, resolutions, network backbones and numerical solvers.
   H4  Simplicity: Training overhead and parameter growth are negligible (<0.5 % extra parameters, <5 % extra training time).

2. Universal comparison set
   a. Baselines
      • Full model (no skipping)
      • Original ASE with its published dropping rule
   b. State-of-the-art accelerators that keep the backbone intact (e.g. DDIM, DPM-Solver++, PNDM) to show trade-off curves
   c. Ablations of Auto-ASE
      • No sparsity loss (λ = 0)
      • Shared vs individual gates
      • Different gate shapes h(t)
      • Soft-gating at inference (no STE)

3. Evaluation axes (applied in every experiment)
   Quantitative quality: FID, KID, IS (for images) or task-specific metrics
   Qualitative quality: curated sample grids + human Turing test where feasible
   Efficiency:  (i) average number of executed blocks, (ii) total UNet/Transformer calls, (iii) wall-clock latency, (iv) TFLOPs, (v) peak & average GPU memory, (vi) Joule/1000 images measured by nvidia-smi
   Robustness: metric distributions across 3 random seeds and across 3 λ values
   Schedule insight: plots of g_k(t) heat-maps & histogram of closed gates

4. Experimental matrix (re-used each time)
   Tier-1  In-domain sanity: original public UNet × CIFAR-10 × DDPM solver (50 steps)
   Tier-2  Cross-architecture: replace UNet by DiT-XL/2 or ADM-KD while keeping CIFAR-10
   Tier-3  Cross-data & resolution: ImageNet-64, LSUN-256, Stable-Diffusion 512×512 latent UNet; keep the same Auto-ASE hyper-parameters
   Tier-4  Cross-solver: DDIM 25 steps, DPM-Solver++ 15 steps, PLMS 50 steps
   Tier-5  Stress tests: (i) halve/ double λ, (ii) prune 70 % of blocks, (iii) deploy on corrupted noise schedules

5. Success criteria (must be met on every Tier-1 & Tier-2 run, and on ≥75 % of Tier-3/4/5 runs)
   • ΔFID ≤ +0.5 (or KID ≤ +0.002) relative to full model
   • ≥20 % speed-up vs full model; ≥5 % extra speed-up vs best fixed ASE schedule
   • <0.5 % parameter growth; <5 % extra training time
   • For robustness tiers: variance of ΔFID across seeds ≤ 0.8 and no catastrophic failure (FID < ×1.5 of baseline)

6. Measurement protocol
   • All runs on a single NVIDIA A100-80GB, PyTorch compiled with CUDA 11.8 + cuDNN 8.
   • Disable CPU-GPU overlaps to obtain deterministic timing; average over 1 k samples after 50 warm-ups.
   • Log with Weights & Biases to expose full metrics, curves and gate heat-maps.

7. Reporting template (identical for all papers/sections)
   Table 1  Quality & efficiency metrics for Baselines vs Auto-ASE
   Figure 1 Trade-off curve: FID vs wall-clock time
   Figure 2 Gate activation heat-map g_k(t)
   Table 2 Ablation results
   Appendix: energy profile & hardware counters

By adhering to this uniform strategy, each experiment will systematically attack the four key hypotheses with controlled baselines, rich metrics and clear success thresholds, allowing reviewers to judge Auto-ASE’s effectiveness from multiple, reproducible perspectives.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "# src/train.py\n\n\"\"\"\nRuns a single experiment variation.\nThis file should be executed ONLY by src.main.  It performs the complete\ntraining loop, optional validation, sampling/FID evaluation and finally saves\nall metrics + figures in a structured directory so that src.evaluate can later\naggregate across runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (relative to repo root)\nfrom . import preprocess as preprocess\nfrom . import model as model_lib\n\n# ----------------------------- Utility helpers ----------------------------- #\n\ndef set_seed(seed: int) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef save_json(obj: Dict, path: str | Path) -> None:\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ----------------------------- Main training ------------------------------- #\n\ndef train(config: Dict, results_dir: Path, run_id: str) -> Dict:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------------------------------------------------------- #\n    # 1.  Data                                                               #\n    # --------------------------------------------------------------------- #\n    train_loader, val_loader = preprocess.get_dataloaders(config)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Model + diffusion utilities                                        #\n    # --------------------------------------------------------------------- #\n    model = model_lib.get_model(config)\n    model.to(device)\n\n    # Optimiser & schedulers\n    optim_cfg = config.get(\"optimizer\", {})\n    lr = optim_cfg.get(\"lr\", 1e-4)\n    betas = optim_cfg.get(\"betas\", (0.9, 0.999))\n    weight_decay = optim_cfg.get(\"weight_decay\", 0.0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n\n    epochs = config.get(\"training\", {}).get(\"epochs\", 1)\n    grad_clip = config.get(\"training\", {}).get(\"grad_clip_norm\", 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 3.  Training loop                                                      #\n    # --------------------------------------------------------------------- #\n    history: Dict[str, List] = {\"train_loss\": [], \"val_loss\": []}\n    start_time = time.time()\n    scaler = torch.cuda.amp.GradScaler(enabled=config.get(\"training\", {}).get(\"amp\", True))\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        num_batches = 0\n        pbar = tqdm(train_loader, desc=f\"[Run {run_id}] Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            imgs = batch[0].to(device)  # torchvision FakeData returns tuple(img, target)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=config.get(\"training\", {}).get(\"amp\", True)):\n                loss_dict = model.training_step(imgs)\n                loss = loss_dict[\"loss\"]\n            scaler.scale(loss).backward()\n            # Gradient clipping\n            if grad_clip is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = running_loss / max(1, num_batches)\n        history[\"train_loss\"].append(avg_train_loss)\n\n        # --------------------- optional validation ---------------------- #\n        if val_loader is not None:\n            model.eval()\n            val_running_loss = 0.0\n            val_batches = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs = batch[0].to(device)\n                    loss_dict = model.training_step(imgs)\n                    val_running_loss += loss_dict[\"loss\"].item()\n                    val_batches += 1\n            avg_val_loss = val_running_loss / max(1, val_batches)\n        else:\n            avg_val_loss = None\n        history[\"val_loss\"].append(avg_val_loss)\n\n        # ---------------- progress logging ----------------------------- #\n        print(\n            json.dumps(\n                {\n                    \"run_id\": run_id,\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n        )\n\n    training_time = time.time() - start_time\n\n    # --------------------------------------------------------------------- #\n    # 4.  Evaluation (FID)                                                   #\n    # --------------------------------------------------------------------- #\n    metrics: Dict[str, float] = {}\n    if config.get(\"evaluation\", {}).get(\"compute_fid\", False):\n        try:\n            from torchmetrics.image.fid import FrechetInceptionDistance\n        except ImportError:\n            raise ImportError(\n                \"torchmetrics not installed. Please add 'torchmetrics' to your dependencies.\"\n            )\n\n        fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n        model.eval()\n\n        # Accumulate real images (limited to avoid OOM during smoke tests)\n        max_real_batches = config.get(\"evaluation\", {}).get(\"fid_num_batches\", 1)\n        real_batches = 0\n        for batch in train_loader:\n            imgs_real = batch[0].to(device)\n            fid_metric.update(imgs_real, real=True)\n            real_batches += 1\n            if real_batches >= max_real_batches:\n                break\n\n        # Generate synthetic images (simple ancestral sampling)\n        num_gen = imgs_real.shape[0] * max_real_batches\n        model_samples = model.generate(num_gen, device=device)\n        fid_metric.update(model_samples, real=False)\n        fid_score = fid_metric.compute().item()\n        metrics[\"fid\"] = fid_score\n\n    # --------------------------------------------------------------------- #\n    # 5.  Persist metrics & figures                                          #\n    # --------------------------------------------------------------------- #\n    # Save metrics\n    metrics[\"final_train_loss\"] = history[\"train_loss\"][-1]\n    if avg_val_loss is not None:\n        metrics[\"final_val_loss\"] = avg_val_loss\n    metrics[\"training_time_sec\"] = training_time\n\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"history\": history,\n        \"metrics\": metrics,\n    }\n\n    save_json(results, results_dir / run_id / \"results.json\")\n\n    # Figures directory\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Training loss curve\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    xs = list(range(1, epochs + 1))\n    plt.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n    if any(v is not None for v in history[\"val_loss\"]):\n        plt.plot(xs, history[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Training Loss – {run_id}\")\n    # Annotate final value\n    plt.annotate(\n        f\"{history['train_loss'][-1]:.4f}\",\n        xy=(xs[-1], history[\"train_loss\"][-1]),\n        xytext=(xs[-1], history[\"train_loss\"][-1] * 1.05),\n    )\n    plt.legend()\n    plt.tight_layout()\n    out_path = img_dir / f\"training_loss_{run_id}.pdf\"\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------ #\n    # 6.  Print final JSON to STDOUT (required by structured logging)    #\n    # ------------------------------------------------------------------ #\n    print(json.dumps({\"run_id\": run_id, \"status\": \"completed\", \"metrics\": metrics}))\n\n    return results\n\n\n# ----------------------------- CLI wrapper -------------------------------- #\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    p.add_argument(\"--config\", type=str, required=True, help=\"Path to config JSON file specific to this run.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory where outputs will be stored.\")\n    p.add_argument(\"--run-id\", type=str, required=True, help=\"Unique identifier for this run variation.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # Load config (written by main orchestrator)\n    with open(args.config, \"r\") as f:\n        config = json.load(f)\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    set_seed(config.get(\"seed\", 42))\n\n    train(config, results_dir, args.run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "# src/evaluate.py\n\"\"\"Evaluates and compares results from all experiment variations.\nReads *results.json files and produces comparison figures + a JSON report.\nThis script is triggered by src.main once all training runs are complete.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n# ------------------------------------------------------------------------- #\n# Utility\n# ------------------------------------------------------------------------- #\n\ndef load_results(results_dir: Path) -> List[Dict]:\n    results = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file) as f:\n                results.append(json.load(f))\n    return results\n\n\ndef aggregate_metrics(all_results: List[Dict]) -> pd.DataFrame:\n    rows = []\n    for res in all_results:\n        row = {\"run_id\": res[\"run_id\"]}\n        metrics = res.get(\"metrics\", {})\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\n# ------------------------------------------------------------------------- #\n# Figure generation helpers\n# ------------------------------------------------------------------------- #\n\ndef barplot_metric(df: pd.DataFrame, metric: str, out_dir: Path):\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.ylabel(metric)\n    plt.xticks(rotation=45, ha=\"right\")\n    # Annotate each bar with value\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"{metric} comparison\")\n    plt.tight_layout()\n    fname = f\"{metric}.pdf\".replace(\" \", \"_\")\n    plt.savefig(out_dir / fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# ------------------------------------------------------------------------- #\n# Main evaluation pipeline\n# ------------------------------------------------------------------------- #\n\ndef evaluate(results_dir: Path):\n    results_dir = Path(results_dir)\n    out_img_dir = results_dir / \"images\"\n    out_img_dir.mkdir(exist_ok=True, parents=True)\n\n    all_results = load_results(results_dir)\n    if len(all_results) == 0:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df = aggregate_metrics(all_results)\n\n    # Identify numeric metrics (excluding run_id)\n    metric_columns = [c for c in df.columns if c != \"run_id\"]\n    generated_figures = []\n    for metric in metric_columns:\n        fname = barplot_metric(df, metric, out_img_dir)\n        generated_figures.append(fname)\n\n    # ------------------------------------------------------------------ #\n    #  JSON summary printed to STDOUT                                   #\n    # ------------------------------------------------------------------ #\n    summary = {\"best_by_metric\": {}, \"figures\": generated_figures}\n    for metric in metric_columns:\n        if metric.startswith(\"loss\"):\n            best_run = df.loc[df[metric].idxmin(), \"run_id\"]\n        else:\n            best_run = df.loc[df[metric].idxmax(), \"run_id\"]\n        summary[\"best_by_metric\"][metric] = best_run\n\n    print(json.dumps(summary, indent=2))\n\n\n# ------------------------------------------------------------------------- #\n# CLI\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate & compare experiment results\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory holding experiment outputs\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    evaluate(Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "# src/preprocess.py\n\"\"\"Common preprocessing pipeline with dataset placeholders.\nThe logic here is COMPLETE for the built-in \"dummy\" dataset used during smoke\ntests.  For real experiments, simply extend the `get_dataset` function with\nactual dataset-specific loading code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# ------------------------------------------------------------------------- #\n# Config-driven helpers\n# ------------------------------------------------------------------------- #\n\ndef get_transforms(config: dict):\n    \"\"\"Creates torchvision transforms based on config.\n\n    For image datasets we support optional resizing and normalisation.\n    \"\"\"\n    tfms: List = []\n    resize = config.get(\"data\", {}).get(\"resize\")\n    if resize is not None:\n        tfms.append(transforms.Resize(resize))\n    tfms.append(transforms.ToTensor())\n\n    # Normalisation (ImageNet stats by default)\n    if config.get(\"data\", {}).get(\"normalize\", True):\n        mean = config.get(\"data\", {}).get(\"mean\", [0.485, 0.456, 0.406])\n        std = config.get(\"data\", {}).get(\"std\", [0.229, 0.224, 0.225])\n        tfms.append(transforms.Normalize(mean, std))\n\n    return transforms.Compose(tfms)\n\n\n# ------------------------------------------------------------------------- #\n# Dataset factory (with placeholders for extension)                          #\n# ------------------------------------------------------------------------- #\n\ndef get_dataset(name: str, train: bool, config: dict):\n    \"\"\"Returns a torch.utils.data.Dataset instance.\n\n    Built-in:\n        • \"dummy\"  – torchvision.datasets.FakeData with tiny size (used for CI / smoke tests)\n\n    PLACEHOLDER: Extend this function with actual dataset logic, e.g. CIFAR-10,\n    ImageNet-64, LSUN, etc.  Keep the interface unchanged so the rest of the\n    pipeline remains intact.\n    \"\"\"\n\n    if name == \"dummy\":\n        # A tiny fake dataset with 1-channel or 3-channel images depending on config.\n        image_size = config.get(\"data\", {}).get(\"image_size\", (3, 32, 32))\n        dataset = datasets.FakeData(\n            size=config.get(\"data\", {}).get(\"num_samples\", 256),\n            image_size=image_size,\n            num_classes=10,\n            transform=get_transforms(config),\n        )\n        return dataset\n\n    # ------------------------- PLACEHOLDER ------------------------------ #\n    # Insert real dataset paths / download logic here. For example:\n    # if name == \"cifar10\":\n    #     root = Path(config[\"data\"][\"root\"])\n    #     return datasets.CIFAR10(root=root, train=train, transform=get_transforms(config), download=True)\n\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented yet.\")\n\n\n# ------------------------------------------------------------------------- #\n# Dataloader helper                                                         #\n# ------------------------------------------------------------------------- #\n\ndef get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader | None]:\n    batch_size = config.get(\"training\", {}).get(\"batch_size\", 16)\n    num_workers = config.get(\"data\", {}).get(\"num_workers\", os.cpu_count() // 2)\n\n    dataset_name = config.get(\"dataset\")\n    train_dataset = get_dataset(dataset_name, train=True, config=config)\n\n    val_loader = None\n    if config.get(\"training\", {}).get(\"validation_split\", 0.0) > 0.0:\n        val_split = config[\"training\"][\"validation_split\"]\n        val_size = int(len(train_dataset) * val_split)\n        train_size = len(train_dataset) - val_size\n        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    return train_loader, val_loader\n",
    "model_py": "# src/model.py\n\"\"\"Model architecture implementations.\nIncludes baseline UNet-style model plus Auto-ASE variant with learnable gates.\nThe gating logic is FULLY implemented here; swapping datasets or changing the\nunderlying block structure can be done without touching the base logic.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ------------------------------------------------------------------------- #\n# Time embedding helpers (positional)\n# ------------------------------------------------------------------------- #\n\ndef timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n    \"\"\"From OpenAI's ADM code: create sinusoidal embeddings.\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:  # zero pad\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\n# ------------------------------------------------------------------------- #\n# Gating mechanism (Auto-ASE core)\n# ------------------------------------------------------------------------- #\n\nclass GatedBlock(nn.Module):\n    \"\"\"Wraps an arbitrary nn.Module block with a learnable gate following Auto-ASE.\n\n    During training the gate is continuous (sigmoid).  During inference the gate\n    is binarised via the straight-through estimator (STE).\n    \"\"\"\n\n    def __init__(self, block: nn.Module, t_dim: int, h_function: str = \"linear\"):\n        super().__init__()\n        self.block = block\n        self.w = nn.Parameter(torch.zeros(1))  # gate logit parameter\n        self.t_proj = nn.Linear(t_dim, 1)\n        self.h_function = h_function\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor, train: bool = True):\n        # Compute gate scalar g_k(t) per sample in batch\n        h_t = 1.0 - torch.sigmoid(self.t_proj(temb))  # shape (B,1)\n        gate_cont = torch.sigmoid(self.w * h_t)  # (B,1)\n        gate = gate_cont if train else (gate_cont > 0.5).float()  # STE at inference\n\n        # Reshape for broadcasting over feature maps\n        while gate.dim() < x.dim():\n            gate = gate.unsqueeze(-1)\n        y = x + gate * (self.block(x, temb) - x)\n        return y, gate_cont.mean()  # use continuous gate stat for loss\n\n\n# ------------------------------------------------------------------------- #\n# Simple UNet-like backbone (CIFAR-10 compatible, kept intentionally small)\n# ------------------------------------------------------------------------- #\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, t_dim: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.emb_proj = nn.Linear(t_dim, out_ch)\n        self.activation = nn.SiLU()\n        self.skip = in_ch == out_ch\n\n    def forward(self, x: torch.Tensor, temb: torch.Tensor):\n        h = self.activation(self.conv1(x))\n        # Add time embedding\n        temb_broadcast = self.emb_proj(temb)[:, :, None, None]\n        h = h + temb_broadcast\n        h = self.activation(self.conv2(h))\n        if self.skip:\n            h = h + x\n        return h\n\n\nclass SimpleUNet(nn.Module):\n    \"\"\"UNet with optional gating wrappers based on Auto-ASE.\"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        base_channels: int = 64,\n        time_dim: int = 128,\n        gated: bool = False,\n        lambda_gate: float = 0.05,\n    ):\n        super().__init__()\n        self.time_dim = time_dim\n        self.lambda_gate = lambda_gate\n        self.gated = gated\n\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim),\n        )\n\n        # Encoder\n        self.down1 = self._make_block(img_channels, base_channels, gated)\n        self.down2 = self._make_block(base_channels, base_channels * 2, gated)\n        self.pool = nn.AvgPool2d(2)\n        # Bottleneck\n        self.bottleneck = self._make_block(base_channels * 2, base_channels * 2, gated)\n        # Decoder\n        self.up1 = self._make_block(base_channels * 4, base_channels, gated)\n        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n\n        # Output layer\n        self.out_conv = nn.Conv2d(base_channels, img_channels, 1)\n\n    def _make_block(self, in_ch: int, out_ch: int, gated: bool):\n        block = ConvBlock(in_ch, out_ch, self.time_dim)\n        if gated:\n            return GatedBlock(block, self.time_dim)\n        return block\n\n    # ------------------------------------------------------------------ #\n    # Diffusion-specific helpers                                         #\n    # ------------------------------------------------------------------ #\n    def forward(self, x: torch.Tensor, t: torch.Tensor, train: bool = True):\n        temb = timestep_embedding(t, self.time_dim)\n        temb = self.time_mlp(temb)\n\n        gated_stats: List[torch.Tensor] = []\n\n        def apply(block, *args):\n            if isinstance(block, GatedBlock):\n                y, g_stat = block(*args, train=train)\n                gated_stats.append(g_stat)\n                return y\n            else:\n                return block(*args)\n\n        # Encoder\n        d1 = apply(self.down1, x, temb)\n        p1 = self.pool(d1)\n        d2 = apply(self.down2, p1, temb)\n        p2 = self.pool(d2)\n\n        # Bottleneck\n        bn = apply(self.bottleneck, p2, temb)\n\n        # Decoder\n        up = F.interpolate(bn, scale_factor=2, mode=\"nearest\")\n        up = torch.cat([up, d2], dim=1)\n        up = apply(self.up1, up, temb)\n        up = torch.cat([up, d1], dim=1)\n        out = self.out_conv(up)\n        return out, gated_stats\n\n    # ------------------------ Training interface ---------------------- #\n    def training_step(self, x0: torch.Tensor) -> dict:\n        \"\"\"Implements standard DDPM noise-prediction loss + gate sparsity.\"\"\"\n        device = x0.device\n        batch_size = x0.size(0)\n        config_T = 1000\n        t = torch.randint(0, config_T, (batch_size,), device=device)\n        betas = torch.linspace(1e-4, 0.02, config_T, device=device)\n        alphas = 1.0 - betas\n        alpha_bars = torch.cumprod(alphas, dim=0)\n\n        noise = torch.randn_like(x0)\n        sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n        x_noisy = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n\n        pred_noise, gate_stats = self.forward(x_noisy, t)\n        noise_loss = F.mse_loss(pred_noise, noise)\n        gate_reg = (\n            torch.stack(gate_stats).mean() if gate_stats else torch.tensor(0.0, device=device)\n        )\n        total_loss = noise_loss + self.lambda_gate * gate_reg\n        return {\n            \"loss\": total_loss,\n            \"noise_loss\": noise_loss.detach(),\n            \"gate_loss\": gate_reg.detach(),\n        }\n\n    # --------------------- Simple ancestral sampling ------------------- #\n    def generate(self, num_samples: int, device: torch.device) -> torch.Tensor:\n        \"\"\"Very basic DDPM sampling loop (for evaluation) – not optimised.\"\"\"\n        self.eval()\n        with torch.no_grad():\n            img_size = 32  # assume square for simplicity – can be changed later\n            x = torch.randn(num_samples, 3, img_size, img_size, device=device)\n            T = 100\n            betas = torch.linspace(1e-4, 0.02, T, device=device)\n            alphas = 1.0 - betas\n            alpha_bars = torch.cumprod(alphas, dim=0)\n\n            for t_ in reversed(range(T)):\n                t = torch.full((num_samples,), t_, device=device, dtype=torch.long)\n                eps_theta, _ = self.forward(x, t, train=False)\n                alpha_bar = alpha_bars[t][:, None, None, None]\n                beta = betas[t][:, None, None, None]\n                x0_pred = (x - torch.sqrt(1 - alpha_bar) * eps_theta) / torch.sqrt(alpha_bar)\n                coef1 = 1 / torch.sqrt(alphas[t][:, None, None, None])\n                coef2 = beta / torch.sqrt(1 - alpha_bar)\n                x = coef1 * (x - coef2 * eps_theta)\n                if t_ > 0:\n                    noise = torch.randn_like(x)\n                    x += torch.sqrt(beta) * noise\n            x = torch.clamp(x, -1.0, 1.0)\n            return x.cpu()\n\n\n# ------------------------------------------------------------------------- #\n# Model factory                                                             #\n# ------------------------------------------------------------------------- #\n\ndef get_model(config: dict) -> nn.Module:\n    model_name = config.get(\"model\")\n    lambda_gates = config.get(\"diffusion\", {}).get(\"lambda_gates\", 0.05)\n    if model_name in {\"dummy_baseline\", \"baseline_unet\"}:\n        return SimpleUNet(gated=False, lambda_gate=0.0)\n    elif model_name in {\"dummy_auto_ase\", \"auto_ase\"}:\n        return SimpleUNet(gated=True, lambda_gate=lambda_gates)\n\n    # ------------------------- PLACEHOLDER -------------------------------- #\n    # Insert additional architectures (DiT, ADM-KD, Stable-Diffusion UNet etc.) here\n\n    raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "main_py": "# src/main.py\n\"\"\"Main orchestrator script.\nReads a YAML configuration file (either smoke_test.yaml or full_experiment.yaml)\nand sequentially executes every experiment variation by spawning src.train as a\nsub-process.  After all runs finish it calls src.evaluate to aggregate results.\nStructured logging to stdout/stderr + per-run log files is implemented via a\ntee-like mechanism.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\n# The directory in which this file resides\nROOT = Path(__file__).resolve().parent.parent\nSRC_DIR = ROOT / \"src\"\nCONFIG_DIR = ROOT / \"config\"\n\nTRAIN_MODULE = \"src.train\"\nEVAL_MODULE = \"src.evaluate\"\n\n\n# ------------------------------------------------------------------------- #\n# Process helpers                                                           #\n# ------------------------------------------------------------------------- #\n\ndef tee_stream(stream, *files):\n    \"\"\"Yields lines from stream while simultaneously writing to file handles.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        for f in files:\n            f.write(line.decode())\n        yield line.decode()\n\n\ndef run_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(stdout_path, \"w\") as so, open(stderr_path, \"w\") as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Stream STDOUT\n        for line in tee_stream(proc.stdout, so, sys.stdout):\n            pass\n        # Stream STDERR\n        for line in tee_stream(proc.stderr, se, sys.stderr):\n            pass\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Sub-process {' '.join(cmd)} exited with code {proc.returncode}\")\n\n\n# ------------------------------------------------------------------------- #\n# Orchestrator                                                              #\n# ------------------------------------------------------------------------- #\n\ndef execute_runs(experiments: List[Dict], results_dir: Path):\n    for exp in experiments:\n        run_id = exp.get(\"run_id\")\n        if run_id is None:\n            raise ValueError(\"Every experiment variation must have a 'run_id' field\")\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Persist run-specific config to JSON (so train.py can read it)\n        cfg_path = run_dir / \"config.json\"\n        with open(cfg_path, \"w\") as f:\n            json.dump(exp, f, indent=2)\n\n        # Build command\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config\",\n            str(cfg_path),\n            \"--results-dir\",\n            str(results_dir),\n            \"--run-id\",\n            run_id,\n        ]\n        print(f\"\\n=== Launching run '{run_id}' ===\")\n        run_subprocess(cmd, stdout_path=run_dir / \"stdout.log\", stderr_path=run_dir / \"stderr.log\")\n        print(f\"=== Run '{run_id}' completed ===\\n\")\n\n    # After all runs: evaluate\n    eval_cmd = [sys.executable, \"-m\", EVAL_MODULE, \"--results-dir\", str(results_dir)]\n    run_subprocess(eval_cmd, stdout_path=results_dir / \"evaluate_stdout.log\", stderr_path=results_dir / \"evaluate_stderr.log\")\n\n\n# ------------------------------------------------------------------------- #\n# CLI                                                                       #\n# ------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Auto-ASE experiment orchestrator\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test defined in config/smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment defined in config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory where outputs will be saved\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n\n    with open(cfg_file, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    experiments = cfg.get(\"experiments\")\n    if not experiments:\n        raise ValueError(\"Configuration file must contain 'experiments' list\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    execute_runs(experiments, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"auto_ase_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Auto-ASE experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n  { name=\"Research Team\", email=\"research@example.com\" }\n]\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\n# For FID computation\npillow = \"*\"\ntorchmetrics = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# This configuration runs two tiny experiments on a dummy dataset to make sure\n# the entire pipeline executes correctly. It is deliberately lightweight so it\n# can be executed in <30 seconds on CPU-only CI.\n\nexperiments:\n  - run_id: dummy_baseline\n    dataset: dummy\n    model: dummy_baseline\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: False\n\n  - run_id: dummy_auto_ase\n    dataset: dummy\n    model: dummy_auto_ase\n    seed: 123\n    training:\n      epochs: 1\n      batch_size: 16\n      amp: False\n    diffusion:\n      timesteps: 100\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: False\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# PLACEHOLDER: This template will be populated in the next step with actual\n# datasets, models and hyper-parameters. The structure MUST remain identical\n# so that src.main can parse it without changes.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER-baseline\n    dataset: DATASET_PLACEHOLDER\n    model: baseline_unet\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.0\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  - run_id: DATASET_PLACEHOLDER-auto_ase\n    dataset: DATASET_PLACEHOLDER\n    model: auto_ase\n    seed: 42\n    training:\n      epochs: 1\n      batch_size: 128\n      amp: True\n    diffusion:\n      timesteps: 1000\n      beta_schedule: linear\n      lambda_gates: 0.05\n    evaluation:\n      compute_fid: True\n      fid_num_batches: 30\n\n  # Additional ablations / variants can be appended here following the same key names.\n"
}
