
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The study evaluates both state-of-the-art self-supervised video representation learning methods and community-contributed methods from an open challenge. For video data, methods like Masked Autoencoder (MAE) with ViT-B, MaskFeat with MViTv2-S, and ρBYOL with SlowFast (Slow pathway 8x8) were used, employing pretext tasks such as masking spatio-temporal volumes for reconstruction, predicting HOG features, or contrastive learning with positive samples. Community methods incorporated large pre-trained vision models (BEiT, ResNet, MobileNetV3), variations of contrastive learning (SimCLR, MoCo), trajectory data as additional inputs, and hand-crafted features. For trajectory data, specialized methods like Trajectory Variational Autoencoder (TVAE) for reconstruction, T-Perceiver for temporal relationships and feature reconstruction/prediction, T-GPT for next frame prediction using a Transformer, T-PointNet for permutation-invariant features using PointNet, and T-BERT for agent embeddings via masked modeling and contrastive learning were employed.

# Repository Content
File Path: utils/__init__.py
Content:

File Path: utils/augmentation.py
Content:
import numpy as np
import torch
from torchvision.transforms import functional as F
import torchvision.transforms as T


class TransformsSimCLR:
    def __init__(self, size, pretrained=True, n_channel=3, validation=False) -> None:
        self.MAX_SIZE = (512, 512)
        self.train_transforms = T.Compose([
            T.ToTensor(),
            T.Lambda(self.constrained_crop),
            T.RandomHorizontalFlip(),
            T.RandomVerticalFlip(),
            # Taking the means of the normal distributions of the 3 channels
            # since we are moving to grayscale
            T.Normalize(mean=np.mean([0.485, 0.456, 0.406]).repeat(n_channel),
                        std=np.sqrt(
                            (np.array([0.229, 0.224, 0.225])**2).sum()/9).repeat(n_channel)
                        ) if pretrained is True else T.Lambda(lambda x: x)
        ])

        self.validation_transforms = T.Compose([
            T.ToTensor(),
            T.Resize(size=size),
            # Taking the means of the normal distributions of the 3 channels
            # since we are moving to grayscale
            T.Normalize(mean=np.mean([0.485, 0.456, 0.406]).repeat(n_channel),
                        std=np.sqrt(
                            (np.array([0.229, 0.224, 0.225])**2).sum()/9).repeat(n_channel)
                        ) if pretrained is True else T.Lambda(lambda x: x)
        ])

        self.size = size
        self.validation = validation

    def constrained_crop(self, img):
        crop = np.random.randint(
            low=[0, 0, self.bounding_box[3], self.bounding_box[2]], 
            high=[1 + self.bounding_box[1], 1 + self.bounding_box[0], self.MAX_SIZE[1] + 1, self.MAX_SIZE[0] + 1], 
            size=4
        )
        return F.resized_crop(img, crop[0], crop[1], crop[2]-crop[0]+1, crop[3]-crop[1]+1, self.size)

    def __call__(self, x, bb):
        self.bounding_box = bb# * 224/512
        if not self.validation:
            x_i, x_j = self.train_transforms(x), self.train_transforms(x)
            return x_i, x_j
        else:
            return self.validation_transforms(x)
File Path: utils/average_motion.py
Content:
import numpy as np
from numpy.linalg import norm

from utils.keypoint_dataset import DATA_DIR, MouseKeypointSnippetDataset, get_keypoints_map

keypoints_map = get_keypoints_map(DATA_DIR, "submission", filled_holes=True)
keypoint_dataset = MouseKeypointSnippetDataset(keypoints_map, 1800)

v_arr = []
for i, seq in enumerate(keypoint_dataset):
    v = np.gradient(seq[:,:,:10], axis=0)
    v = norm(v, axis=3)
    v = np.sum(v, axis=2)
    v = np.max(v, axis=1)
    v_arr.append(v)
v_arr = np.concatenate(v_arr)

np.save("cache/average_motion.npy", v_arr)
File Path: utils/embed_frames_beit.py
Content:
# # Prepare submission

import os

import numpy as np
import torch
from transformers import BeitFeatureExtractor, BeitModel
from tqdm import tqdm

from utils.video_dataset import VideoDataset
data_dir = '/data/behavior-representation'

frame_number_map = np.load(os.path.join(data_dir, 'frame_number_map.npy'), allow_pickle=True).item()

video_size = 'full_size'
video_set = 'submission'

video_data_dir = os.path.join(data_dir, 'videos', video_size, video_set)

ds = VideoDataset(video_data_dir, frame_number_map, channels_first=True)

batch_size=32
dataloader = torch.utils.data.DataLoader(
    ds,
    batch_size=batch_size,
    shuffle=False,
    drop_last=False,
    pin_memory=False,
    num_workers=8,
)
print("len(dataloader), ", len(dataloader))

feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-large-patch16-512')
model = BeitModel.from_pretrained('microsoft/beit-large-patch16-512')
device = "cuda:0" if torch.cuda.is_available() else "cpu"
model.to(device)

n_samples, embedding_length = len(ds), 1024
embedding_array = np.zeros((n_samples, embedding_length))

cache_path = os.path.join("cache", "beit_embeddings.npz")
os.makedirs("cache", exist_ok=True)

#### This is how to load embeddings ###
# print("Loading embeddings")
# a = np.load(cache_path)
# embedding_array = a["arr_0"]

for i, x in enumerate(tqdm(dataloader)):
    with torch.no_grad():
        inputs = feature_extractor(images=[xx for xx in x], return_tensors="pt").to(device)
        outputs = model(**inputs)
        embeddings = outputs["pooler_output"].cpu().numpy()
        start_idx, end_idx = i * batch_size, (i+1) * batch_size
        embedding_array[start_idx:end_idx] = embeddings
        if i % 1000 == 0:
            print(f"Caching embedding array to {cache_path}")
            np.savez(cache_path, embedding_array)

print(f"Caching embedding array to {cache_path}")
np.savez(cache_path, embedding_array)

File Path: utils/find_cage_features.py
Content:
import numpy as np
from skimage.transform import hough_line, hough_line_peaks
from skimage.filters import gaussian


def find_edge(contrast_image, is_horizontal=True):
    tested_angles = np.linspace(-np.pi / 60, np.pi / 60, 300)
    if is_horizontal:
        tested_angles += np.pi/2

    contrast_image -= contrast_image.min()
    contrast_image /= contrast_image.max()
    contrast_image = contrast_image > 0.8

    h, theta, d = hough_line(contrast_image, theta=tested_angles)
    hpeaks = hough_line_peaks(h, theta, d, threshold=0.9*h.max(), min_distance=100,  min_angle=50)
    # if len(hpeaks[0]) > 1:
    #     print("Warning: Found many lines")
    #     print(hpeaks)
    angle, dist = hpeaks[1][0], hpeaks[2][0]
    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])
    slope=np.tan(angle + np.pi/2)
    return (x0,y0), slope


def find_intersection(edge_0, edge_1):
    (x0,y0), slope0 = edge_0
    (x1,y1), slope1 = edge_1

    a0 = y0 - slope0 * x0
    a1 = y1 - slope1 * x1

    x_intersection = (a1 - a0) / (slope0 - slope1)
    y_intersection = y0 + slope0 * (x_intersection - x0)

    return x_intersection, y_intersection


def find_corners(frame):
    smoothed_frame = gaussian(frame, sigma=10)

    bottom_edge_contrast = smoothed_frame[:-1,:,0] - smoothed_frame[1:,:,0]
    top_edge_contrast = -bottom_edge_contrast
    right_edge_contrast = smoothed_frame[:,:-1,0] - smoothed_frame[:,1:,0]
    left_edge_contrast = - right_edge_contrast
    edges = [find_edge(contrast_image, is_horizontal) for contrast_image, is_horizontal in zip(
        [top_edge_contrast, bottom_edge_contrast, left_edge_contrast, right_edge_contrast],
        [True, True, False, False]
    )]
    [top_edge, bottom_edge, left_edge, right_edge] = edges
    top_left = find_intersection(top_edge, left_edge)
    top_right = find_intersection(top_edge, right_edge)
    bottom_left = find_intersection(bottom_edge, left_edge)
    bottom_right = find_intersection(bottom_edge, right_edge)
    return [top_left, top_right, bottom_left, bottom_right]


def get_feeding_location(top_left, top_right):
    mid_point = (np.array(top_left) + top_right) / 2
    return mid_point[0], mid_point[1] - 20
File Path: utils/handcrafted_geometries.py
Content:
import os
import numpy as np
from numpy.linalg import norm

from tqdm import tqdm
from utils.find_cage_features import find_corners, get_feeding_location

from utils.keypoint_dataset import DATA_DIR, MouseKeypointSnippetDataset, get_keypoints_map

import matplotlib.pyplot as plt

from utils.video_dataset import VideoDataset

NUM_MICE = 3
BODY_KEYPOINTS= (0, 9)
MAX_SIZE = 512
CLIP_LENGTH = 1800

BP = { 
    'nose': 0, 
    'left_ear': 1,  
    'right_ear': 2,  
    'neck': 3, 
    'left_forepaw': 4, 
    'right_forepaw': 5, 
    'center_back': 6, 
    'left_hindpaw': 7, 
    'right_hindpaw': 8, 
    'tail_base': 9, 
    'tail_middle': 10, 
    'tail_tip': 11
}

# ############### Helper functions ###############

def polygon_area(m):
    x, y = m[:, :, :, 0], m[:, :, :, 1]
    correction = x[:, :, -1] * y[:, :, 0] - y[:, :, -1]* x[:, :, 0]
    main_area = np.sum(x[:, :, :-1] * y[:, :, 1:], axis=-1) - np.sum(y[:, :, :-1] * x[:, :, 1:], axis=-1)
    return 0.5*np.abs(main_area + correction)


def encode_angle(angle):
    return np.stack([np.cos(angle), np.sin(angle)], axis=-1)


def normalize_mouse(sequence):
    sample = sequence
    
    m_vector = sample[:, :, 3, :] - sample[:, :, 9, :]
    m_unit_vector = m_vector / norm(m_vector, axis=-1)[..., np.newaxis]

    basis = np.stack([m_unit_vector, np.stack([-m_unit_vector[:, :, 1], m_unit_vector[:, :, 0]], axis=-1)], axis=-1)
    pos = sample[:, :, :, :] - (sample[:, :, 3, :][:, :, np.newaxis, :] + sample[:, :, 9, :][:, :, np.newaxis, :])/2

    out = np.zeros_like(sample)
    for i in range(sample.shape[0]):
        for j in range(3):
            out[i, j] = np.dot(pos[i, j], basis[i, j])
    return out


def replace_nans_with_feature_mean(array):
    col_mean = np.nanmean(array, axis=0)
    print("Mean of each feature:")
    print(col_mean)

    inds = np.where(np.isnan(array))

    array[inds] = np.take(col_mean, inds[1])
    return array


# ############### Individual mouse features ###############

class DistanceFeature:
    def __init__(self, keypoints, bp1, bp2):
        self.bp1, self.bp2 = bp1, bp2
        dist = norm(keypoints[:, :, self.bp1, :] - keypoints[:, :, self.bp2, :], axis=-1)
        self.mean, self.std = dist.mean(), dist.std()

    def plot_distribution(self, keypoints):
        dist = norm(keypoints[:, :, self.bp1, :] - keypoints[:, :, self.bp2, :], axis=-1)
        plt.hist(dist.flatten(), range=[0, 100], bins=50)
        plt.show()

    def __call__(self, sequence):
        dist = norm(sequence[:, :, self.bp1, :] - sequence[:, :, self.bp2, :], axis=-1)
        return (dist - self.mean) / self.std


class BodyLengthFeature(DistanceFeature):
    def __init__(self, keypoints):
        super().__init__(keypoints, BP['nose'], BP['tail_base'])


class PawHeadDistanceFeature:
    def __init__(self, keypoints):
        dist = self._calc_distance(keypoints)
        self.mean, self.std = dist.mean(), dist.std()
    
    def _calc_distance(self, kps):
        head_center = np.array([kps[:, :, BP['left_ear'], :], kps[:, :, BP['right_ear'], :], kps[:, :, BP['nose'], :]]).mean(0)
        dist_left = norm(kps[:, :, BP['left_forepaw'], :] - head_center, axis=-1)
        dist_right = norm(kps[:, :, BP['left_forepaw'], :] - head_center, axis=-1)
        dist = np.min(np.array([dist_left, dist_right]), axis=0)
        return dist

    def plot_distribution(self, keypoints):
        dist = self._calc_distance(keypoints)
        plt.hist(dist.flatten(), range=[0, 20], bins=50)
        plt.show()

    def __call__(self, sequence):
        dist = self._calc_distance(sequence)
        return (dist - self.mean) / self.std


class PawNoseDistanceFeature:
    def __init__(self, keypoints):
        dist = self._calc_distance(keypoints)
        self.mean, self.std = dist.mean(), dist.std()
    
    def _calc_distance(self, kps):
        dist_left = norm(kps[:, :, BP['left_forepaw'], :] - kps[:, :, BP['nose'], :], axis=-1)
        dist_right = norm(kps[:, :, BP['left_forepaw'], :] - kps[:, :, BP['nose'], :], axis=-1)
        dist = np.min(np.array([dist_left, dist_right]), axis=0)
        return dist

    def plot_distribution(self, keypoints):
        dist = self._calc_distance(keypoints)
        plt.hist(dist.flatten(), range=[0, 40], bins=50)
        plt.show()

    def __call__(self, sequence):
        dist = self._calc_distance(sequence)
        return (dist - self.mean) / self.std


class AreaFeature:
    def __init__(self, keypoints, surface_keypoints):
        self.surface_keypoints = surface_keypoints
        area = polygon_area(keypoints[:, :, self.surface_keypoints, :])
        self.mean, self.std = area.mean(), area.std()

    def plot_distribution(self, keypoints):
        area = polygon_area(keypoints[:, :, self.surface_keypoints, :])
        plt.hist(area.flatten(), range=[0, 200], bins=50)
        plt.show()

    def __call__(self, sequence):
        area = polygon_area(sequence[:, :, self.surface_keypoints, :])
        return (area - self.mean) / self.std


class BodyAreaFeature(AreaFeature):
    def __init__(self, keypoints):
        super().__init__(keypoints=keypoints, surface_keypoints=(BP['left_forepaw'], BP['right_forepaw'], BP['left_hindpaw'], BP['right_hindpaw']))


class HeadAreaFeature(AreaFeature):
    def __init__(self, keypoints):
        super().__init__(keypoints=keypoints, surface_keypoints=(BP['nose'], BP['right_ear'], BP['left_ear']))


class SpeedFeature:
    def __init__(self):
        pass
    def __call__(self, sequence):
        center = sequence.mean(2)
        speed = norm(np.gradient(center, axis=0), axis=-1)
        return speed


class InternalAngleFeature:
    def __init__(self, keypoints):
        angle = self._calculate_angle(keypoints)
        self.mean, self.std = angle.mean(), angle.std()

    def _calculate_angle(self, kps):
        neck_nose = kps[:, :, BP['nose'], :] - kps[:, :, BP['neck'], :]
        neck_tailbase = kps[:, :, BP['neck'], :] - kps[:, :, BP['tail_base'], :]
        dot = np.einsum('bmi,bmi->bm', neck_nose, neck_tailbase).astype(np.float32)
        vector_norm = (norm(neck_nose, axis=-1) * norm(neck_tailbase, axis=-1))
        angle = np.arccos(np.divide(dot, vector_norm, out=np.zeros_like(dot), where=vector_norm!=0))
        return angle

    def plot_distribution(self, keypoints):
        angle = self._calculate_angle(keypoints)
        plt.hist(angle.flatten(), range=[0, np.pi], bins=50)
        plt.show()

    def __call__(self, sequence):
        angle = self._calculate_angle(sequence)
        return encode_angle((angle - self.mean) / self.std)


class DirectionChangeFeature:
    def __call__(self, sequence) -> np.array:
        heading = self._angle(sequence[:, :, BP['nose']], sequence[:, :, BP['center_back']])
        heading_change = np.gradient(heading, 1)[0]
        encoded_heading_change = encode_angle(heading_change)
        return encoded_heading_change.reshape(-1, 6)

    def plot_distribution(self, keypoints):
        heading = self._angle(keypoints[:, :, BP['nose']], keypoints[:, :, BP['center_back']])
        heading_change = np.gradient(heading)
        plt.hist(np.array(heading_change).flatten(), range=[-np.pi/10, np.pi/10], bins=50)
        plt.show()

    def _angle(self, seq1, seq2):
        return ((np.arctan2(seq1[..., 0] - seq2[..., 0], seq1[..., 1] - seq2[..., 1]) + np.pi / 2) % (np.pi * 2))


def get_mouse_features(kps, features_objects):
    [body_length, head_paw_distance, nose_paw_distance, body_area, head_area, internal_angle, direction_change, speed_feature] = features_objects

    feature_array = []
    feature_array.append(body_length(kps))
    feature_array.append(head_paw_distance(kps))
    feature_array.append(nose_paw_distance(kps))
    feature_array.append(body_area(kps))
    feature_array.append(head_area(kps))
    feature_array.append(internal_angle(kps).reshape(-1, 6))
    feature_array.append(direction_change(kps))
    feature_array.append(speed_feature(kps))
    feature_array = np.hstack(feature_array).astype(np.float32)

    return feature_array

# Environment
def get_mouse_cage_features(m_body, corners, feeding_location):
    front_feeding_distance = norm(m_body[0] - feeding_location)
    min_corner_distance = np.inf
    for point in m_body:
        for corner in corners:
            corner_distance = norm(corner - point)
            min_corner_distance = min(min_corner_distance, corner_distance)
    mouse_length = norm(m_body[0] - m_body[1])

    return [front_feeding_distance, min_corner_distance, mouse_length]


def get_mice_cage_features(kps, corners):
    feeding_location = get_feeding_location(*corners[:2])

    features = []
    for m0 in range(NUM_MICE):
        m_body = kps[m0, BODY_KEYPOINTS]
        mouse_cage_features = get_mouse_cage_features(m_body, corners, feeding_location)
        features.append(mouse_cage_features)
    features = np.array(features)
    min_features = features.min(axis=0)
    max_features = features.max(axis=0)

    return list(min_features) + list(max_features)

# ############### Group features ###############

class TriangleAreaFeature:
    def __init__(self, keypoints) -> None:
        self.max = self.get_stats(keypoints)

    def get_stats(self, keypoints):
        centers = keypoints[:, :, BP['center_back']]
        m1 = centers[:, 0]
        m2 = centers[:, 1]
        m3 = centers[:, 2]

        m1_m2 = m2 - m1
        m1_m3 = m3 - m1
        areas = np.abs(0.5 * (m1_m2[:, 0] * m1_m3[:, 1] - m1_m3[:, 0] * m1_m2[:, 1]))
        return areas.max()

    def __call__(self, sequence):
        centers = sequence[:, :, BP['center_back']]
        m1 = centers[:, 0]
        m2 = centers[:, 1]
        m3 = centers[:, 2]

        m1_m2 = m2 - m1
        m1_m3 = m3 - m1
        return np.abs(0.5 * (m1_m2[:, 0] * m1_m3[:, 1] - m1_m3[:, 0] * m1_m2[:, 1])) / self.max


class TriangleAngleFeatures:
    def __init__(self) -> None:
        pass

    def __call__(self, sequence):
        centers = sequence[:, :, BP['center_back']]
        m1 = centers[:, 0]
        m2 = centers[:, 1]
        m3 = centers[:, 2]

        m1_m2 = m2 - m1
        m1_m3 = m3 - m1
        m2_m3 = m3 - m2

        dot = np.einsum('bi,bi->b', m1_m2, m1_m3).astype(np.float32)
        vector_norm = (norm(m1_m2, axis=-1) * norm(m1_m3, axis=-1))
        angle_1 = np.arccos(np.divide(dot, vector_norm, out=np.zeros_like(dot), where=vector_norm!=0))

        dot = np.einsum('bi,bi->b', m1_m2, m2_m3).astype(np.float32)
        vector_norm = (norm(m1_m2, axis=-1) * norm(m1_m3, axis=-1))
        angle_2 = np.arccos(np.divide(dot, vector_norm, out=np.zeros_like(dot), where=vector_norm!=0))

        dot = np.einsum('bi,bi->b', m1_m3, m2_m3).astype(np.float32)
        vector_norm = (norm(m1_m2, axis=-1) * norm(m1_m3, axis=-1))
        angle_3 = np.arccos(np.divide(dot, vector_norm, out=np.zeros_like(dot), where=vector_norm!=0))
        
        return np.stack((encode_angle(np.stack((angle_1, angle_2, angle_3), axis=-1).min(1)), encode_angle(np.stack((angle_1, angle_2, angle_3), axis=-1).max(1))), axis=-1).reshape(-1, 4)


def get_group_features(kps, group_features_objects):
    triangle_area, triangle_angle = group_features_objects

    feature_array = []
    feature_array.append(triangle_area(kps).reshape(-1, 1))
    feature_array.append(triangle_angle(kps))

    feature_array = np.hstack(feature_array).astype(np.float32)
    return feature_array

# ############### Mouse pair features ###############

def get_mouse_mouse_features(m0_body, m1_body):
    m0_vector = m0_body[0] - m0_body[1]
    m0_unit_vector = m0_vector / norm(m0_vector)
    m1_vector = m1_body[0] - m1_body[1]
    m1_unit_vector = m1_vector / norm(m1_vector)

    features = []

    front_front_distance = norm(m1_body[0] - m0_body[0])
    front_rear_distance = norm(m1_body[0] - m0_body[1])
    rear_front_distance = norm(m1_body[1] - m0_body[0])
    rear_rear_distance = norm(m1_body[1] - m0_body[1])
    features.extend([front_front_distance, front_rear_distance, rear_front_distance, rear_rear_distance])

    orientation_angle = np.arccos(m0_unit_vector.T @ m1_unit_vector)
    front_front_unit_vector = (m1_body[0] - m0_body[0]) / norm(m1_body[0] - m0_body[0])
    position_angle = np.arccos(m0_unit_vector.T @ front_front_unit_vector)
    oa_cos, oa_sin = encode_angle(orientation_angle)
    pa_cos, pa_sin = encode_angle(position_angle)
    features.extend([oa_cos, oa_sin, pa_cos, pa_sin])
    
    return features


def get_mice_mice_features(kps):
    features = []
    for m0 in range(NUM_MICE):
        m0_body = kps[m0, BODY_KEYPOINTS] / MAX_SIZE
        for m1 in range(m0+1, NUM_MICE):
            m1_body = kps[m1, BODY_KEYPOINTS] / MAX_SIZE
            mouse_mouse_features_1 = get_mouse_mouse_features(m0_body, m1_body)
            mouse_mouse_features_2 = get_mouse_mouse_features(m1_body, m0_body)
            features.append(mouse_mouse_features_1)
            features.append(mouse_mouse_features_2)
    features = np.array(features)
    min_features = features.min(axis=0)
    max_features = features.max(axis=0)

    return list(np.sort(features, axis=0).flatten())#list(min_features) + list(max_features)


def get_static_features(kps, corners):
    mice_mice_features = get_mice_mice_features(kps)
    mice_cage_features = get_mice_cage_features(kps, corners)
    
    return np.array(mice_mice_features + mice_cage_features)


if __name__ == "__main__":
    keypoints_map = get_keypoints_map(DATA_DIR, "submission", filled_holes=True)
    keypoint_dataset = MouseKeypointSnippetDataset(keypoints_map, CLIP_LENGTH)

    frame_number_map = np.load(os.path.join(DATA_DIR, 'frame_number_map.npy'), allow_pickle=True).item()
    video_size = 'full_size'
    video_set = 'submission'
    video_data_dir = os.path.join(DATA_DIR, 'videos', video_size, video_set)

    dataset = VideoDataset(video_data_dir, frame_number_map, channels_first=False)

    all_keypoints = []
    for _, values in keypoints_map.items():
        all_keypoints.append(values['keypoints'])
    all_keypoints = np.vstack(all_keypoints)

    feature_array = []

    # Individual
    body_length = BodyAreaFeature(all_keypoints)
    head_paw_distance = PawHeadDistanceFeature(all_keypoints)
    nose_paw_distance = PawNoseDistanceFeature(all_keypoints)
    body_area = BodyAreaFeature(all_keypoints)
    head_area = HeadAreaFeature(all_keypoints)
    internal_angle = InternalAngleFeature(all_keypoints)
    direction_change = DirectionChangeFeature()
    speed_feature = SpeedFeature()

    # Group
    triangle_area = TriangleAreaFeature(all_keypoints)
    triangle_angle = TriangleAngleFeatures()

    features_objects = [body_length, head_paw_distance, nose_paw_distance, body_area, head_area, internal_angle, direction_change, speed_feature]
    group_features_objects = [triangle_area, triangle_angle]

    for i, keypoint_clip in enumerate(tqdm(keypoint_dataset)):
        keypoint_clip = keypoint_clip.astype(np.int64)
        frame = dataset[i * CLIP_LENGTH]
        corners = find_corners(frame)
        clip_features = []
        for kps in keypoint_clip:
            static_features = get_static_features(kps, corners)
            clip_features.append(static_features)
        
        individual_mouse_features = get_mouse_features(keypoint_clip, features_objects)
        group_mouse_features = get_group_features(keypoint_clip, group_features_objects)

        clip_features = np.array(clip_features)
        clip_features = np.concatenate((clip_features, individual_mouse_features), axis=1)
        clip_features = np.concatenate((clip_features, group_mouse_features), axis=1)

        # Dynamic features through gradient of keypoints and clip features
        dynamic_features_5 = np.gradient(clip_features, 5, axis=0)
        dynamic_features_kps_5 = norm(np.gradient(keypoint_clip, 5, axis=0), axis=-1).reshape(-1, 12*3)

        clip_features = np.concatenate((clip_features, dynamic_features_5, dynamic_features_kps_5), axis=1)

        feature_array.extend(clip_features)

    feature_array = np.array(feature_array, dtype=np.float32)

    feature_array = replace_nans_with_feature_mean(feature_array)
    
    np.save('cache/handcrafted_features.npy', feature_array)

File Path: utils/keypoint_dataset.py
Content:
from copy import deepcopy
import torch
import numpy as np
import os

class MouseKeypointSnippetDataset(torch.utils.data.Dataset):
    """Pytorch dataset of keypoint snippets.

        Each snippet is a slice from a single sequence, an array of shape
        (snippet_length, num_mice=3, num_keypoints=12, keypoint_dimensionality=2)
    """
    def __init__(self, sequence_keypoints_map, snippet_length=80):
        self.sequence_keypoints_map = sequence_keypoints_map
        self.snippet_length = snippet_length

        self.idx_keypoints_map = self._compute_idx_keypoints_map()

    def _compute_idx_keypoints_map(self):
        """Compute a mapping from index to (sequence_id, starting_index)."""

        end_index = -self.snippet_length + 1 if self.snippet_length > 1 else None
        idx_keypoints_map = []
        for sequence_id, sequence_info in self.sequence_keypoints_map.items():

            for i, _ in enumerate(sequence_info["keypoints"][:end_index]):
                idx_keypoints_map.append((sequence_id, i))
        
        return idx_keypoints_map

    def __len__(self):
        return len(self.idx_keypoints_map)

    def __getitem__(self, idx):
        sequence_id, starting_index = self.idx_keypoints_map[idx]
        sequence_keypoints = self.sequence_keypoints_map[sequence_id]["keypoints"]
        return sequence_keypoints[starting_index:starting_index + self.snippet_length]


def get_keypoints_map(data_dir, set='all', filled_holes=False):
    """
    _summary_

    Args:
        data_dir (_type_): _description_
        set (str, optional): _description_. Defaults to 'all'.

    Raises:
        ValueError: _description_

    Returns:
        _type_: _description_
    """
    if filled_holes:
        submission_keypoints = np.load(os.path.join(data_dir, "submission_keypoints_filled_holes.npy"), allow_pickle=True).item()
        user_train_keypoints = np.load(os.path.join(data_dir, "train_keypoints_filled_holes.npy"), allow_pickle=True).item()
    else:
        submission_keypoints = np.load(os.path.join(data_dir, "submission_keypoints.npy"), allow_pickle=True).item()
        user_train_keypoints = np.load(os.path.join(data_dir, "user_train.npy"), allow_pickle=True).item()

    if set == 'all':
        sequence_keypoints_map = deepcopy(user_train_keypoints["sequences"])
        sequence_keypoints_map.update(submission_keypoints["sequences"])
    elif set == 'train':
        sequence_keypoints_map = deepcopy(user_train_keypoints["sequences"])
    elif set == 'submission':
        sequence_keypoints_map = deepcopy(submission_keypoints["sequences"])
    else:
        raise ValueError(f"Incorrect argument, set='{set}'. Valid opitions: ['all', 'train', 'submission']")
    return sequence_keypoints_map


DATA_DIR = '/data/behavior-representation'


if __name__=='__main__':

    user_train_keypoints = get_keypoints_map(DATA_DIR, "train")
    submission_keypoints = get_keypoints_map(DATA_DIR, "submission")
    sequence_keypoints_map = get_keypoints_map(DATA_DIR, "all")

    assert len(sequence_keypoints_map) == \
        len(user_train_keypoints) + len(submission_keypoints)

    keypoint_dataset = MouseKeypointSnippetDataset(sequence_keypoints_map, 4)
    print(keypoint_dataset[0])
    print(keypoint_dataset[19076])
    print(len(keypoint_dataset))
    print(keypoint_dataset[0].shape)

File Path: utils/train_simclr_model.py
Content:
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from utils.keypoint_dataset import get_keypoints_map
from math import sqrt
import torchvision
from simclr import SimCLR
from simclr.modules import NT_Xent
from simclr.modules import LARS
from tqdm import tqdm

class MouseSimCLR(pl.LightningModule):
    def __init__(self, batch_size, embedding_size: int = 128, weight_decay: float = 1e-6):
        super().__init__()

        self.batch_size = batch_size
        self.embedding_size = embedding_size
        self.weight_decay = weight_decay

        resnet_encoder = torchvision.models.resnet50(pretrained=True)
        n_features = resnet_encoder.fc.in_features
        self.simclr = SimCLR(resnet_encoder, self.embedding_size, n_features)
        world_size = 1
        temperature = 0.5
        self.criterion = NT_Xent(batch_size, temperature, world_size)

    def forward(self, batch):
        if len(batch) == 2:
            x_i = batch[0]
            x_j = batch[1]
            h_i, h_j, z_i, z_j = self.simclr(x_i, x_j)
            return h_i, h_j, z_i, z_j
        else:
            x = batch
            h = self.simclr.encoder(x)
            z = self.simclr.projector(h)
            return h, z

    def training_step(self, batch, _):
        x_i = batch[0]
        x_j = batch[1]
        h_i, h_j, z_i, z_j = self.simclr(x_i, x_j)
        loss = self.criterion(z_i, z_j)
        self.log('train_loss', loss)
        return loss

    def configure_optimizers(self):
        # learning_rate = 0.3 * self.batch_size / 256
        # optimizer = LARS(
        #     self.parameters(),
        #     lr=learning_rate,
        #     weight_decay=self.weight_decay,
        #     exclude_from_weight_decay=["batch_normalization", "bias"],
        # )
        optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)

        # "decay the learning rate with the cosine decay schedule without restarts"
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, self.trainer.max_epochs, eta_min=0, last_epoch=-1
        )
        return [optimizer], [scheduler]

if __name__ == "__main__":
    from utils.augmentation import TransformsSimCLR
    from utils.video_dataset import SimCLRDataset, create_bounding_box
    import os
    from pytorch_lightning.callbacks import ModelCheckpoint

    data_dir = '/data/behavior-representation'
    video_size = 'full_size'
    video_set = 'submission'

    kp = create_bounding_box(data_dir)['sequences']


    video_data_dir = os.path.join(data_dir, 'videos', video_size, video_set)
    train_transform = TransformsSimCLR((224, 224), pretrained=False)
    train_dataset = SimCLRDataset(video_data_dir, kp, transform=train_transform)

    val_transform = TransformsSimCLR((224, 224), pretrained=False, validation=True)
    val_dataset = SimCLRDataset(video_data_dir, kp, transform=val_transform)

    batch_size = 76

    weight = np.load("cache/average_motion.npy")
    # weight = np.sqrt(weight)
    p = weight / weight.sum()
    num_samples = 100000
    sampler = torch.utils.data.WeightedRandomSampler(p, num_samples)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=True,
        pin_memory=False,
        num_workers=8,
        sampler=sampler
    )
    predict_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        pin_memory=False,
        num_workers=8
    )
    model = MouseSimCLR(batch_size=batch_size)

    checkpoint_callback = ModelCheckpoint(monitor="train_loss", save_top_k=1, save_last=True)
    # trainer = pl.Trainer(gpus=[1], max_epochs=1, precision=16, callbacks=[checkpoint_callback], limit_predict_batches=1/60)
    trainer = pl.Trainer(gpus=[0], max_epochs=25, precision=16, callbacks=[checkpoint_callback])
    trainer.fit(model=model, train_dataloaders=train_loader)    

    predictions = trainer.predict(model, predict_loader)
    cache_path = 'cache/simclr_embeddings.pt'
    print(f"Saving embeddings to {cache_path}")
    torch.save(predictions, cache_path)
File Path: utils/video_dataset.py
Content:
import os
import torch
import numpy as np
import cv2
from PIL import Image
from tqdm import tqdm
from torchvision.transforms import functional as F

class VideoDataset(torch.utils.data.Dataset):
    """
    Reads frames from video files
    """
    def __init__(self, 
                 datafolder, 
                 frame_number_map,
                 channels_first,
                 start_idx=0,
                 end_idx=None):
        """
        Initializing the dataset with images and labels
        """
        self.datafolder = datafolder
        self.frame_number_map = frame_number_map
        self.channels_first = channels_first

        self._setup_frame_map()
        self.start_idx = start_idx
        self.end_idx = end_idx if end_idx is not None else self.length

    def _setup_frame_map(self):
        self._video_names = np.array(list(self.frame_number_map.keys()))
        # IMPORTANT: the frame number map should be sorted for self.get_video_name to work
        frame_nums = np.array([self.frame_number_map[k] for k in self._video_names])
        self._frame_numbers = frame_nums[:, 0] - 1 # start values
        assert np.all(np.diff(self._frame_numbers) > 0), "Frame number map is not sorted"

        self.length = frame_nums[-1, 1] # last value is the total number of frames

    def get_frame_info(self, global_index):
        """ Returns corresponding video name and frame number"""
        video_idx = np.searchsorted(self._frame_numbers, global_index) - 1
        frame_index = global_index - (self._frame_numbers[video_idx] + 1)
        return self._video_names[video_idx], frame_index
    
    def __len__(self):
        return self.end_idx - self.start_idx
    
    def __getitem__(self, idx):
        idx = self.start_idx + idx
        video_name, frame_index = self.get_frame_info(idx)

        video_path = os.path.join(self.datafolder, video_name + '.avi')
        
        if not os.path.exists(video_path):
            raise FileNotFoundError(video_path)
       
        cap = cv2.VideoCapture(video_path)
        num_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if frame_index < 0 or frame_index >= num_video_frames:
            raise ValueError(f"Frame {frame_index} not found in video {video_name}!")
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
        success, frame = cap.read()

        img_pil = Image.fromarray(frame)
        img_out = img_pil.convert('RGB')
        img_out = np.array(img_out)
        if self.channels_first:
            img_out = img_out.swapaxes(0,2)
        return img_out


class SimCLRDataset(VideoDataset):
    def __init__(self, 
                 datafolder, 
                 keypoints,
                 channels_first=False,
                 transform=None,
                 start_idx=0,
                 end_idx=None,
                 bounding_box_out=False):
        self.transform = transform
        self.keypoints = keypoints
        self.bounding_box_out = bounding_box_out
        frame_number_map = {video_id: (idx * 1800, (idx + 1) * 1800) for idx, video_id in enumerate(keypoints.keys())}
        super().__init__(datafolder, 
                 frame_number_map,
                 channels_first,
                 start_idx=start_idx,
                 end_idx=end_idx)
    
    def __getitem__(self, idx):
        idx = self.start_idx + idx
        video_name, frame_index = self.get_frame_info(idx)

        video_path = os.path.join(self.datafolder, video_name + '.avi')
        
        if not os.path.exists(video_path):
            raise FileNotFoundError(video_path)
       
        cap = cv2.VideoCapture(video_path)
        num_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if frame_index < 0 or frame_index >= num_video_frames:
            raise ValueError(f"Frame {frame_index} not found in video {video_name}!")
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index.item())
        success, frame = cap.read()

        img_pil = Image.fromarray(frame)
        
        # Get bounding box
        bbox_type = np.random.randint(-1, 3)
        if bbox_type == -1:
            bbox = self.keypoints[video_name]['bbox']
        else:
            bbox = self.keypoints[video_name][f'bbox_{bbox_type}']
        
        if self.transform is not None:
            try:
                img_tensor = self.transform(img_pil, bbox[frame_index])
            except IndexError as e:
                print(e)
                img_tensor = self.transform(img_pil, [0, 0, 512, 512])
        else:
            img_tensor = F.to_tensor(img_pil)
        if self.bounding_box_out:
            return {
                'img': img_tensor, 
                'bbox': self.keypoints[video_name]['bbox'][frame_index],
                'bbox_0': self.keypoints[video_name]['bbox_0'][frame_index],
                'bbox_1': self.keypoints[video_name]['bbox_1'][frame_index],
                'bbox_2': self.keypoints[video_name]['bbox_2'][frame_index]
                }
        return img_tensor


def create_bounding_box(data_dir):

    if not os.path.exists(os.path.join(data_dir, f'submission_bbox.npy')):
        ########## Prepare bounding boxes from keypoints ##########

        scaling_factor = 1

        # Preparing some bounding box information to be used for cropping frames during training
        keypoints = np.load(os.path.join(data_dir, f'submission_keypoints.npy'), allow_pickle=True).item()

        # Bounding Box for all mice
        padbbox = 0
        crop_size = 512
        for sk in tqdm(keypoints['sequences'].keys()):
            kp = keypoints['sequences'][sk]['keypoints']
            bboxes = []
            for frame_idx in range(len(kp)):
                allcoords = np.int32(kp[frame_idx].reshape(-1, 2))
                minvals = max(np.min(allcoords[:, 0]) - padbbox, 0), max(np.min(allcoords[:, 1]) - padbbox, 0)
                maxvals = min(np.max(allcoords[:, 0]) + padbbox, crop_size), min(np.max(allcoords[:, 1]) + padbbox, crop_size)
                bbox = (*minvals, *maxvals)
                bbox = np.array(bbox)
                bbox = np.int32(bbox * scaling_factor)
                bboxes.append(bbox)
            keypoints['sequences'][sk]['bbox'] = np.array(bboxes)

        # Bounding Box for one mouse at a time
        for i in range(3):
            padbbox = 0
            crop_size = 512
            for sk in tqdm(keypoints['sequences'].keys()):
                kp = keypoints['sequences'][sk]['keypoints']
                bboxes = []
                for frame_idx in range(len(kp)):
                    allcoords = kp[frame_idx, i]
                    minvals = max(np.min(allcoords[:, 0]) - padbbox, 0), max(np.min(allcoords[:, 1]) - padbbox, 0)
                    maxvals = min(np.max(allcoords[:, 0]) + padbbox, crop_size), min(np.max(allcoords[:, 1]) + padbbox, crop_size)
                    bbox = (*minvals, *maxvals)
                    bbox = np.array(bbox)
                    bbox = np.int32(bbox * scaling_factor)
                    bboxes.append(bbox)
                keypoints['sequences'][sk][f'bbox_{i}'] = np.array(bboxes)

        # Can save it you want and load later
        np.save(os.path.join(data_dir, f'submission_bbox.npy'), keypoints)
    else:
        keypoints = np.load(os.path.join(data_dir, f'submission_bbox.npy'), allow_pickle=True).item()
    
    return keypoints
Output:
{
    "experimental_code": "import osimport numpy as npimport torchfrom transformers import BeitFeatureExtractor, BeitModelfrom tqdm import tqdmfrom utils.video_dataset import VideoDatasetdata_dir = '/data/behavior-representation'frame_number_map = np.load(os.path.join(data_dir, 'frame_number_map.npy'), allow_pickle=True).item()video_size = 'full_size'video_set = 'submission'video_data_dir = os.path.join(data_dir, 'videos', video_size, video_set)ds = VideoDataset(video_data_dir, frame_number_map, channels_first=True)batch_size=32dataloader = torch.utils.data.DataLoader(    ds,    batch_size=batch_size,    shuffle=False,    drop_last=False,    pin_memory=False,    num_workers=8,)print(\"len(dataloader), \", len(dataloader))feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-large-patch16-512')model = BeitModel.from_pretrained('microsoft/beit-large-patch16-512')device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"model.to(device)n_samples, embedding_length = len(ds), 1024embedding_array = np.zeros((n_samples, embedding_length))cache_path = os.path.join(\"cache\", \"beit_embeddings.npz\")os.makedirs(\"cache\", exist_ok=True)for i, x in enumerate(tqdm(dataloader)):    with torch.no_grad():        inputs = feature_extractor(images=[xx for xx in x], return_tensors=\"pt\").to(device)        outputs = model(**inputs)        embeddings = outputs[\"pooler_output\"].cpu().numpy()        start_idx, end_idx = i * batch_size, (i+1) * batch_size        embedding_array[start_idx:end_idx] = embeddings        if i % 1000 == 0:            print(f\"Caching embedding array to {cache_path}\")            np.savez(cache_path, embedding_array)print(f\"Caching embedding array to {cache_path}\")np.savez(cache_path, embedding_array)",
    "experimental_info": "Method: BEiT (Community-contributed video method).Model: microsoft/beit-large-patch16-512 (pre-trained).Input: Video frames from '/data/behavior-representation/videos/full_size/submission'.Batch Size: 32.Device: CUDA if available, else CPU.Output: Embeddings of length 1024, cached to 'cache/beit_embeddings.npz'."
}
