
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Taehong Moon1 Moonseok Choi 2 EungGu Yun3 Jongmin Yoon2 Gayoung Lee 4 Jaewoong Cho 1 Juho Lee 2 5 Abstract Diffusion models have shown remarkable perfor- mance in generation problems over various do- mains including images, videos, text, and audio. A practical bottleneck of diffusion models is their sampling speed, due to the repeated evaluation of score estimation networks during the infer- ence. In this work, we propose a novel frame- work capable of adaptively allocating compute required for the score estimation, thereby reduc- ing the overall sampling time of diffusion mod- els. We observe that the amount of computa- tion required for the score estimation may vary along the time step for which the score is esti- mated. Based on this observation, we propose an early-exiting scheme, where we skip the sub- set of parameters in the score estimation network during the inference, based on a time-dependent exit schedule. Using the diffusion models for im- age synthesis, we show that our method could significantly improve the sampling throughput of the diffusion models without compromising im- age quality. Furthermore, we also demonstrate that our method seamlessly integrates with var- ious types of solvers for faster sampling, capi- talizing on their compatibility to enhance over- all efficiency. The source code and our ex- periments are available at https://github. com/taehong-moon/ee-diffusion 1. Introduction Diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have shown remarkable success in diverse domains including image synthesis (Ho et al., 2020; Dhari- This work is partially done at KAIST AI. 1KRAFTON 2Graduate School of AI, KAIST 3Independent researcher 4Naver AI Lab, South Korea 5AITRICS, South Korea. Correspondence to: Juho Lee <juholee@kaist.ac.kr>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). wal & Nichol, 2021; Ho et al., 2022a), text-to-image gen- eration (Ramesh et al., 2022; Rombach et al., 2022), 3D point cloud generation (Luo & Hu, 2021), text-to-speech generation (Jeong et al., 2021), and video generation (Ho et al., 2022b). These models learn the reverse process of introducing noise into the data to data and denoise inputs progressively during inference using the learned reverse model. One major drawback of diffusion models is their slow sampling speed, as they require multiple steps of forward passes through score estimation networks to generate a sin- gle sample, unlike the other methods such as GANs (Good- fellow et al., 2014) that require only a single forward pass through a generator network. To address this issue, sev- eral approaches have been proposed to reduce the number of steps required for the sampling of diffusion models, for instance, by improving ODE/SDE solvers (Kong & Ping, 2021; Lu et al., 2022; Zhang & Chen, 2023) or distilling into models requiring less number of sampling steps (Sal- imans & Ho, 2022; Song et al., 2023). Moreover, in ac- cordance with the recent trend reflecting scaling laws of large models over various domains, diffusion models with a large number of parameters are quickly becoming main- stream as they are reported to produce high-quality sam- ples (Peebles & Xie, 2022). Running such large diffusion models for multiple sampling steps incurs significant com- putational overhead, necessitating further research to opti- mize calculations and efficiently allocate resources. On the other hand, recent reports have highlighted the ef- fectiveness of early-exiting schemes in reducing computa- tional costs for Large Language Models (LLMs) (Schuster et al., 2022; Hou et al., 2020; Liu et al., 2021; Schuster et al., 2021). The concept behind early-exiting is to bypass the computation of transformer blocks when dealing with relatively simple or confident words. Given that modern score-estimation networks employed in diffusion models share architectural similarities with LLMs, it is reasonable to introduce the early-exiting idea to diffusion models as well, with the aim of accelerating the sampling speed. In this paper, we introduce Adaptive Score Estimation (ASE) for faster sampling from diffusion models, draw- ing inspiration from the early-exiting schemes utilized in 1 arXiv:2408.05927v1  [cs.CV]  12 Aug 2024A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models LLMs. What sets diffusion models apart and distinguishes our proposal from a straightforward application of the early-exiting scheme is the time-dependent nature of the score estimation involved in the sampling process. We hy- pothesize that the difficulty of score estimation may vary at different time steps, and based on this insight, we adapt the computation of blocks differently for each time step. As a result, we gain the ability to dynamically control the computation time during the sampling procedure. To ac- complish this, we present a time-varying block-dropping schedule and a straightforward algorithm for fine-tuning a given diffusion model to be optimized for this schedule. ASE successfully accelerates the sampling speed of diffu- sion models while maintaining high-quality samples. Fur- thermore, ASE is highly versatile, as it can be applied to score estimation networks with various backbone architec- tures and can be combined with different solvers to further enhance sampling speed. We demonstrate the effectiveness of our method through experiments on real-world image synthesis tasks. 2. Related Work Fast Sampling of Diffusion Models. Diffusion proba- bilistic models (Sohl-Dickstein et al., 2015; Song & Er- mon, 2019; Ho et al., 2020; Dhariwal & Nichol, 2021) have shown their effectiveness in modeling data distribu- tions and have achieved the state-of-the-art performance, especially in the field of image synthesis. These models employ a progressive denoising approach for noisy inputs which unfortunately lead to heavy computational costs. To overcome this issue, multiple works have been proposed for fast sampling. DDIM (Nichol & Dhariwal, 2021) accel- erates the sampling process by leveraging non-Markovian diffusion processes. FastDPM (Kong & Ping, 2021) uses a bijective mapping between continuous diffusion steps and noises. DPM-Solver (Lu et al., 2022) analytically solves linear part exactly while approximating the non-linear part using high-order solvers. DEIS (Zhang & Chen, 2023) utilizes exponential integrator and polynomial extrapola- tion to reduce discretization errors. In addition to utiliz- ing a better solver, alternative approaches have been pro- posed, which involve training a student model using net- work distillation (Salimans & Ho, 2022). Recently, consis- tency model (Song et al., 2023; Song & Dhariwal, 2024) proposed a distillation scheme to directly find the consis- tency function from the data point within the trajectory of the probability flow. And Kim et al. (2023) refined the consistency model with input-output time parameterization within the score function and adversarial training. While previous approaches focused on reducing the timestep of sampling, recent studies proposed an alternative way to ac- celerate sampling speed by reducing the processing time of diffusion model itself. In particular, Block Caching (Wim- bauer et al., 2023) aim to re-use the intermediate feature which is already computed in previous timestep while To- ken Merging (Bolya & Hoffman, 2023) target to reduce the number of tokens. Concurrent work (Tang et al., 2023) sug- gests early exiting scheme on diffusion models. However, it requires additional module which is used to estimate an uncertainty of intermediate features. Our work is orthogo- nal to these existing approaches, as we focus on reducing the number of processed blocks for each time step, rather than targeting a reduction in the number of sampling steps. Early Exiting Scheme for Language Modeling. The recent adoption of Large Language Models (LLMs) has brought about significant computational costs, prompting interest in reducing unnecessary computations. Among the various strategies, an early-exiting scheme that dy- namically selects computation layers based on inputs has emerged for Transformer-based LLMs. DynaBERT (Hou et al., 2020) transfers knowledge from a teacher network to a student network, allowing for flexible adjustments to the width and depth. Yijin et al. (Liu et al., 2021) employ mu- tual information and reconstruction loss to assess the diffi- culty of input words. CAT (Schuster et al., 2021) incorpo- rates an additional classifier that predicts when to perform an early exit. CALM (Schuster et al., 2022) constrains the per-token exit decisions to maintain the global sequence- level meaning by calibrating the early-exiting LLM us- ing semantic-level similarity metrics. Motivated by the aforementioned works, we propose a distinct early-exiting scheme specifically designed for diffusion models. 3. Method This section describes our main contribution - Adaptive Score Estimation (ASE) for diffusion models. The sec- tion is organized as follows. We first give a brief recap on how to train a diffusion model and provide our intu- ition on the time-varying complexity of score estimation. Drawing from such intuition, we empirically demonstrate that precise score estimation can be achieved with fewer parameters within a specific time interval. To this end, we present our early-exiting algorithm which boosts inference speed while preserving the generation quality. 3.1. Time-Varying Complexity of Score Estimation Training Diffusion Models. Let x0 ‚àº pdata(x) := q(x) be a sample from a target data distribution. In a diffu- sion model, we build a Markov chain that gradually injects Gaussian noises to x0 to turn it into a sample from a noise distribution p(xT ), usually chosen as standard Gaussian distribution. Specifically, given a noise schedule (Œ≤t)T t=1, the forward process of a diffusion model is defined as 2A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models q(xt |xt‚àí1) = N(xt | p 1 ‚àí Œ≤txt‚àí1, Œ≤tI). (1) Then we define a backward diffusion process with a param- eter Œ∏ as, pŒ∏(x1:T ) = p(xT ) TY t=1 pŒ∏(xt‚àí1 |xt), q (xT |x0) ‚âà N(0, I). (2) so that we can start from xT ‚àº N(0, I) and denoise it into a sample x0. The parameter Œ∏ can be optimized by minimizing the negative of the lower-bound on the log- evidence, L(Œ∏) = ‚àí TX t=1 Eq [DKL[q(xt‚àí1 |xt, x0)‚à•pŒ∏(xt‚àí1 |xt)]] ‚â• ‚àílog pŒ∏(x0), (3) where q(xt‚àí1|xt, x0) = N  xt‚àí1; Àú¬µt(xt, x0), ÀúŒ≤tI  , Àú¬µt(xt, x0) = 1‚àöŒ±t  xt ‚àí Œ≤t‚àö1 ‚àí ¬ØŒ±t Œµt  . (4) The model distribution pŒ∏(xt‚àí1 |xt) is chosen as a Gaus- sian, pŒ∏(xt‚àí1 |xt) = N(xt‚àí1 |¬µŒ∏(xt, t), œÉ2 t I), ¬µŒ∏(xt, t) = 1‚àöŒ±t  xt ‚àí Œ≤t‚àö1 ‚àí ¬ØŒ±t ŒµŒ∏(xt, t)  , (5) and the above loss function then simplifies to L(Œ∏) = TX t=1 Ex0,Œµt h Œª(t) Œµt ‚àí ŒµŒ∏(‚àö¬ØŒ±tx0 + ‚àö 1 ‚àí ¬ØŒ±tŒµt, t) 2i , (6) where Œª(t) = Œ≤2 t 2œÉ2 t Œ±t(1‚àí¬ØŒ±t) . The neural network ŒµŒ∏(xt, t) takes a corrupted sample xt and estimates the noise that might have applied to a clean sample x0. Under a simple reparameterization, one can also see that, ‚àáxt log q(xt |x0) = ‚àí Œµt‚àö1 ‚àí ¬ØŒ±t ‚âà ‚àíŒµŒ∏(xt, t)‚àö1 ‚àí ¬ØŒ±t := sŒ∏(xt, t), (7) where sŒ∏(xt, t) is the score estimation network. In this pa- rameterization, the loss function can be written as, L(Œ∏) = TX t=1 Ex0,xt h Œª‚Ä≤ t‚à•‚àáxt log q(xt |x0) ‚àí sŒ∏(xt, t)‚à•2 i , (8) so learning a diffusion model amounts to regressing the score function of the distribution q(xt |x0). The op- timal regressor of the score function ‚àáxt log q(xt) at time step t is obtained by taking the expectation of the conditional score function over the noiseless distribution Ex0 |xt [‚àáxt log q(xt |x0)] = ‚àáxt log q(xt). Suppose we train our diffusion model using the standard parameterization (i.e., Œµ-parameterization), where the ob- jective is to minimize the gap ‚à•ŒµŒ∏ ‚àí Œµ‚à•2. When t is close to 1, this gap primarily represents noise, constituting only a small fraction of the entire x0. Consequently, it indicates that learning does not effectively occur in the proximity to the noise. Given that a diffusion model is trained across all time steps with a single neural network, it is reasonable to anticipate that a significant portion of the parameters are allocated for the prediction of near data regime ( t close to 0). This intuition leads to our dropping schedule pruning more parameters when t is close to 1. Adaptive Computation for Score Estimation To get the samples from diffusion models, we can apply Langevin dy- namics to get samples from the distribution given the score function ‚àáxlog p(x). Depending on the number of iter- ation N and step size Œ≤, we can iteratively update xt as follows: xt+1 = xt + Œ≤‚àáx log p(xt) + p 2Œ≤zt, (9) where zt ‚àº N(0, I). Due to this iterative evaluation, the total sampling time can be roughly be computed as T √ó œÑ, where T is the num- ber of sampling steps and œÑ is the processing of diffusion model per time step. To enhance sampling efficiency, con- ventional approaches aim to reduce the number of time steps within the constrained value of œÑ. Our experiments indicate that it‚Äôs feasible to reduce œÑ by performing score estimation for specific time intervals using fewer parame- ters. While one could suggest employing differently sized models for estimating scores at various time intervals to re- duce overall sampling time, our strategy introduces a sim- ple early exiting framework within a single model, avoid- ing extra memory consumption. Furthermore, our method focus on reducing the processing time œÑ while maintain- ing accurate predictions within a given time interval. To accomplish this, we introduce adaptive score estimation, wherein the diffusion model dynamically allocates param- eters based on the time t. For challenging task such as time t ‚Üí 0, the full parameter is utilized, while it induces skip- ping the subset of parameters near prior distribution. 3.2. Adaptive Layer Usage in Diffusion Process We hereby introduce an early exiting framework to accel- erate the sampling process of pre-trained diffusion models. 3A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Noise-EasyData-Easy FID: 8.88 FID: 47.71 Figure 1.Snapshot samples of Noise-Easy / Data-Easy schedules when fine-tuned DiT on ImageNet. While the data-easy sched- ule struggles to produce a discernible dog image, the noise-easy schedule successfully generates a clear dog image, achieving a converged FID score of 8.88. Drawing upon the intuition presented in ¬ß 3.1, we first ex- plain how to decide the amount of parameters to be used for score estimation. After dropping the selected blocks, we design a fine-tuning algorithm to adjust the output of intermediate building blocks of diffusion models. Which time interval can be accurately estimated with fewer parameters? To validate our hypothesis in the context of training diffusion models, we conduct a toy ex- periment regarding the difficulty of score estimation for different time steps. We conduct tests under two scenar- ios: one assuming that estimation near the prior distribu- tion requires fewer parameters (Noise-Easy schedule), and the other assuming that estimation near the data distribu- tion demands fewer parameters (Data-Easy schedule). As shown in Figure 1, one can easily find that the noise-easy schedule successfully generates a clear dog image where as the data-easy schedule struggles to produce a discernible dog image. Which layer can be skipped for score estimation? To accelerate inference in diffusion models, we implement a dropping schedule that takes into account the complexity of score estimation near t ‚Üí 1 compared to t ‚Üí 0. For the DiT model trained on ImageNet, which consists of 28 blocks, we design a dropping schedule that starts from the final block. Based on our intuition, we drop more DiT blocks as time approaches 1, as shown in Figure 2. Con- versely, for scores near the data, which represent more chal- lenging tasks, we retain all DiT blocks to utilize the entire parameter set effectively. In U-ViT, the dropping schedule has two main distinctions from DiT: the selection of candidate modules to drop and the subset of parameters to be skipped. Unlike DiT, we limit dropping to the decoder part in U-ViT. This decision is motivated by the presence of symmetric long skip connec- tions between encoder and decoder, as dropping encoder modules induce the substantial information loss. Moreover, when dropping the parameters in U-ViT, we preserve the linear layer of a building block to retain feature informa- tion connected through skip connections, while skipping score function Block 1Block 2DecoderDecoderBlock 2DecoderBlock 3DecoderBlock 4Decoder Block 3Decoder Figure 2.Schematic for time-dependent exit schedule. Consider- ing the varying difficulty of score estimation, we drop more build- ing blocks of architecture near noise. While we skip the whole building blocks in DiT, we partially skip the blocks in U-ViT due to the long skip-connection. the remaining parameters. 3.3. Fine-tuning Diffusion Models Following the removal of blocks based on a predetermined dropping schedule, we need to fine-tune the model. This is attributed to the early exit approach, where the interme- diate outputs of each building block are directly connected to the decoder. Consequently, the decoder encounters input values that differ from the distribution it learned during its initial training, requiring adjustments. To address this issue, we propose a novel fine-tuning algo- rithm that focuses on updating minimal information near time t ‚Üí 0 while updating unseen information near time t ‚Üí 1. To force the differential information update, we leverage two different techniques: (i) adapting Exponential Moving Average (EMA), and (ii) weighting the coefficients Œª(t). The EMA technique is employed to limit the frequency of information updates, thereby preserving the previous knowledge acquired by the model during its initial train- ing phase. A high EMA rate results in a more gradual modification of parameters. In our approach, we deliber- ately maintain a high EMA rate to enhance the stability of our training process. During the gradual parameter up- date, we aim to specifically encourage modifications in a subset of parameters that align the predicted scores more closely with the prior distribution. To prioritize the learn- ing of this score distribution, we apply a higher coefficient to the Œª(t) term, which in turn multiplies on the expectation of the training loss. Once the model‚Äôs performance appears to have plateaued, we adjust the Œª(t) value back to 1, aim- ing to facilitate comprehensive learning across the entire score distribution spectrum. We provide the pseudo-code for fine-tuning diffusion models in Appendix A. 4A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 4. Experiments 4.1. Experimental Setting Experimental Details. Throughout the experiments, we use DiT (Peebles & Xie, 2022) and U-ViT (Bao et al., 2022), the two representative diffusion models. We employ three pre-trained models: (1) DiT XL/2 trained on ImageNet (Krizhevsky et al., 2017) with the resolution of 256 √ó 256; (2) U-ViT-S/4 trained on CelebA (Liu et al., 2015) with the resolution of 64 √ó 64; (3) PixArt-Œ±-SAM- 256 trained on SAM dataset (Kirillov et al., 2023). For the fine-tuning step in both DiT and U-ViT experiments, we employ a hybrid loss (Nichol & Dhariwal, 2021) with a re- weighted time coefficient and linear schedule for injecting noise. We use AdamW (Loshchilov & Hutter, 2017) op- timizer with the learning rate of 2 ¬∑ 10‚àí5. We use cosine annealing learning rate scheduling to ensure training sta- bility for the U-ViT models. Batch size is set to 64, and 128 for fine-tuning DiT XL/2, U-ViT-S/4, respectively. We use T = 1000 time steps for the forward diffusion process. In case of PixArt experiment, we fine-tune our model with 100K SAM data, the batch size of 200 √ó4, and 2200 itera- tions while the pre-trained model is trained with 10M data, the batch size of 176 √ó64 and 150K iterations. For further experimental details, we refer readers to Appendix A. Evaluation Metrics. We employ Fr ¬¥echet inception dis- tance (FID) (Heusel et al., 2017) for evaluating image generation quality of diffusion models. We compute the FID score between 5,000 generated samples from diffu- sion models and the full training dataset. In case of text- to-image experiment, we measure the FID score with MS- COCO valid dataset (Lin et al., 2014). To evaluate the sam- pling speed of diffusion models, we report the wall-clock time required to generate a single batch of images on a sin- gle NVIDIA A100 GPU. Baselines. In this study, we benchmark our method against a range of recent techniques which aims reduc- ing the processing time of diffusion models. This in- cludes DeeDiff (Tang et al., 2023), token merging (ToMe; Bolya & Hoffman, 2023), and block caching (Wimbauer et al., 2023). When extending ToMe to U-ViT architec- ture, we specifically apply the token merging technique to self-attention and MLP modules within each block of the U-ViT. Of note, U-ViT treats both time and condi- tion as tokens in addition to image patches. To improve generative modeling, we exclude these additional tokens and focus solely on merging tokens associated with im- age patches, following the approach outlined by (Bolya & Hoffman, 2023). For block caching, we employ caching strategies within the attention layers. Naive caching may aggravate feature misalignment especially when caching is more aggressive in order to achieve faster sampling speed. To resolve such an issue, (Wimbauer et al., 2023) further propose shift-scale alignment mechanism. As we explore high-acceleration regime, we report results for both the original block caching technique and its variant with the shift-scale mechanism applied (termed SS in Figure 3). We only report the best performance attained among the diverse hyperparameter settings in the following sections. The remaining results will be deferred to Appendix C as well as experimental details for baseline strategies. 4.2. Inference Speed and Performance Trade-off Figure 3 presents a trade-off analysis between generation quality and inference speed, comparing our approach to other baseline methods. We can readily find that ASE largely outperforms both ToMe and block caching strate- gies. ASE boosts sampling speed by approximately 25- 30% while preserving the FID score. Techniques based on feature similarity, such as ToMe and block caching, are straightforward to implement yet fail to bring significant performance gain, or even in some cases, bring an increase in processing time. This can primarily be attributed to the additional computational overhead intro- duced by token partitioning and the complexity of bipartite soft matching calculations for token merging, which out- weighs the advantages gained from reducing the number of tokens. This observation is particularly noteworthy, as even for the CelebA dataset, the number of tokens in U- ViT remains relatively small, and U-ViT does not decrease the token count through layers, as is the case with U-Net. Regarding block caching, it yields only slight enhance- ments in inference speed while preserving the quality of generation. Although block caching can be straightfor- wardly applied to various diffusion models, it encounters a notable constraint: it relies significantly on scale-shift alignment, necessitating extra fine-tuning. Additionally, its effectiveness depends on the specifc architectural charac- teristics of the model being used. We postulate that this de- pendency may be related to the presence of residual paths within the architecture. It is crucial to highlight that our method effectively increases sampling speed without sacri- ficing the quality of the generated output. In Table 2, we further compare DeeDiff with our method using the performances reported in (Table 1; Tang et al., 2023). ASE and DeeDiff share the same essence as both are grounded in the early-exiting framework. The distinc- tion lies in the dynamic sampling process. To determine when to perform early-exiting for dynamic sampling, an additional module needs to be added to the model, whereas ASE does not require any additional memory. Furthermore, ASE exhibits faster acceleration while maintaining or im- proving FID, but for DeeDiff, there is a trade-off between 5A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 0 5 10 15 20 25 30 35 40 Acceleration (%) 101 102 FID ImageNet / DDIM-50 T oken Merging Block Caching w/o SS ASE (Ours) ‚àí5 0 5 10 15 20 25 30 Acceleration (%) 101 102 FID CelebA / DPM-50 T oken Merging Block Caching w/o SS Block Caching w/ SS ASE (Ours) Figure 3.Trade-off between image generation quality and sampling speed on ImageNet with DiT (left) and CelebA with U-ViT (right). We generate samples from DDIM and DPM sampler with 50 steps for ImageNet and CelebA, respectively. ASE largely outperforms other techniques, preserving FID score while boosting sampling speed by approximately 25-30%. Here, SS stands for scale-shift adjustment used together with block caching. Table 1.Trade-off between image generation quality and sampling speed on ImageNet (DiT; DDPM sampler) and CelebA (U-ViT; EM sampler). ASE consistently maintains image generation quality while achieving a notable increase in sampling speed of approximately 30%; ASE can be effectively used in conjunction with fast solvers. Refer to Table 5 in Appendix A for detailed description of our dropping schedules. (DiT) ImageNet DDPM-250 FID (‚Üì) Accel. ( ‚Üë) Baseline 9.078 - D2-DiT 8.662 23.43% D3-DiT 8.647 30.46% D4-DiT 9.087 34.56% D7-DiT 9.398 38.92% (U-ViT) CelebA EM-1000 FID (‚Üì) Accel. ( ‚Üë) Baseline 2.944 - D1-U-ViT 2.250 21.3% D2-U-ViT 2.255 24.8% D3-U-ViT 3.217 29.7% D6-U-ViT 4.379 32.6% the advantage in GFLOPs and the potential disadvantage in generation quality. In the case of ToMe and block caching, both methods fall significantly short of achieving the per- formance of ASE or DeeDiff. 4.3. Compatability with Diverse Sampling Solvers We demonstrate the compatibility of the proposed method with diverse sampling methods. First of all, we verify that our method can be successfully applied to accelerate sam- pling speed without degrading generation qualtiy. In Ta- ble 1, we generated samples with DDPM (Ho et al., 2020) in DiT architecture and get samples from Euler-Maruyama solver. Here, we present results of four varying dropping schedules in each experiments. In a nutshell, n in D- n schedule represents the acceleration scale. For instance, D3-DiT and D3-U-ViT schedules bring similar scales in terms of acceleration in sampling speed. We refer readers to Table 5 for detailed guide on ASE dropping schedules. Furthermore, we show that our method can be seamlessly incorporated with fast sampling solver, such as DDIM (Song et al., 2020) solvers and DPM solver (Lu et al., 2022). From the DiT results presented in , we we ob- serve that our approach effectively achieves faster infer- ence while utilizing fewer parameters, yet maintains the same level of performance. In case of U-ViT, we show that our method notably achieves an over 30% accelera- tion, while preserving similar quality in generation with the DPM solver. Notably in Figure 4, we highlight that our method is robust across various time steps within both DDIM and DPM solver. This indicates that our method effectively estimates scores across the entire time interval. The reasons for our method‚Äôs robustness and efficiency in achieving faster inference will be further explained in ¬ß 5. 4.4. Large Scale Text-to-Image Generation Task To demonstrate that our method can be extended to large- scale datasets, we apply it to the pre-trained PixArt- Œ± model. While there may be concerns that fine-tuning with a large-scale dataset could potentially slow down the fine- tuning process, we find that using only 1 % of the origi- nal data is sufficient for our method to achieve the desired performance. To evaluate our method, we employ a DPM 6A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 0 5 10 15 20 25 30 Acceleration (%) 8.9 9.0 9.1 9.2 9.3FID ImageNet / DDIM solver 0 5 10 15 20 25 Acceleration (%) 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4FID CelebA / DPM solver 50 steps 100 steps 200 steps 25 steps 50 steps Figure 4.Robustness of ASE across varying sampling timesteps: ImageNet with DDIM solver (left), and CelebA with DPM solver (right). Both experiments employed U-ViT architecture. ASE displays robust performance throughout different timesteps in both different experimental settings. Table 2.Trade-off between image generation quality and sam- pling speed on CelebA (U-ViT; DPM-50). Compared to the other baselines, ASE displays a remarkable sampling speed in terms of acceleration in GFLOPs. CelebA Methods Accel. ( ‚Üë) FID ( ‚Üì) U-ViT - 2.87 DeeDiff (Tang et al., 2023) 45.76% 3.9 ToMe (Bolya & Hoffman, 2023) 3.05% 4.963 Block Caching (Wimbauer et al., 2023) 9.06% 3.955 ASE (Ours) 23.39% 1.92 solver with 20 steps and classifier-free guidance (Ho & Sal- imans, 2022). Although the original model achieves an FID score of 12.483, the ASE-enhanced model attains an FID score of 12.682, with a 14 % acceleration in terms of wall- clock time. An example of an image generated from a given prompt is shown in Figure 5. 5. Further Analysis Ablation Study on Dropping Schedules. Although it is empirically understood that we can eliminate more param- eters near the prior distribution, it remains to be deter- mined which time-dependent schedules yield optimal per- formance in generation tasks. To design an effective drop- ping schedule, we conduct an ablation study as follows: we create four distinct schedules that maintained the same to- tal amount of parameter dropping across all time intervals, but vary the amount of dropping for each specific interval. These schedules are tested on a U-ViT backbone trained on the CelebA dataset. Specifically, the decoder part of this architecture consists of six blocks, and Figure 6 illus- trates how many blocks are utilized at each timet. By fine- Pre-trained model ASE (ours) Figure 5.Comparison between samples produced by pre-trained PixArt-Œ± and ASE-enhanced PixArt- Œ±. Text prompts are ran- domly chosen. tuning in this manner, we evaluate the generation quality of the models, as shown in Table 3. As the results indicate, Schedule 1 outperforms the others, demonstrating the most superior and stable performance across varying time steps. Viewpoint of Multi-task Learning. Diffusion models can be seen as a form of multi-task learning, as they use a single neural network to estimate the scores at every time t. In the context of multi-task learning, negative transfer phenomenon can occur, leading to a decrease in the gen- eration quality of diffusion models. Recent work, such as DTR (Park et al., 2023), improve generation quality by jointly training a mask with the diffusion model. This ap- proach minimizes negative transfer by reducing interfer- ence between tasks. Similarly, our method, despite us- ing fewer parameters, is designed to achieve a compara- ble effect. By explicitly distinguishing the parameters used for predicting specific intervals through early-exiting, our approach can mitigate the issues associated with negative 7A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Schedule-16655442211Schedule-26655441122Schedule-36655112244Schedule-46611224455 Figure 6.Dropping schedules designed for the ablation study. We divide the sampling time into ten uniform intervals, and drop a specific amount of blocks. The number indicates the amount of blocks left after dropping the rest. Table 3.FID score on CelebA dataset with U-ViT backbone across ablated dropping schedules. In both DPM-25 and DPM- 50, schedule-1 exhibits the best performance. Methods DPM-25 DPM-50 Schedule-1 2.116 2.144 Schedule-2 2.456 2.28 Schedule-3 2.173 3.128 Schedule-4 2.966 3.253 transfer. To illustrate the efficacy of our method in mitigating neg- ative transfer, we hereby conduct a toy experiment. Con- sider score estimation over a specific time intervalt ‚àà [s, l] as a single task. In the experiment, we equally divide the whole sampling time into ten intervals, thereby defining a total of ten tasks. To verify the presence of negative transfer in the diffusion model, we create both a baseline model and expert models trained specifically for each in- terval. In order to check whether the pre-trained model is sufficiently trained, we further train the baseline model, and Table 4 shows that further-training degrades the per- formance. Also, the multi-experts model outperforms the baseline model, indicating successful reduction of task in- terference. Furthermore, replacing the pre-trained model with the ASE module ( Mixed-k models) in a single time interval leads to performance gains. In Table 4, we can readily observe that the mixed schedules outperform the baseline model across all intervals in terms of image gen- eration quality. This finding suggests that our training ap- proach can not only effectively boost sampling speed but also preserves model performance via mitigating negative transfer effect. 6. Conclusion and Limitations In this paper, we present a novel method that effectively reduces the overall computational workload by using an early-exiting scheme in diffusion models. Specifically, our method adaptively selects the blocks involved in denois- ing the inputs at each time step, taking into account the OursBaselineMixed-kExperts :heavy:light k Figure 7.Schematic for different types of dropping schedules de- signed to validate negative transfer phenomenon. Mixed-k re- places the original heavy model with light ASE model only on kth time interval. Experts employ individually fine-tuned heavy models at each time interval. Table 4.FID score on CelebA dataset with U-ViT backbone across NTR-inspired dropping schedules. Experts outperform both baseline and further fine-tuned model thereby indicating that negative transfer does exist. Moreover, all the mixed-k sched- ules, despite only replacing a single time interval, demonstrate improved performance compared to the original baseline model. Methods DPM-25 DPM-50 Baseline 3.355 3.316 Further-trained 4.262 4.028 Multi-Experts 2.987 2.942 Mixed-1 2.938 3.054 Mixed-3 2.654 3.232 Mixed-5 3.287 3.187 Mixed-7 2.292 2.969 Mixed-9 2.933 3.027 assumption that fewer parameters are required for early de- noising steps. Surprisingly, we demonstrate that our method maintains performance in terms of FID scores even when reducing calculation costs by 30%. Our approach is not limited to specific architectures, as we validate its effectiveness on both U-ViT and DiTs models. A limitation of our pro- posed method is that we manually design the schedule for the early-exiting scheme. As future work, we acknowledge the need to explore automated methods for finding an opti- mal schedule. Impact Statement Our work is improving diffusion models which can be mis- used for generating fake images or videos, contributing to the spread of deepfake content or the creation of mislead- ing information. Also, given that these models are trained on data collected from the internet, there is a risk of harm- ful biases being embedded in the generated samples such as emphasizing stereotypes. 8A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Acknowledgement The authors would like to express their sincere gratitude to Jaehyeon Kim and Byeong-Uk Lee for their insightful and constructive discussions. This work was partly supported by Institute for Information & communications Technol- ogy Promotion(IITP) grant funded by the Korea govern- ment(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program(KAIST), KAIST-NA VER Hy- percreative AI Center, Korea Foundation for Advanced Studies (KFAS), No.2022-0-00713, Meta-learning Appli- cable to Real-world Problems), and National Research Foundation of Korea (NRF) funded by the Ministry of Ed- ucation (NRF2021M3E5D9025030). References Bao, F., Li, C., Cao, Y ., and Zhu, J. All are worth words: a vit backbone for score-based diffusion models. arXiv preprint arXiv:2209.12152, 2022. Bolya, D. and Hoffman, J. Token merging for fast stable diffusion. arXiv preprint arXiv:2303.17604, 2023. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sas- try, G., Askell, A., et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pp. 1877‚Äì1901, 2020. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the North American Chapter of the Association for Computational Linguistics (ACL), 2019. Dhariwal, P. and Nichol, A. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems 34 (NeurIPS 2021), pp. 8780‚Äì8794, 2021. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y . Generative adversarial nets. In Advances in Neural Information Processing Systems 27 (NIPS 2014), 2014. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion prob- abilistic models. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pp. 6840‚Äì6851, 2020. Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1‚Äì33, 2022a. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. arXiv:2204.03458, 2022b. Hou, L., Huang, Z., Shang, L., Jiang, X., Chen, X., and Liu, Q. Dynabert: Dynamic bert with adaptive width and depth. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), 2020. Jeong, M., Kim, H., Cheon, S. J., Choi, B. J., and Kim, N. S. Diff-tts: A denoising diffusion model for text-to- speech. In International Speech Communication Associ- ation, 2021. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y ., Uesaka, T., He, Y ., Mitsufuji, Y ., and Ermon, S. Consistency trajectory models: Learning probabil- ity flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y ., et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pp. 4015‚Äì4026, 2023. Kong, Z. and Ping, W. On fast sampling of diffusion proba- bilistic models. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models (INNF+ 2021), 2021. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84‚Äì90, 2017. Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra- manan, D., Doll¬¥ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740‚Äì 755. Springer, 2014. Liu, Y ., Meng, F., Zhou, J., Chen, Y ., and Xu, J. Faster depth-adaptive transformers. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 13424‚Äì 13432, 2021. 9A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , 2015. Liu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., and Hu, H. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3202‚Äì3211, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay reg- ularization. arXiv preprint arXiv:1711.05101, 2017. Lu, C., Zhou, Y ., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neu- ral Information Processing Systems 35 (NeurIPS 2022), 2022. Luo, S. and Hu, W. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Nichol, A. Q. and Dhariwal, P. Improved denoising diffu- sion probabilistic models. In Proceedings of The 38th International Conference on Machine Learning (ICML 2021), pp. 8162‚Äì8171, 2021. Park, B., Woo, S., Go, H., Kim, J.-Y ., and Kim, C. De- noising task routing for diffusion models. arXiv preprint arXiv:2310.07138, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with la- tent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684‚Äì10695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Con- volutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted In- tervention (MICCAI), 2015. Salimans, T. and Ho, J. Progressive distillation for fast sam- pling of diffusion models. In International Conference on Learning Representations (ICLR), 2022. Schuster, T., Fisch, A., Jaakkola, T., and Barzilay, R. Con- sistent accelerated inference via confident adaptive trans- formers. In Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2021. Schuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D., Tran, V ., Tay, Y ., and Metzler, D. Confident adaptive language modeling. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequi- librium thermodynamics. In Proceedings of The 32nd International Conference on Machine Learning (ICML 2015), pp. 2256‚Äì2265, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion im- plicit models. arXiv preprint arXiv:2010.02502, 2020. Song, Y . and Dhariwal, P. Improved techniques for training consistency models. International Conference on Learn- ing Representations (ICLR), 2024. Song, Y . and Ermon, S. Generative modeling by estimating gradients of the data distribution. In Advances in Neu- ral Information Processing Systems 32 (NeurIPS 2019), 2019. Song, Y ., Dhariwal, P., Chen, M., and Sutskever, I. Consis- tency models. In Proceedings of The 39th International Conference on Machine Learning (ICML 2023), 2023. Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Seg- menter: Transformer for semantic segmentation. In Pro- ceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 7262‚Äì7272, 2021. Tang, S., Wang, Y ., Ding, C., Liang, Y ., Li, Y ., and Xu, D. Deediff: Dynamic uncertainty-aware early exiting for accelerating diffusion model generation. arXiv preprint arXiv:2309.17074, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. Atten- tion is all you need. In Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017. Wimbauer, F., Wu, B., Schoenfeld, E., Dai, X., Hou, J., He, Z., Sanakoyeu, A., Zhang, P., Tsai, S., Kohler, J., et al. Cache me if you can: Accelerating diffu- sion models through block caching. arXiv preprint arXiv:2312.03209, 2023. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P. Segformer: Simple and efficient design for semantic segmentation with transformers. In Advances in Neural Information Processing Systems 34 (NeurIPS 2021), pp. 12077‚Äì12090, 2021. Yang, X., Shih, S.-M., Fu, Y ., Zhao, X., and Ji, S. Your ViT is secretly a hybrid discriminative-generative diffusion model. arXiv preprint arXiv:2208.07791, 2022. 10A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Zhang, Q. and Chen, Y . Fast sampling of diffusion models with exponential integrator. In Proceedings of The 39th International Conference on Machine Learning (ICML 2023), 2023. 11A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models A. Experimental Details A.1. How to design dropping schedules? Diverse Time-dependent Dropping Schedules In Table 1, we briefly introduce the difference between the diverse sched- ules, D1 to D6. We hereby provide the formal definition of D- n schedules. We refer the reader to Table 5. First, the sampling time [0, 1] is divided into ten intervals with equal length. For the DiT architecture, we designated the blocks to be dropped among the total of 28 blocks. In the case of D1-DiT, we utilized all 28 blocks near the data. As we moved towards the noise side, we gradually discarded some blocks per interval, resulting in a final configuration of using the smallest number of blocks near the noise. The higher the number following ‚ÄôD‚Äô, the greater the amount of discarded blocks, thereby reducing the processing time of the diffusion model. For the most accelerated configuration, D7-DiT, we designed a schedule where only 8 blocks pass near the noise. Table 5.Number of blocks used for varying dropping schedules. All schedules use the same number of blocks within a fixed time interval. Of note, n in D-n schedule represents the acceleration scale. For instance, D3-DiT and D3-U-ViT schedules bring similar scales in terms of acceleration in sampling speed. Reported acceleration performance is measured with DDPM and EM solver applied to DiT and U-ViT, respectively. Schedule Acceleration Sampling timestept [0,0.1] [0 .1,0.2] [0 .2,0.3] [0 .3,0.4] [0 .4,0.5] [0 .5,0.6] [0 .6,0.7] [0 .7,0.8] [0 .8,0.9] [0 .9,1.0] D2-DiT 23.43% 28 28 25 25 22 22 19 19 16 16 D3-DiT 30.46% 28 28 24 24 20 20 16 16 12 12 D4-DiT 34.56% 28 28 26 24 20 18 12 10 8 8 D7-DiT 38.92% 28 28 24 21 18 15 10 10 8 8 D1-U-ViT 21.3% 6 6 4 4 2 2 2 2 1 1 D2-U-ViT 24.8% 5 5 4 4 2 2 1 1 1 1 D3-U-ViT 29.7% 3 3 2 2 2 2 1 1 1 1 D6-U-ViT 32.6% 2 2 2 2 1 1 1 1 1 1 For the U-ViT architecture as we depicted in Figure 8, we aimed to preserve the residual connections by discarding sub- blocks other than nn.Linear, rather than skipping the entire building block. Additionally, the target of dropping was limited to the decoder part, distinguishing it from DiT. Similarly, for D1-U-ViT, we allowed the entire decoder consisting of 6 blocks to pass near the data, and as we moved towards the noise side, we gradually discarded a single block per interval, resulting in only 1 blocks passing near the noise, while the remaining blocks only passed through nn.Linear. Block 1 ùíô(ùüé)ùíô(ùëª) Block 2Decoder Decoder Block 4 Decoder Decoder ùíô(ùüé)ùíô(ùëª) ùë´ùíäùëª ùëº- ùëΩùíäùëª Block 1 Decoder Block 2 Block 4 Decoder Figure 8.Schematic for the dropping schedules of DiT (left) and U-ViT (right). Due to the existence of residual connections in U-ViT, dropping encoder or decoder blocks in a straightforward manner cause severe performance degradation. In the case of U-ViT, the decoder blocks, except for the linear layer connected to encoder residual connections, are dropped. A.2. Pseudo-code for fine-tuning diffusion models 12A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Algorithm 1 Adjusting the output of intermediate building block of diffusion models Require: Training dataset D, Teacher parameter Œ∏T = [Œ∏1 T , . . . , Œ∏N T ], Student parameter Œ∏S = [Œ∏1 S, . . . , Œ∏N S ], EMA rate Œ±, Pre-defined Exit Schedule S(t), Time-dependent coefficient Œª(t), Re-weighting cycle C, Learning rate Œ∑. Œ∏T ‚Üê Œ∏S, t‚àº [0, 1] while not converged do Sample a mini-batch B ‚àº D. for i = 1, . . . ,|B| do Take the input xi from B. for l = 1, . . . , Ndo if l ‚â§ S(t) then Àúxi ‚Üê perturb(xi, t) ‚Ñìi ‚Üê Œª(t) ¬∑ loss( Àúxi, t) else Break for loop end if end for end for Œ∏S ‚Üê Œ∏S ‚àí Œ∑‚àáŒ∏S 1 |B| P i ‚Ñìi. Update Œ∏T ‚Üê Œ±Œ∏T + (1 ‚àí Œ±)Œ∏S end while A.3. Computational Efficiency of ASE Additional Fine-tuning cost of ASE Compared with ToMe (Bolya & Hoffman, 2023) and Block Caching (Wimbauer et al., 2023), our method requires fine-tuning. Nonetheless, we demonstrate its negligible fine-tuning cost and high effi- ciency by reporting the computational costs for fine-tuning in Table 6. Table 6.Fine-tuning costs when we apply ASE into pre-trained DiT on ImageNet and U-ViT on CelebA. These tables show the number of iterations and batch sizes used during the fine-tuning process. (DiT) ImageNet iteration * batch size Baseline 400K * 256 D2-DiT 400K * 32 (12.50 %) D3-DiT 450K * 32 (14.06 %) D4-DiT 500K * 32 (15.63 %) (U-ViT) CelebA iteration * batch size Baseline 500K * 128 D1-U-ViT 40K * 128 (8 %) D2-U-ViT 50K * 128 (10 %) D3-U-ViT 150K * 64 (15 %) D6-U-ViT 200K * 64 (20 %) Results on actual inference time of ASE In Table 7, we provide additional results on wall-clock time. We note that the acceleration rate in the original paper is also measured in terms of wall-clock time. Table 7.Wall-clock time of generating samples with ASE-enhanced models. Left table is the result of DiT model fine-tuned on ImageNet and right table is the result of U-ViT model fine-tuned on CelebA. (DiT) ImageNet DDPM-250 FID (‚Üì) Wall-clock time (s) (‚Üì) Baseline 9.078 59.60 D2-DiT 8.662 45.63 D3-DiT 8.647 41.44 D4-DiT 9.087 39.00 D7-DiT 9.398 36.40 (U-ViT) CelebA EM-1000 FID (‚Üì) Wall-clock time (s) (‚Üì) Baseline 2.944 216.70 D1-U-ViT 2.250 170.54 D2-U-ViT 2.255 162.95 D3-U-ViT 3.217 152.34 D6-U-ViT 4.379 146.05 13A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models B. Related Work Transformers in Diffusion Models. The pioneering diffusion models (Ho et al., 2020; Song & Ermon, 2019; Dhariwal & Nichol, 2021), especially in the field of image synthesis, have adopted a U-Net (Ronneberger et al., 2015) backbone architecture with additional modifications including the incorporation of cross- and self-attention layers. Motivated by the recent success of transformer (Vaswani et al., 2017) networks in diverse domains (Brown et al., 2020; Devlin et al., 2019; Xie et al., 2021; Strudel et al., 2021; Liu et al., 2022), several studies have attempted to leverage the Vision Transformer (ViT) (Dosovitskiy et al., 2021) architecture for diffusion models. Gen-ViT (Yang et al., 2022) is a pioneering work that shows that standard ViT can be used for diffusion backbone. U-ViT (Bao et al., 2022) enhances ViT‚Äôs performance by adding long skip connections and additional convolutional operation. Diffusion Transformers (DiTs) (Peebles & Xie, 2022) investigate the scalability of transformers for diffusion models and demonstrate that larger models consistently exhibit improved performance, albeit at the cost of higher GFLOPs. Our approach focuses on enhancing the efficiency of the transformer through adaptive block selection during calculations, and can be applied to existing transformer-based approaches, such as DiTs, to further optimize their performance. C. Further Analysis on Baselines Analysis on ToMe In this section, we conducted experiments on three different cases for applying ToMe to the building block of a given architecture. The ‚ÄòF‚Äô schedule denotes applying ToMe starting from the front-most block, the ‚ÄòR‚Äô schedule denotes starting from the back-most block, and the ‚ÄòB‚Äô schedule represents symmetric application from both ends. In the Figure 3, we report the experiment results that showed the most competitive outcomes. Furthermore, we present the remaining experiments conducted using various merging schedules, as illustrated in Table 8, Table 9. In summary, for the DiT architecture, the ‚ÄòB‚Äô schedule performed well, while the ‚ÄòR‚Äô schedule demonstrated satisfactory performance for the U-ViT architecture. Table 8.Diverse merging schedule experiments on DiT with DDIM sampler. DDIM-50 B2 B4 B6 B8 All FID (‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) FID ( ‚Üì) Accel. ( ‚Üë) attn-ratio-2-down-1 9.172 0.29% 9.421 0.37% 10.43 0.60% 13.926 0.69% 117.194 1.92% attn-ratio-3-down-1 9.313 0.49% 9.745 0.82% 12.918 1.03% 22.495 1.45% 170.170 6.08% attn-ratio-4-down-1 9.409 0.85% 10.314 1.59% 17.567 2.27% 37.763 2.97% 214.759 10.34% attn-ratio-5-down-1 9.741 0.91% 11.284 2.26% 25.675 2.63% 58.550 4.07% 247.608 16.66% attn-ratio-6-down-1 10.014 0.99% 12.441 2.34% 38.124 3.72% 81.987 5.07% 274.591 21.55% Table 9.Diverse merging schedule experiments on U-ViT with DPM sampler. DPM-50 R2 R3 R4 R5 FID (‚Üì) Accel. (‚Üë) FID ( ‚Üì) Accel. (‚Üë) FID ( ‚Üì) Accel. (‚Üë) FID ( ‚Üì) Accel. (‚Üë) attn-ratio-2-down-1 38.505 -3.98% 45.544 -5.89% 65.755 -7.51% 79.086 -9.15% attn-ratio-3-down-1 120.596 -2.97% 141.073 -4.53% 200.132 -5.85% 232.040 -7.07% attn-ratio-4-down-1 264.153 -2.13% 279.270 -2.76% 311.823 -3.69% 319.599 -4.57% attn-ratio-5-down-1 308.350 -1.13% 315.334 -1.53% 332.565 -1.90% 343.486 -2.02% attn-ratio-6-down-1 330.501 0.05% 344.353 0.41% 362.002 0.69% 372.612 1.10% Analysis on Block Caching To ensure fair comparison between baseline methods, we faithfully implement block caching algorithm on both DiT and U-ViT architecture. In this experiment, we applied it to the attention part of the U-ViT blocks, and Table 10 shows the trade-off between generation quality and inference speed depending on the presence or absence of the scale-shift mechanism. 14A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models D. Qualitative Comparison We present comprehensive experimental results, primarily including qualitative analyses. Figure 9 and Figure 10 shows the superior quality of generated samples under various dropping schedules. Additionally, in the Figure 11 and Figure 12, we show the robustness of ASE across varing sampling timesteps. Notably, we provide visual representations of randomly generated images for each time-dependent early exiting schedule. In the Figure 13, it illustrates the results obtained by sampling from fine-tuned DiT checkpoint using both the DDPM and DDIM sampler. Similarly, in the Figure 14, it exhibits the results obtained by sampling from fine-tuned U-ViT checkpoint using both the EM and DPM sampler. 59.6s, 0% 45.63s, 23.4%39.0s, 34.5% 22.5s, 0% 17.7s, 21.3%14.7s, 34.6% 5.71s, 0% 4.51s, 21.0%3.74s, 34.5% DDPM solverDDIM solverDPM solver Figure 9.Images sampled from ASE-enhanced DiT model with diverse dropping schedules. 20.9s, 0% 18.0s, 13.8%15.5s, 25.8% DPM solver Figure 10.Images sampled from ASE-enhanced U-ViT model with diverse dropping schedules. Table 10.Additional block caching experiments on U-ViT with DPM sampler. DPM-50 Attn(wo SS) Attn(w SS) FID (‚Üì) Accel. (‚Üë) FID (‚Üì) Accel. (‚Üë) attn-ths-0.1 4.462 9.70% 3.955 9.06% attn-ths-0.2 14.083 18.73% 9.707 18.11% attn-ths-0.3 53.770 22.80% 32.518 22.35% attn-ths-0.4 60.390 24.98% 45.523 24.26% 15A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models 25 step15step10 step 25 step15step10 step DPM solverDPM solver Figure 11.Images sampled from the fine-tuned DiT model with DPM sampler. 20 step15 step10 step 20 step15 step10 step DPM solverDPM solver Figure 12.Images sampled from the fine-tuned U-ViT model with DPM sampler. 16A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Figure 13.Images sampled from the fine-tuned DiT model. Top: DDPM sampler-250 steps; Bottom: DDIM sampler-50 steps. 17A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Figure 14.Images sampled from the fine-tuned U-ViT model. Top: EM solver-1000 steps; Bottom: DPM solver-25 steps. 18
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper addresses the slow sampling speed of diffusion models, a practical bottleneck due to repeated score estimation network evaluations. It proposes Adaptive Score Estimation (ASE), a novel early-exiting framework that adaptively allocates compute for score estimation by skipping a subset of network parameters based on a time-dependent exit schedule. The key observation is that computation difficulty varies across time steps (easier near noise, harder near data). ASE significantly improves sampling throughput by approximately 25-30% without compromising image quality, seamlessly integrates with various faster sampling solvers (DDIM, DPM-Solver, DDPM, EM), and reduces processing time per time step. Furthermore, the method demonstrates robustness across different time steps and effectively mitigates the negative transfer phenomenon in diffusion models operating as multi-task learners.",
    "methodology": "The core methodology is Adaptive Score Estimation (ASE), an early-exiting scheme inspired by techniques used in Large Language Models. It introduces a time-varying block-dropping schedule where more network parameters (blocks) are skipped as the time step 't' approaches 1 (closer to the noise distribution, considered 'noise-easy'), while full parameters are retained as 't' approaches 0 (closer to the data distribution, considered 'data-hard'). A novel fine-tuning algorithm is proposed to adjust the model after block removal, focusing on updating minimal information near t‚Üí0 and unseen information near t‚Üí1. This fine-tuning process leverages Exponential Moving Average (EMA) for stable, gradual parameter modification and weighted coefficients Œª(t) in the loss function to prioritize learning specific score distributions. For DiT architecture, entire building blocks are skipped. For U-ViT, dropping is limited to the decoder part, with linear layers preserved within blocks due to long skip connections, to prevent substantial information loss.",
    "experimental_setup": "The method was evaluated using three pre-trained diffusion models: DiT XL/2 on ImageNet (256x256), U-ViT-S/4 on CelebA (64x64), and PixArt-Œ±-SAM-256 (fine-tuned on 100K SAM data). Fine-tuning employed a hybrid loss with a re-weighted time coefficient, linear noise schedule, AdamW optimizer (LR 2e-5), cosine annealing LR for U-ViT, and batch sizes of 64 (DiT) or 128 (U-ViT). Forward diffusion used 1000 time steps. Evaluation metrics included Fr√©chet Inception Distance (FID) for image generation quality (5,000 samples, MS-COCO for text-to-image) and wall-clock time on a single NVIDIA A100 GPU for sampling speed. Baselines included DeeDiff, Token Merging (ToMe), and Block Caching (with and without shift-scale adjustment). Compatibility was demonstrated with DDPM, DDIM, DPM-Solver, and Euler-Maruyama (EM) solvers. Ablation studies on U-ViT (CelebA) investigated various time-dependent dropping schedules, and a negative transfer study compared baseline, further-trained, multi-experts, and mixed-k models.",
    "limitations": "A primary limitation is that the dropping schedule for the early-exiting scheme is manually designed. The paper acknowledges the need for automated methods to find an optimal schedule. Furthermore, the authors highlight general ethical concerns regarding diffusion models: the potential for misuse in generating fake content (deepfakes, misleading information) and the risk of embedding harmful biases and emphasizing stereotypes due to training on internet-sourced data.",
    "future_research_directions": "The main future research direction identified is to explore automated methods for finding an optimal dropping schedule for the early-exiting scheme. This would alleviate the current manual design process and potentially lead to even more efficient and effective adaptive computation strategies for diffusion models."
}
